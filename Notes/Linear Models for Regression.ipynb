{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e59f5cb-6f08-4b5b-946a-888b8b659f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79d00d97-9a89-4152-b9cc-e7f93e4acb19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> Gradient Descent:\n",
      "    * used to minimize error\n",
      "    * start with a feature weighted by some initial value\n",
      "        o change weight and look at the gradient of the error / loss function WRT that feature\n",
      "        o if gradient changes sign, change weight in other direction by subtracing the gradient from the weight in steps of \n",
      "          a predetermined size (learning rate, alpha)\n",
      "        o when gradient is zero / below some threshold, you have correct weight to minimize error\n",
      "        o if you are trying to maximize, rather than minimize, move in the direction of the gradient, not the opposite\n",
      "\n",
      "--> Batch Gradient Descent:\n",
      "    * Input some training data\n",
      "    * Initialize with some random values\n",
      "    * While not converged:\n",
      "        o compute MSE for some feature w\n",
      "        o compute the gradient of the error for w\n",
      "        o update w = w - (del(error) wrt w)\n",
      "    * Convergence conditions:\n",
      "        o gradient -> 0 i.e. error < e for some threshold e close to zero\n",
      "        o error doesn't decrease i.e. wf - wo < e for some threshold e close to zero\n",
      "        o alternatively, fix number of iterations\n",
      "    * Pros:\n",
      "        o few updates\n",
      "        o stable convergence\n",
      "    * Cons:\n",
      "        o spacially and computationally intensive\n",
      "        o premature convergence\n",
      "\n",
      "--> Mini-Batch Gradient Descent:\n",
      "    * split the train data into small batches and update based on batches\n",
      "    * Input some training data\n",
      "    * Initialize with random values\n",
      "    * for epoch in range(num_epochs):\n",
      "        shuffle data and split into batches\n",
      "        for batch in batch_set:\n",
      "            compute del(error wrt w)\n",
      "            update w = w - alpha * del(error wrt w)\n",
      "    * Pros:\n",
      "        o higher update frequency -> more robust convergence\n",
      "        o more effecient memory-wise and in implementation\n",
      "    * Cons:\n",
      "        o require hyperparameter: mini-batch size\n",
      "        o if batch size is small, unstable convergence\n",
      "\n",
      "--> Closed Form Solution:\n",
      "    * error(w) = (y - Xw)^T * (y - Xw)\n",
      "        o where X is an n x (p+1) matrix and each row is a data point\n",
      "    * differentiate: del(error(w)) = -2 * X^T * (y - Xw)\n",
      "    * error is minimized when gradient is zero\n",
      "    * w = (X^T * X)^-1 * X^T * y\n",
      "\n",
      "--> Polynomial Regression:\n",
      "    * relationship between sample x and label y in modeled as an nth degree polynomial\n",
      "    * ex.:\n",
      "        o x = (x1, x2)\n",
      "        o lin. reg: y_hat = w0 + w1x1 + w2x2\n",
      "        o 2nd deg poly. reg: y_hat = w0 + w1x1 + w2x2 + w3x1x2 + w4x1^2 + w5x2^2\n",
      "    * prediction is still linear wrt weights w\n",
      "    \n",
      "--> Performance Evaluation:\n",
      "    * goal of training ML models: good future prediction\n",
      "    * best model on training data will memorize data, not generalize well\n",
      "    * for future performance, split into train and test set\n",
      "        o generally 80:20 or 90:10 ratio\n",
      "    * trained model is evaluated on test set\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "--> Gradient Descent:\n",
    "    * used to minimize error\n",
    "    * start with a feature weighted by some initial value\n",
    "        o change weight and look at the gradient of the error / loss function WRT that feature\n",
    "        o if gradient changes sign, change weight in other direction by subtracing the gradient from the weight in steps of \n",
    "          a predetermined size (learning rate, alpha)\n",
    "        o when gradient is zero / below some threshold, you have correct weight to minimize error\n",
    "        o if you are trying to maximize, rather than minimize, move in the direction of the gradient, not the opposite\n",
    "\n",
    "--> Batch Gradient Descent:\n",
    "    * Input some training data\n",
    "    * Initialize with some random values\n",
    "    * While not converged:\n",
    "        o compute MSE for some feature w\n",
    "        o compute the gradient of the error for w\n",
    "        o update w = w - (del(error) wrt w)\n",
    "    * Convergence conditions:\n",
    "        o gradient -> 0 i.e. error < e for some threshold e close to zero\n",
    "        o error doesn't decrease i.e. wf - wo < e for some threshold e close to zero\n",
    "        o alternatively, fix number of iterations\n",
    "    * Pros:\n",
    "        o few updates\n",
    "        o stable convergence\n",
    "    * Cons:\n",
    "        o spacially and computationally intensive\n",
    "        o premature convergence\n",
    "\n",
    "--> Mini-Batch Gradient Descent:\n",
    "    * split the train data into small batches and update based on batches\n",
    "    * Input some training data\n",
    "    * Initialize with random values\n",
    "    * for epoch in range(num_epochs):\n",
    "        shuffle data and split into batches\n",
    "        for batch in batch_set:\n",
    "            compute del(error wrt w)\n",
    "            update w = w - alpha * del(error wrt w)\n",
    "    * Pros:\n",
    "        o higher update frequency -> more robust convergence\n",
    "        o more effecient memory-wise and in implementation\n",
    "    * Cons:\n",
    "        o require hyperparameter: mini-batch size\n",
    "        o if batch size is small, unstable convergence\n",
    "\n",
    "--> Closed Form Solution:\n",
    "    * error(w) = (y - Xw)^T * (y - Xw)\n",
    "        o where X is an n x (p+1) matrix and each row is a data point\n",
    "    * differentiate: del(error(w)) = -2 * X^T * (y - Xw)\n",
    "    * error is minimized when gradient is zero\n",
    "    * w = (X^T * X)^-1 * X^T * y\n",
    "\n",
    "--> Polynomial Regression:\n",
    "    * relationship between sample x and label y in modeled as an nth degree polynomial\n",
    "    * ex.:\n",
    "        o x = (x1, x2)\n",
    "        o lin. reg: y_hat = w0 + w1x1 + w2x2\n",
    "        o 2nd deg poly. reg: y_hat = w0 + w1x1 + w2x2 + w3x1x2 + w4x1^2 + w5x2^2\n",
    "    * prediction is still linear wrt weights w\n",
    "    \n",
    "--> Performance Evaluation:\n",
    "    * goal of training ML models: good future prediction\n",
    "    * best model on training data will memorize data, not generalize well\n",
    "    * for future performance, split into train and test set\n",
    "        o generally 80:20 or 90:10 ratio\n",
    "    * trained model is evaluated on test set\n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef77d3d-243f-4c0e-b1f4-6272defedc6e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfc662d-f609-49e8-a134-f6f97c1713d5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
