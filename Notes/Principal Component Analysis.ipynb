{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92cecbc0-57fe-48cf-b4ff-dbf84cde9224",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### High-Dimensional Data:\n",
    "    Each instance has many features\n",
    "\n",
    "    Problems:\n",
    "        redundant features\n",
    "        hard to interpret/visualize\n",
    "        computationally challenging\n",
    "        need large dataset to learn complex rules\n",
    "\n",
    "    Dimension Reduction:\n",
    "        feature selection\n",
    "            determine a few important features\n",
    "        latent feature\n",
    "             discover latent features (topics in documents)\n",
    "\n",
    "    Regularization:\n",
    "        The Lasso\n",
    "            regularization with lambda * abs(w)\n",
    "            favors sparse solultions\n",
    "                many weights == 0\n",
    "\n",
    "    Latent Features:\n",
    "        combinations of observed features:\n",
    "            provide more efficient representation\n",
    "            capture underlying relations that govern the data\n",
    "            \n",
    "        Ex. topics (sports, science, news, etc.) in documents\n",
    "\n",
    "        Methods\n",
    "            Linear\n",
    "                principal component analysis\n",
    "\n",
    "    Linear Algebra Review:\n",
    "        vector projection\n",
    "            two vectors u, v in Rn s.t. u = [u1 u2 ... un], v = [v1 v2 ... vn]\n",
    "            dot product = u . v = sum(ui*vi) = ||u||*||v||cos(theta)\n",
    "            scalar projection of u onto v is\n",
    "                ||u||cos(theta) = (u . v) / ||v||\n",
    "\n",
    "            vector projection of u onto v is the orthogonal projection of u\n",
    "                projv(u) = (||u||cos(theta)) * (v / ||v||) = ((u . v) / ||v||) * (v / ||v||)\n",
    "                if v is a unit vector, then projv(u) = (u . v) * v\n",
    "\n",
    "        evectors and evalues\n",
    "            usually normalize evectors s.t. for evector x, ||x|| == 1\n",
    "\n",
    "        singular value decomposition\n",
    "            take m x n matrix A\n",
    "            a singular value decomp of A is\n",
    "                A = UEV^T\n",
    "                U [=] m x m, E [=] m x n, V [=] n x n\n",
    "                U and V are orthogonal (i.e. columns and rows are orthonormal vectors)\n",
    "                E is diagonal\n",
    "\n",
    "        principal component analysis\n",
    "            when data lies on or near a low d-dimensional linear subspace, axes of this subspace are an effective \n",
    "            representation of the data\n",
    "\n",
    "            dataset X = {x1, x2, ..., xn}, xi has D features\n",
    "            assume data has sample mean of 0\n",
    "            if data is not zero-centered, subtract sample mean from each data point\n",
    "            sample covariance matrix\n",
    "                mean(X^T * X)\n",
    "\n",
    "            principal components are orthogonal unit-form directions\n",
    "                capture variance in data\n",
    "                1st PC = direction of greatest variability in data\n",
    "\n",
    "            projection of data points along 1st PC discriminate the data most along any one direction\n",
    "\n",
    "            steps\n",
    "                take a data points xi\n",
    "                scalar projection of xi on the 1st PC v is v . xi\n",
    "\n",
    "            2nd PC = next orthogonal direction of greatest variability\n",
    "                remove all variability in first direction, then find next direction of greatest variability\n",
    "\n",
    "            let v1, v2, ..., vd denote the PCs\n",
    "                orhogonal and unit vorm s.t vi . vj = 0 and vi . vi = 1\n",
    "            find the unit form vector that maximizes sample variance of projection\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c034a6ae-892f-44b8-b161-5278f235ddd0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Dimensional Reduction Part 2\n",
    "    PCA Review:\n",
    "        find direction of greatest variability --> 1st PC\n",
    "        remove all variability in the first direction and find next direction of greatest variability --> 2nd PC\n",
    "        repeat for nth PC\n",
    "\n",
    "        want to find the unit vector v s.t v1, v2, ..., vn are the principal components that maximizes sample variance\n",
    "        of projection\n",
    "            1/n (v^T * X * X^T * v)\n",
    "\n",
    "        Lagrange:\n",
    "            find v and lambda â‰¥ 0 that maximize (v^T * X * X^T * v) - lambda(v^T * v - 1) --> (X * X^T)v = lambda * v\n",
    "\n",
    "        v is the eigenvector of covariance matrix X * X^T\n",
    "            lambda is the eigenvalue, denotes amount of variability in that dimension\n",
    "\n",
    "            a zero evalue indicates no variability in that direction\n",
    "            only keep PCs with non-zero eigenvalues\n",
    "\n",
    "    Local Embedding:\n",
    "        an alternative dimensional reduction approach\n",
    "\n",
    "        PCA tries to find a global structure\n",
    "            issue: far away points can become close together\n",
    "\n",
    "        Local embedding aims to preserve local structure\n",
    "            low dimensional neighborhood should be the same as original\n",
    "\n",
    "        Three methods:\n",
    "            isomap: dense neighborhood\n",
    "            locally linear embedding (LLE): sparse neighborhood\n",
    "            t-SNE: probabilistic neighborhood\n",
    "            others: spectral embedding, multi-dim scaling (MDS), etc.\n",
    "\n",
    "    ISOMAP:\n",
    "        estimate the geodesic distance between faraway points\n",
    "\n",
    "        nearby points: euclidean distance is a good approximation of geodesic distance\n",
    "            assumption: points live on a low-D manifold\n",
    "            manifold: subspace that look locally euclidean (i.e. flat and low-D)\n",
    "\n",
    "        faraway points: estimate the distance by a series of short hops between neighboring points\n",
    "            build a nearest neighbor graph: edges connecting neighboring data points\n",
    "            measure distance: find shortest paths in graph\n",
    "                distance between every pair of points is the shortest distance between them on the graph\n",
    "                construct a distance matrix\n",
    "            find points in low-D space s.t. distances between points in the space is equal to distance on graph\n",
    "                HOW?\n",
    "                input: nxn distance matrix, denoted by M\n",
    "                output: n points in low-D space y1, y2, ..., yn in Rd s.t. for every (yi, yj):\n",
    "                    ||yi - yj|| ~ M(i, j)\n",
    "                let D(i, j) = M(i, j)^2 and assume mean(y) = 0\n",
    "                we can show:\n",
    "                    Y * Y^T = -.5 * (I - mean(1*1^T)) * D * (I - mean(1*1^T))\n",
    "                    where 1 is a vector of all ones\n",
    "\n",
    "    SVD:\n",
    "        A = UEV^T\n",
    "        if A is +definite\n",
    "            U = V\n",
    "            E is diagonal matrix of eigenvalues\n",
    "\n",
    "    Locally Linear Embedding:\n",
    "        Local fitting:\n",
    "            select neighbors for each point\n",
    "            reconstruct with linear weights\n",
    "                each point can be written as a linear combo of its neighbors\n",
    "                \n",
    "        Global embedding:\n",
    "            given nxn sparse weight matrix W = {w(i, j)}\n",
    "            compute nxd matrix Y s.t. it maintains local approximation\n",
    "\n",
    "        Limitations:\n",
    "            requires dense data points on the manifold\n",
    "            a good neighborhood is essential\n",
    "                too few neighbors: over-fitting\n",
    "                too many neighbors: results will not match local geometry\n",
    "\n",
    "    Stochastic Neighborhood Embedding:\n",
    "        local neighborhood distribution:\n",
    "            encode high-D neighborhood info as a distribution\n",
    "            intuition: random walk between data points --> high probability to jump to a close points\n",
    "\n",
    "            close points are neighbors w/ high probability\n",
    "            ex. gaussian distro\n",
    "\n",
    "        global embedding\n",
    "            find low-D points such that their neighborhood distro is similar to original space\n",
    "            measure distance between distributions\n",
    "                most common: KL (Kullback-Leibler) divergence\n",
    "\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c33023-8b1f-420c-9c43-56c03014c830",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa09df36-6352-47e3-a77f-e75d9542422e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b88e46f-3679-4624-be20-e68c726439a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2]),)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b604ad-4129-4dde-97d1-222f1b5948a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
