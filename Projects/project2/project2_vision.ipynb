{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "780305ee-d6f9-47ec-8501-d02d3f0050d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26421880/26421880 [00:21<00:00, 1218461.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29515/29515 [00:00<00:00, 151626.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4422102/4422102 [00:03<00:00, 1253059.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5148/5148 [00:00<00:00, 10699839.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Length of train data loader: 188 batches of 256\n",
      "Length of validation data loader: 47 batches of 256\n",
      "Length of test data loader: 40 batches of 256\n",
      "torch.Size([256, 1, 28, 28]) torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "# Do not change this cell\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainset = torchvision.datasets.FashionMNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=ToTensor()\n",
    ")\n",
    "validationset = torchvision.datasets.FashionMNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=ToTensor()\n",
    ")\n",
    "testset = torchvision.datasets.FashionMNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=ToTensor()\n",
    ")\n",
    "classes = trainset.classes\n",
    "\n",
    "valid_size = 0.2\n",
    "train_length = len(trainset)\n",
    "indices = list(range(len(trainset)))\n",
    "split = int(np.floor(valid_size * train_length))\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx = indices[split:]\n",
    "valid_idx = indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "validation_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, sampler=train_sampler)\n",
    "valid_loader = DataLoader(trainset, batch_size=batch_size, sampler=validation_sampler)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Length of train data loader: {len(train_loader)} batches of {batch_size}\")\n",
    "print(f\"Length of validation data loader: {len(valid_loader)} batches of {batch_size}\")\n",
    "print(f\"Length of test data loader: {len(test_loader)} batches of {batch_size}\")\n",
    "\n",
    "# Check out what is inside the training data loader\n",
    "train_features_batch, train_label_batch = next(iter(train_loader))\n",
    "print(train_features_batch.shape, train_label_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5b1f4b4-5c23-4e84-af74-fbf3f7404ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change this cell\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d98f1c46-9ce7-4aa4-ae58-5234cfe6ca48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJUAAAGiCAYAAADuqoTuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA9hAAAPYQGoP6dpAADceUlEQVR4nOx9d3hc5dH92d77rrTSqvdmS+6WbWzcsQ3GppkWTC8BEkpCQhJCQkhI+JKQjwQC5JdQQkuAOAkYTDEYjAsuuMlNliVZXVrtrna1vd3fH8q83G2yZEjy2XCexw9Iunv37t658847c+aMgOM4Dl/hK3yBEP63L+ArnHn4yqi+wheOr4zqK3zh+MqovsIXjq+M6it84fjKqL7CF46vjOorfOH4yqi+wheOr4zqK3zh+MqovsIXjv+aUT322GMoKiqCXC7HjBkzsGPHjv/WpXyFLxj/FaP6y1/+grvuugv3338/Pv30U9TX12Pp0qUYGBj4b1zOV/iCIfhvFJRnzJiBadOm4Xe/+x0AIB6PIz8/H7fffju++93v/qcv5yt8wRD/p98wHA5j9+7duPfee9nvhEIhFi1ahG3btqV9TSgUQigUYj/H43E4nU6YTCYIBIJ/+zV/BYDjOAwPDyM3NxdC4egL3H/cqAYHBxGLxZCdnZ3w++zsbBw5ciTtax566CH8+Mc//k9c3lc4CTo7O5GXlzfqMf9xozoV3HvvvbjrrrvYz263GwUFBad0rj/96U+YOXMm3G43jEYjotEoBAIBIpEILr30Uhw+fDjlNVdccQV+/OMfIxKJIB6P48EHH8RLL72U9vxWqxWPPPIIioqKIBAI8Morr+DXv/41kqMMmUyGiy66CAqFAhzHob6+Hr29vRAKhYjH4/jf//1f+P3+hNfk5eXhT3/6E9RqNfMcP/7xj9N6eKvVipqaGsjlckyZMgUvvPACfD4fe6j5yM/PR11dHcLhMHJyciAUCtHV1QWxWIydO3fC5XKxYzUazUm/4/+4UZnNZohEIvT39yf8vr+/H1arNe1rZDIZZDLZmN9DLpcjGAwCGPkShoeHAQAikQilpaWIxWL4+OOP4fV6MTAwgGg0ih/96Eeor69Pa1QTJ06EWq3G0NAQ9Ho9xOLMX1skEsHWrVvR19eHioqKjN7XZDLBZrNhcHAQQqEQ7e3t4DgOxcXFMBgMMBqNKUaVk5MDiUQCiUQCpVKJWCwGtVqdcm6BQICioiKIRCJ0d3fD6XRCoVAgFotBqVSy74NQXl4OuVwOiUQCuVwOmUwGk8kEhUIBs9mcYFRjCTf+47s/qVSKKVOmYOPGjex38XgcGzduRGNj4+c+f1ZWFq688kosXrwY8+fPx8KFC9nflEolVCoVcnNzceLECRw6dAj5+fkwm82QSCQZvV9+fj6Ghobwt7/9DRKJBOFwOOP7R6NRhMNhtLS0YHBwEMPDwyleChhZ7vV6PWQyGcRiMUKhECQSCftduiUmPz8fsVgMBw8exMcffwyO49Jei1QqBcdx0Gg08Pv9iEajcDgcAACJRJJyvFwuh0gkQjQaRSgUQjQahd/vh1KphF6vz/hZM+G/svzdddddWLt2LaZOnYrp06fjN7/5DXw+H6655prPdV65XI558+ahrKwMs2bNgkAgwJEjRyASiRCLxaBSqdjScdVVVzGv09fXB4lEktG1KxQK9PT0YPfu3Vi7du2oRhUOhxEMBtmNGRoaSnuc2WxGPB4HMPJQSSQSiEQiuN1uxGKxtF5bLpeD4zi89957UCqVKCgoYOfgQywWIx6PQ6fTARgx9EgkgtraWuzatSvhWJFIBIPBALlcDpfLhXg8jkgkgkAgAJPJBLPZnPGzZsJ/JU+1Zs0a/PKXv8QPf/hDNDQ0YO/evdiwYUNK8D5WiEQiGI1GLFmyBAUFBZBIJAgGg4hEIrDZbLBYLABGjEOpVGLHjh0oLCyEXq9P8FxarTbtzobjOHR1deHw4cMIh8OIRCIZryUUCsHv90MkEsHj8cDn86UcIxAIoFQqEQwG4XK5IBKJIBaLEY1G0d3djXg8DoPBkPI6j8cDjuOwe/dudHd3o7OzM+1yJBQKIRAI2OfhOA6RSAQNDQ0QiUQpx4ZCIZjNZmg0Guh0Ouh0OqhUKnAch3g8Pu4d9n8tUL/ttttw2223fa5z5OXlIT8/H1lZWcjPz4dKpYJGo4FUKoVEIkEsFoNWq4XVamXeiOO4hOVIIBAgHo/DbrdDo9Gk/QIHBwchl8vZTY1Go6NeV25uLrq7uzE4OAiLxYLjx48n/F0oFLK4LxaLMSMQCATgOA4ejwcajYYF7YTjx49DKBRCIpHg6NGj7HMmQyQSQaPRQCQSsc8Tj8ehVqshl8vZ+wAjXo1+jsfjiEajEIlELIWjVCohFApTgvvRcFrX/ubPn49FixZh+vTpsFqtyMnJQXZ2NpRKJXJycjB9+nTk5+czIyCDMBgM7CYKhUL2JarV6rSeavfu3cjJyWE36GT54unTpyMQCECr1WL27NkpfyfDCIVCUKlULPCPRqPQ6XQYGhpK6+GcTicEAgEqKyvh9/tRWlqadgMjFouhVqvh9/shk8kgFAohlUoRDAaZARMEAgH0ej08Hg/8fj8CgQACgQDC4TDC4TAMBsNJ81Ip7z+uo/+P4YUXXoBOp0NWVhb7cp1OJwYGBmA2m2E0GhEKhdDR0QFg5GnlOI4dS65doVAgEolALpenfZ/jx49Dr9dDo9HA6/WmjWP4IC9JT38yIpEIXnrpJeTn52PFihUQCATMc6nVajz33HNobm5Oea3dbsfBgwdRU1ODjz76KMVACEKhECqVCl6vF3K5HLFYDCKRCIFAgHkmAsdxiMVi6OjogEgkglKphEQigVgsRm9vL6RS6aifNR1Oa6OKx+NwuVwJW15CT08Penp6En4XDAZht9vx61//OmEJAAC9Xo/ly5enfZ+mpiYIBAJYrVYEAoGTXpfT6UQsFoPb7cbg4GDaY2KxGDQaDfR6PeLxONvSx2IxyOXytMYYj8exadMmXHrppezY5BgJAFvm9+zZA6fTyTYPLS0t0Ol06OzsZMdGIhF4vV6IxWIoFApoNBq2cXA6nZBKpSdd7pPxX6n9fV54PB62sxkPpFIpVq1ahfXr16csLwUFBaiursY777yTsryJxWIsWrQIe/bsQWNjIz799FPm/dJhypQp6O3thdlshtPpRFdXV9rjRCIRbDYbbDYbyyN1dnaio6Mj4400Go2YPn06tm7dirPOOgv79u1LOb/BYIBKpUJvb29CLKRWqyGVSjE0NJRgtJWVlXA4HBCJRMjOzkYsFkNPTw9EIhFUKhVOnDjBjnW73dBqtRk/O/AlM6pTASUSV69eDYFAgM7OTmzYsAEej+c/8v7JUKvVCAaDEIlEkEgk8Hq9GY8ViURYtWoVysvLsX//frz11lsZ40GxWIxp06axNMvw8DC2b9/OksiEsRjVab38/bshFAqxbNkyLFy4ELNnz4ZSqcTBgwdRUVGBV155BUePHv2PX9OUKVPQ0NAAqVSK9957D3v27El7nE6nw1VXXYVvfOMbEIlE8Pl8kEqlePvtt9Mu4SqVCtdddx0aGhoQiUSwd+9eHDx4MMWoxoLTevf374ZCocDUqVMBjMRonZ2dGBoagkajwfTp0/8rDAmJRIJrrrkG559//qixzs0334zvfOc7LLVgNpvx6KOP4utf/3ra6xYIBJBIJLDb7ejp6YFCoTjla/xSG5VWq0VVVRVLNEokEuTn57O/63Q62Gw2qNVqVkOUSqWwWq0oLi7OuFsERpae3NxcTJgwARUVFcjLy0sbVAsEAmRlZY1r2y4SiRCPx9OeDxhZIs855xyW56Jzi8ViXHbZZcjKykp5TTQaxcDAAOx2O0QiEZxO55g2JelwRix/FosF+fn5rFDd3d09arJOrVajtrYWjY2NrDTz1ltvoba2FmVlZfjRj37EirVmsxkdHR1wuVyIRCIsC26z2SCTyTJ+8QaDATU1NVi4cCE0Gg0ikQh++ctforu7GxMmTMDx48cRCARQUVGBiy++GLt378b27dvhdrtHTVmUlpYiEolAIpFkLKFkZWWhuLiYGRUleAUCASwWCwwGQ0pBPxKJoKWlBX6/HwqF4pSXPuA0NyqhUIj8/HycffbZcLvdyMnJQW1tLXp7e/Hqq68iEolAKBSir68PcrkcJpMJRqMRkyZNYglBqnvdcsstCIVCGBgYYBlknU6HWCwGj8cDm80GuVwOt9sNAGwLzq/tCYVCqNVqFBQUoK6uDkqlEh9//DGWLVuGY8eOwWw2Y3BwEF/72tfgcDgwNDSE3NxcRCIRlJeXo6KiAm1tbWhubobT6YTT6UxZ4urq6gB85uHSQa1WQ6FQsBINx3EQiUQs0Wu1WlPYE7FYDD6fD263G4FAgNF8TgWntVFVV1ejoKAA3d3dEAqFOHLkCPbu3ct2h1KpFGKxGG63G5MmTcLZZ58NsViMcDgMpVLJaCBOpxOHDh2CUqlEf38/2yGZTCYMDg6ynFYsFoNUKkUoFALHcQm0E4FAgIqKCqxatQoHDx6Ey+XCnDlzcP/992P58uXw+Xzw+/0wGAxsudNqtRCJRBCJROA4Dn6/HwUFBSgrK4NOp0NzczNeeOEF9h5CoRBWq5VVA0pKStJ+LzabLSExSuUaqiDk5uamvCYWiyEWiyEQCLDP+qU0qoMHDybwn5Lrenx89NFHOHr0KORyOXQ6HVwuFztWJpOx6jz/HFT0pfINeTBaTigzn5WVhYqKCsybNw+lpaU4ceIEJBIJS0WEw2EYjUbU1dVh27ZtcDqdzKipRBOLxRAMBhEOh6FQKOD3+9Hb25vwGYiKQkaSyVPl5OQk/ExxVSAQgFKpTFusJhoNebTR4sWT4bQ2KgBjfpqCwSBOnDjBvjQg1QjJWPjnDAaD2LRpE0pLS6FWqxEIBNDa2soKrQDg8/lgt9uxfv165h1jsRi+9a1vseKvUCiEy+XCwMAA7r33XlYuSd6J0TVFIpEUig3xwcioVCpVSmUAGCEmUibfYDBALBaD4zi43W6EQqGMpZfCwkIYjUbI5fLPtfs77Y1qvCA3PxYIBAL09PRg48aNePvtt9kNVKlUmDRpEjMIn883rpxVciPHWEGsBIFAgGg0yn5ONj6O49DU1IScnBwYjUbmqVwuF7xeb0bqjlAohEwmY1ScU8WXzqjGAwrKyXORR6DgPR2T4N+JQCCAo0ePwmg0YnBwEMeOHUt73Ouvv44DBw7g+9//PqMOcxwHn8+HX/ziF2hvb0/7usHBQcTjcYjF4lE5YyfDGVOmUSgUUKvVEIvFkEgk6O/vH9UbiEQiyOVySKVSDA8Pp00k0lKZ7gumckny60QiEUwmE5xOJzQaDcLhcEbjoyK1RqNhxWGHw8Gov+mgUCggEokQiUQQjUYzel2KufgeJxKJwG63Z4w7yQvSsp4upfClKNOIRCJUV1fjsssuY9ljlUqFXbt2Yf369Whra0s4nmKR22+/HfX19VCr1XjppZfw2muvpXyJoy2V6WpuVVVVuPLKKzFx4kS43W4olUq4XC6sX78er7/+eoIBGo1GXHXVVbjooougUqmwZcsWzJw5EwMDA3jppZfw8ssvpzXmsSYkOY5LyUWdDJ/HO/Fx2mfUS0pKcN555yESiaC5uRm7du3C7t27odVqcc455yQEnGKxGCtWrMBNN92EQ4cO4cCBAzCZTIhEIrjnnntQUVGRcn7K95wMSqUSd999NxobG6FQKJCXlweDwYDCwkLccMMNaGhoYMeKRCJcf/31WLt2LeNR1dfXQywWIzc3FzfffDMWLFjwhXw//w2c9p4qJycHGo0GfX19LMdE/HS1Wg2j0Yju7m4AI7uiNWvWsNaj9vZ2nDhxAkajEWVlZbj66qtx3333Me8kl8tx5ZVXssQg/Z52aF6vF1u2bIHP50N5eTlmzJjBuF2UcqDE45w5c7B7925wHAeTyYRp06axfBfFbB6Ph+XBFi1ahPfeey+jp6RmjrFirMdLJBLMmzcPdrsdg4ODUKvVCIVCGeOwdDjtjUoul8Pn80Gr1aKvrw8cxyEQCDDiPj9BqdfrIRQKUV5ejtLSUrz00kuoqqqCy+VCc3MzAoEAu8FE250/fz4UCgXEYjGLTyj1EA6H4XK5sHPnTjidTnz88cdoaGhg+aBIJAKpVIr+/n4cPHiQXUdRUREUCgV8Ph/LekejUbaTi8fjMJvNkMlkKb1/SqUSV111FWKxGP75z3/C5XLBbDazeCkZCoUC06dPx9y5c/HMM8+gt7cXU6dOxeHDh9lGhEAGNXPmTNTU1ECr1SISiWDbtm14+OGHx3xPTnujkkgkaG1tRVVVFeLxOIaGhiCRSOB2u1nOiKBQKLBnz56E7hlqiZo7dy527drFbqxMJoNer2cGRTee4zi2HFJQDoy0g//gBz/ASy+9BK1Wi3A4DLVaDb1ej9/85jd499132XVotVpGbebnqfx+P1QqFdxuNziOg1arRTAYZA2kBQUFWLVqFUpKSuB0OvH9738fQ0NDUKlU2L9/P5599ll2LqlUioqKCnzzm99EfX09IpEIjEYjfD4f6uvr8dhjj2HDhg0sVyaRSLBq1SpYrVb84he/gFwuR3FxMc466yxUV1dDKpWO2prGx2lvVEKhEHa7HWVlZVCpVIjH44jFYizo5MdUUqkUUqkUW7duRUdHB7xeLxoaGlBUVIR9+/bBbDbDarWio6OD3fTkf+RZyKPxg2/ajXEcxxobBgYGUryNQCBAMBhEZ2cnTCYTK/ZyHIdQKITW1lbk5ORALpejoqIC11xzDesMoix+bm4uq/HJ5XJ0dXVBKBTCaDRiwoQJOP/881FTUwOVSgWFQgGTyYSlS5dieHgYYrEYJSUlkMlkWLBgAbRaLbKzs1FRUYHp06fDYrHgzTffxI9//GNkZ2ejr68PCoXiy2FUVCCl/xeLxazkQp22fE+l0+lQUlKCbdu2Yf78+fjwww/xu9/9DhMnTkRfXx+ysrJYXiccDjPuNnXvkmch4yID44O25AKBIMG4+RCLxfB6vazORs2f9Hr6G53HZDJBp9MhHo+jo6MDkUgE9fX16Ovrw5EjR6BWqzF9+nTs3bsXN910E2ukpe4Z6qyJRCLsO+rv70d1dTW+8Y1vsBxWdnY21Go1li9fjgkTJiArK4t56PHUAU97o+LTOshTkTGFw+GUkkReXh5ycnJQXl4Ok8mEn/zkJ+ju7mZdOeRVZDIZCgsLWYCrVquZF+R7rnReiF+8pXYsPkKhEHw+H1wuF3Q6HUKhEFsGKW8WDAYRj8dx+PBh3HvvvSgtLUVZWRmGh4cRCATw97//nXkupVKJ48ePw+12Y9u2bRCLxawNLRQKIRAIJATpwWAQ+/btQ2dnJ+6//35MmTIFNpsNx44dQ2trKzZt2gSpVIrrrrsOANDV1TVmLwWc5kZFPG2fz4eDBw/C4/EgEAjA7/ezpYDvqQKBAP72t79BpVLh9ddfx+TJkzF37lzU1dVh+/btKCkpgVarhdPpRF5eHg4dOoRXX32VxTatra0J9UKO49DS0sLOTwbONzo6jg+6yX6/nwXmAoGAsRWCwWDCEmu322G327F9+/aTfifvv/8+Pvjgg4z1zeTr2bFjB3bs2JG2p/Hb3/52xnOMhtPaqMRiMVQqFQYGBsBxHLq7u9nSVF5ejkAgkFBtl0ql8Hg88Hg8qKurQ29vL1QqFcrKyvDqq68CGOFq2+121lU8npqeQCBAOBxOCOjpHx+Dg4OIRCJwOp2sx45vRP39/SgsLEzZnY0VRF0Z72uS8aWkvoTDYRw7dgx9fX0YGhqCUqmEVquFSqXC3r17YTAYoFQq2fEHDx5ky8SuXbtY3PP000+ju7sbTU1NKb2C40EwGMT69evZ0kMlnmTD7OjowHvvvYfW1lYAYHQa+v9du3ZheHh41E6Z/8s4Y2p/lZWVWLBgAWQyGdrb2/H3v/894+vlcjlqamrYTmh4eDilR44PkUiEefPmoaqqCu+//z6OHj2acTmorq7G9OnTceLECWRnZyMajeL1118fV0ySCdRNfe655yIajeKNN94Y03lJIyKTrBEfubm5KCwsxP79+9PWLL8UtT9CQ0MDpk6dCo1Gg7q6OmzYsCFtQVSv1zNNqqVLlyISiWDLli1QKBRoaWlJ+dJLS0tx5ZVX4rzzzkN2djZWrVqF1157Da+++mrawu/XvvY1XH/99RgaGoJQKITf78fmzZu/EOXlkpISfOc738GECRMQi8UwZcoU/OpXv4LT6cz4mpqaGkycOBEcx+HAgQM4dOhQxmOVSiUsFgs6OjpQV1eHHTt2jCuWIpz2tT+CTCaDVCplAl6ZiGg5OTkQiUSw2+1466238MEHH8Dr9UKhUKSIXYjFYtx+++04++yz4fP50N7eDrlcjksvvRRr1qxJe36FQsFEL9RqNeMnnQxCoRBTpkzB6tWrUV1dDb1en0LgW7hwIWpraxkfa9asWTjrrLMynlMsFqO4uBjr1q3DunXrUFxcPKo2gsViQWtrK3p6ehCLxU5J8Aw4g4yKVOiSKbh8UJu5QqFIEBwTCATQaDRQqVQpxxcVFTENp0AggGAwCLlcjpKSkpQAXCgUMi0COr9IJEoroZgMk8mEq666CpWVlbjhhhtw6623JtB+RSIR8vPzIZFIWPAvFovTUoopt0b/wuEwMxIy+nTfDeXIOI5DR0cHysrKTqm38YxZ/oARBub+/fsxZcqUtDsXmUyGrKwsFj/pdDp4PB6IRCIUFhamFE0NBgPMZjMUCkXCjs5gMKC8vBwqlSpBP5PyU5QsJYM9mV6pTqfDFVdcAaPRiHA4DL/fj6ysrIQbKhQKmcwQ7Wj9fj90Ol0CpVgikeD666/HhAkTEAgEYLVaMW/ePAgEAtZJFAqF8Morr2Dv3r0J10AFbmBEZGT69OmswD0enDFGJRAIEAgE4HQ6WeIwGbREknQQFUx9Ph9MJlPKMpWdnQ2xWIzOzk4YDAZmJE1NTSgsLITJZEowqlgsBq/XmyBgRso0yddKOan8/Hxcf/31yM3NxfDwMCwWC0toGgwGFrfRa5xOJ2w2G4CRXWQyT33ChAmw2WyYMmUKBgcHIRaLYTKZIBaL4ff7WX8kfyMjEAhQUFCQkHOLxWJwOBwwmUzj3hGfUUbl9/vh8/lSCrUEEgNzOBxQKBQIBoNQKBRwuVxMMpEPpVIJv9/PjIqWu8OHD0Ov16O8vDzBu1Fhlv8zv9GCfrdy5Urk5uZCpVIhLy+P7ab0ej38fj+8Xi9TKOa/TqlUwmQyseWPHhL+ZzUajWypo4w6xZr79u2DTCZDTk5OQscNlZu0Wi1bqjmOY6yP8eKMMSqRSASXy8Wy6ZmSedSKZDab0dfXxzxDf39/SppCp9Ox6j4hEolAr9czWgsfFFPxSzUikQh6vZ5pQqnValx55ZWsu4ckfIiWo9FoWEdLQUEBGwRFn4c6quPxOIqLi1PoLmKxmD0cJB1E8VdRURGUSiXkcnmC+rFQKMQNN9yA6upqeL1e9Pb2orCwEBKJBH/+858T2A9jwRkTqFOHyWglBWqLJ9U8mUzG5BH5upsEvV7Pmj/pPQCgrKwMZrM5JUdEVGZ+3U8sFicYq9frxeOPPw6v14vq6mpUVFSgoKAAKpUKwWAQYrEYUqkUDocjIfkZDoexdetWVpqi6928eXNCfi0ej8Nms7EOGgrMqfeQjJ6vflxYWAiNRoOsrCzE43Fs2bIFOp0Oubm5aQXlToYzxlOJxWL4fD5EIpGMLdtUcCb1OhLNIAMh4j8ZpdvtZoMBKJ6iBGS6AFYoFCYIr1Ixmi+lzXEcPvjgA2zZsgX5+fnsJg8MDKC/v5/t0IaHhxPKNCKRCBUVFczzhEIhKJVKFBYWJlxzc3Mz+1zUW0gele89+eWrvr4+OBwOxONxVFdX40c/+lHGeHBM92Lcr/g/ChLOIDHUdEY1NDSEXbt2obe3F2q1GkqlEoFAAF6vl2l68r1cW1sbPvzwQ2zevBlGo5FxpPr7+zFnzpy0TMu+vj50d3ezQnZ/f3/a5GQ4HE5RLQaQUc6RDL61tRVarZaJriWrB/f09ODIkSNMonFwcBB+v58ZSSwWQygUwv79+9m5fT4fdu7ciWnTprH+QKoynIoG1xlTpiEFuEAgwDjro4Ef92QSfOWnB8hj0QSFZDlqglwuZ13BwMi2f7SWq/GAlj7Ks4VCIcZy4IN2uPF4HH6/P+Xv6W451U1jsRicTieLP5PxpSrThMNhJgU9WtkCGOFUkcAXMLINP3DgQEoBlwhqJpMJFRUV6OrqQkFBAZMBSoZIJIJOp0N1dTVqa2shEAjQ3t6O/fv3o7Ozc9zMgWRQy9jJJH5Irno88Pv9KdywU8UZY1Qcx2Hy5Mno6+tDS0sL66BJhlKpxNq1a1FfXw+5XI5IJAK/34+nn34a77//fsrxMpkM1dXV2L17NwKBAKMuHz58OMFIBAIBkwMSiUTo6emBUCiEXq/H5MmToVQqcfjw4VPaov+nYTAYMHXqVHz44YeIRCKMJzZWnDG7P2DE46jV6lE9lVwux5IlS1BeXg5gZITZhAkTUFlZmfb4goICHDlyhHmmYDCIQCCQIjgmk8kY5cXn82FoaAhut5vprhsMhpNm1uVy+efSMMgEoVCIwsLChG6g0TB58mR85zvfwbPPPounnnoKl19++bjKNWeMUUmlUpZNTpbS4UMul7M4x2KxsFEe6eR1qKMmOSZqb2+HwWBIyMArFApotVpUVFTA5XLB4/HA5XKhp6eHzcDJVAMsKirCbbfdhieffBLnnHMOBAIBJk6cmFEqaKw3mFrf77nnHrz00ktYvHgxqqqqcMUVV6CsrCzja6ZOnQq5XI7q6mo0NjYiPz9/XMZ+xix/8XgcbW1tkMvlo05+qq+vR1FRERMzo4p/ZWUlJBJJQqOCTCbD+eefj3g8jgMHDrDWrenTp2Py5MlobW1lS2AsFsPw8DDi8ThKSkpgt9sRjUZZuqKvry8lDhOLxbjwwguxfPlyzJgxA1KpFHl5ecjLy8PixYvxq1/9ilFmhEIhSktLIZVKGZtgeHg4bQlIJpPBZrNh2bJlWLNmDcxmMzweD371q1+xfsVrr7027fdjsVhQV1fHGmg9Hg+6urrG1RJ/xhgVbblra2uxadOmjMeJxWIcPHgQKpUKer2eddD09PRAqVQm5IZ0Oh3Ky8tx3333oaWlBS0tLaisrER+fj6LmQgejwfbt28Hx3Ewm80IhULsxhw+fBhHjx5lcYlarUZdXR1Wr16NlStXQiKRwOFwoL+/Hy6XC3q9HocOHUJJSQmamppgMplgMpkQDAZZYXjixImwWq146aWXMDQ0hEAgAI/Hg2uuuQYrVqxAZWUl5HI5BgYGcPDgQWg0GpZfA5CyaRCLxSgsLMTatWuZCiF130ydOhUvvPDCmOOqM8aoOI5jynejEeLeeustuFwu/OY3v4FQKEQ4HMbBgwfxwAMPpHgSorHEYjFMnjwZ9fX18Hq9GBoaYoMn+TuxeDwOj8fDlPf46Qq6IUKhENOmTcOMGTMgEonw0Ucf4W9/+xs6Ozuh1+uxcuVKNnCcxrZZrVZotVq2BEUiEbS2tqK9vR3l5eUsjtu5cyfq6uoY43RwcJDFdTt37mRqgceOHYPb7WYPIU0hnTRpEqZMmQK1Wo1wOMy8XmVlZQojYzScMUal1+uxZcsWXHzxxaMS0ah9i99TRzqayZDJZGwejUwmY6rAFD+lG4smlUqRnZ2NQCCAUCiUVpVu//79aGlpYZ5OpVJBIpEgEAjgueeeY691u93w+XzYtm1b2m6XdNf88MMPQyKRwGAwsPk4JDspl8uZvFA4HEZubi7uueceptpHYrkksUSlnng8/uU0qqKiIkybNg0ymeykwyhzc3NZJjoUCsFms0EqlabkaYg/RcZDjFIK9tPlgkjHUyqVslJNcpLV4XAwZgTfMPgGk1zDTJeKSPc7GizJnyfDbxlra2tjDR8ajQZvvvkmpk+fzuqP8XgcTU1NaGtrw+TJk9n3Mp4Z1meMUZH3OXToUNryCR9WqxVSqZTd8EwZb6rtUZGY6mhUqsk0cYG8E9Xe0iU9T7X9aSwYrUWL/77Dw8P4y1/+gldffRVGoxFKpZLNWw4Gg8jOzmYhwskSynycMUZFqr8ajSbj1CrCp59+invvvZctgWq1Ou1NaG9vx/PPP4+ioiLEYrGEYZQtLS1pGZGtra0QCASIRCLsNb29vf9nk55UNUgXh56s1JUJZ0ztDwDmzp2LSZMm4e9//zs6Ojoy3kiBQIClS5fCbDZDIBCgubkZn3zySdpjpVIpiouLE5Rfenp6vrCePKVSicbGRiaysX///i/EANVqNVMK7Onpgc1mg9PpxJ49ezKmB2bPno3S0lK2c/3nP/+ZssR/KWp/YrEYkydPxsKFC7F69WoUFhZixYoVeP311/HWW28lUGQJUqkUDzzwAJMfWrduHdsdJcNms+G3v/0tm/Dp8/nw0EMPYcuWLZ/72hctWoTLL78cFRUVrPi7ceNGPPvss5+rpUun0+F73/seqqqqmD5WdnY2vF4vPvroIzzxxBNpDevcc8/F8uXLEQqF0N3djY0bN55Sv+Jpn1Gvra3Fb37zG1xyySWQSCTo6uqCxWLBddddhwcffDClQwYY0YfS6XSMREcjP9KBdkvECOA4bsxT6XNzc/G1r30NFRUVKYGuXq/H7bffjvLyclbikUgkWLp0Kc4777wxnV+lUqX9fCUlJaiurmbTSy0WC9v1zpw5k2lq8SEUCpGTk8PEcWUy2SkL9J/2RjV16lRG2aDdFb8JoLa2NuU1tJWn7XbyXGE+cnJyEjSpOI4b8wDLpUuX4kc/+hHWrVuHv/71r5gwYQL7W1ZWFnJzcyESieB2u9lg7Gg0itLS0pOeW6FQYMaMGVi2bBkmTpyYkN4oKSlhGwWVSsUSn9SKlq6fT6lUwmg0MnIfDXc6FZzWRkWJRJ1OB7lczhoPSJNJpVKlnd9CQ30OHjyI9vb2UefmGQyGlO39WLbXarWa6UQplUpMnDgRBQUF7O+U6yIJa9qJEl99tAZUgUCAhQsXoqCgAFKpFDNnzsTkyZPZ3+n11HNIPCx6ePice/53STth/ibjVHBax1QikQhvv/02urq6cP755ycUPY8ePYo333wzobeNQPUvt9sNu92ekaEAgD291BVDZZhMJD2atH7JJZdg1qxZCX/jGziVTLxeLzs3cdtJdyodv4k6cGQyGUKhEFQqFVs+CaQnHwqFYLfbEYvFIBQK2RKYbqmneNFutzPF5FPlV53WRhWJRLBu3Tp88sknWLJkCbRaLVvGBgYG8OSTT6Z92khZ2OfzYXBwECUlJRkNxGazsfwU8eBp2aLXUHNDRUUFGhoasHDhQkY1IT2Fzs5OzJgxA48//jhisRjMZjNbavgdxeTZ+A+IQCCAWCxGdnY2rFYra9u3Wq0IBoMYGhpK6CUkPXkALHEpkUjQ2dmJkpKStJWAYDAIu90OjUaDzs5OFBQUsJhsvDitjYrA1+Mkb0Jf8GivIcI/BeDJIKFYqv9Fo1G2fPCLyRdffDGuvfZaWCwWlqkn3pVEIoHf72eqK1KplKkn02Ak+i9NcqAlHBgplOfk5EAqlbJ8mtVqZQE68bWoVgeMJF/J4Inew3EccnNzU7L4BPJURUVFrAGDplqMF2eEUZG2JrVpjSbQAYwYlNPpRFdXF7xeb9phjcCIh5DL5ezJpibV5AbOd955B8FgEHV1daz1ye12o7u7m80fJJEzKlpTDJjs8ZLHltTW1qK4uJhx5KPRKPx+PzweD+LxOCQSCWs3o+4dOrdQKGQqzfzmiEzDBjo7OzFz5kwUFRV9rvk0Z4RRUV2NGhLIo2Ta0cViMRw7doxNlToZX4gEa8ljUbsWweFwYN26dfj73//Olh0SMuO3T/G9IV+cli+jHY1GE5oZmpqamDw3FZ5pmSREIhG43W6Ew2FmmGQUOp2OnZ88YiYcO3YMarUaWq0WXq93XPU+Ps4YoxoeHoZKpWJGNTg4mHH547iRuS1EBRkaGspYsB0eHk6YRBWNRjPWFunv/J8zobe3F//4xz+QlZWFoqIiRKNRDA8PY/369aiurk6Q1Ha5XGPuv4tGozh69Ch0Oh3KyspYbBaPxzE8PAy73Z5xegPxuUQiEYaHh09J8QU4Q8o0IpEIFouFBbSkRz5aYdlqtSIajSIajbLgNB2ICkLJScopjTZ4iLwT30slg3Z8SqWSdRQPDQ1hYGBg1IHfY4FCoWCT6ylfRTVIl8uVVsCENLXowRSLxWhra0u5/i9FmQYYWWokEgmuueYacByH/fv348MPP0x7LOmEBwIB9qVVVFRArVYnjM4FPoup5s+fj9tvvx0KhQJ///vf8eKLL6K7uzttv11VVRWUSiXq6+vR0dGBnp4edHZ2plT5qd3K7XanCMYmG5Rer0djYyN8Ph+Lnfr6+tjPSqUSvb297MEIBAKMk3XZZZdBp9PBbrfjt7/9bdo0QV1dHS6++GJUVVWx9rP8/Hzs3r0br7zyyrgYCsAZYlQ5OTm44YYboFQq8emnn0Kj0WDOnDl46623UmKIhoYGnHPOOUxxj5aXCRMm4H/+538SbuiCBQtw7733oqSkBD6fD729vTj//POxZMkSvPjii3jmmWcSzl9RUYHCwkIEg0E2b1Aul6Oqqgp79uw5Ze9z1lln4c4772QdQBzHMfF+SkU8//zz+P73v89eIxQKUVdXB7VazVgSVqs1ZRnVaDT49re/DZvNxtizROo755xzoFar8etf/3pcRe7TOqNOqK2tRV9fHxu8LZPJYDQa09bFSGfA6XTCbDazJKHVak1YUpVKJW666SYUFhay5sxQKASXywWhUIgrr7wSRUVFCefOycmBTCaDWCxmin7kRcda2kmH3t5eNuybanI6nS5hjEhyAbqqqgr19fXM83Ach/nz56eUaKhcRMxWGoUSjUah1WpRUlIy7hrgaW9U1BXs8XhYjDJt2jRWBuFDIBCgpKQEOp0OEokEHR0d8Hg8yMrKglarZWJiwEilv6ioiO2caIBRLBaDQqGAwWBAfX09O570orKystiuiVIber0+Y8Garms0CUfiiSXXNin2AZDigejaSKJILpdDq9UiNzc34Tir1cp0u4DPSHwCgQA6nY4JhowH4zKqhx56CNOmTWOyM6tWrUoRcAgGg7j11lthMpmgVqtx4YUXpkzI7OjowIoVK6BUKpGVlYVvf/vbp1xnonwMbbXj8Tjcbjc0Gk1K/x+pylHmnVIQIpEIGo0GxcXF7FidTgedToeOjg72PtQv6HQ6EYvFEgyFRnfo9XpIpVLWqKBQKFjgnAyZTIaysjKsWLECd9xxB+bOnYvGxkacd955CcXnEydOsEQqP6VAaQhKdfChUCig1+tZC5rL5YJGo0kxXmqAPXLkCNvoUA2Vuo7SXftoGJdRffjhh7j11luxfft2vPvuu4hEIliyZEmC3vadd96J119/Ha+88go+/PBD9PT04IILLmB/j8ViWLFiBdNbevbZZ/HMM8/ghz/84bgunEDDgCgbHYlEsHHjRtjtdthstoREHynRyWQy9pSTFzIajQlLlNFohFQqxeHDh1neKxaLQSaTMZIeP49DATPlpjQaTcJ0dplMlrBF12q1mDt3LtRqNaLRKAoLC1FbW4uuri5s3rwZzc3NCd9ZPB5nA4z4OTgyJv65qbLgcrmYrFIoFILD4Ujx3jRIqb+/n72O4rS+vj7W6DEejCtQ37BhQ8LPzzzzDLKysrB7927MnTsXbrcbf/zjH/Hiiy+yca1PP/00qqursX37dsycORPvvPMODh06hPfeew/Z2dloaGjAT37yE3znO9/Bj370o1Ez4elQUlLCnr5wOIxgMIhQKIRjx46hoaGBdY4Anw21djqdrJxDtTmO45gEYzweh8lkgt/vR19fX8KUUY7jEIlEMDw8nFDElUqlUCqVTFrR6XQyzrdUKmX8LRoQabPZ4HA40NPTg4qKCnR0dMBsNrMZgTQWhT/pVCKRpJ0TmCwLKZFIWMMCtZFRmiLZQLKyshAOh1ncxTeswcFBpvo8Hnyu3R9thYlKsXv3bkQiESxatIgdU1VVhYKCAmzbtg0zZ87Etm3bMGHChASi29KlS3HLLbfg4MGDmDRpUsr7kAsnkDYT8FnJgbwGeYfOzk6mKUVGRZPgPR4P05qigm4gEGA1vXg8zryYRqOBXC5HOByGXC5nScpkCR8ar0Z1NvKc9F9+dp3KNhQPbdy4Ea+//jozHFqe+Z+Zptm73W6oVKqEcyaHDvQ5iWxHxhiNRlOCbp1OB6fTyd6TllWRSMTyWSfLSyXjlI0qHo/jjjvuwOzZs1FXVwdghChPgSkfNIiQjklmTtLPmYj2Dz30EH784x+n/Ru/4EmDDrVaLUvyqdVqtjxTgEvLGOl28ou/5AGI6ckXbdXpdJDJZBgaGmIt8GQoJDK2efNmhEIhltn3eDyoqqpKUNNLfkj4CIVCKfx3kpEkD8WfPQh8xpIgEH9KpVKxzxWJRJgn5YM8EvH1yagoKUzF8PHglI3q1ltvRVNTEz7++ONTPcWYce+99+Kuu+5iP3s8HuTn5wMAm53s8/nY0kA/9/X1JVBIvF4vWxKoNkccI5IiJO9DKYSenh7m0VwuF6PNRCIRpnXO7w/s7OxMabq02+1Qq9Wn3NCwaNEiTJ8+nb2e3xALgOWgCBRw83d0xIFPXv6EQiH27NmD+fPnJwweJ22I0Wb2ZMIpGdVtt92GN954Ax999FGCyq3VakU4HMbQ0FCCt+rv72cf2mq1MsVd/t/pb+lAFfp0OHToEJs6lYy//vWvCYVij8eDf/7zn2zuMH158XgcRUVFCZpTzc3NeOqpp/Daa6/h/fffh9lsRjwex+DgIDo7O9HQ0JBgKH19fXjnnXfSZqy7urrgcrlOWfSsqakJDz30ELKzs9HS0gKXy8XKL8CIke3cuZMdHwqF0NTUhI6ODgQCAWb4IpEoZZXYsmUL2tvbsWHDBsYSpXMODQ1h/vz542/V4saBeDzO3XrrrVxubi7X3Nyc8vehoSFOIpFwr776KvvdkSNHOADctm3bOI7juDfffJMTCoVcf38/O+bJJ5/ktFotFwwGx3QdbrebA/C5/pWXl3NXXnklt3jxYk4gEHzu89E/hULBVVVVcTNnzuTmzp3LlZWVcXK5/As7/3/7n9vtPun9GZdR3XLLLZxOp+M2bdrE9fb2sn9+v58dc/PNN3MFBQXc+++/z+3atYtrbGzkGhsb2d+j0ShXV1fHLVmyhNu7dy+3YcMGzmKxcPfee++Yr+PzGJVQKORsNht3zz33cOvXr+d+9rOfcbW1tZxUKv1cX7ZAIOCqqqq4s846i1u2bBn3z3/+k3vjjTe45cuXc3PnzuVKSkr+K0Ygk8m4oqKiL+zBGYtRjYulkClge/rpp3H11VcDGAmc7777brz00ksIhUJYunQpHn/88YSl7cSJE7jllluwadMmqFQqrF27Fj//+c/HLKxFLAWTyYQ5c+awVAIJTfCTocePH09gIDQ0NODCCy9kAv1dXV2QSqU4cuQI/va3v6WNe9LpHSQzEDQaDaZOncomk2ZlZTFRVsqsb926ddxzXj4PBAIBJk2ahLKyMuzcuRNtbW2f+5xfOEthLPYnl8vx2GOP4bHHHst4TGFhId58883xvHVa3H333ZgzZw66urrQ29uLQCDAAlSr1Qq1Wo329nZ897vfZTzu+vp65OXl4ejRowiHwxCJRJDJZJg8eTLefPPNhKLv4sWLMX/+fLbL47ezi0Qi9Pb24o9//CM8Hg9LdlIQTZV9yjmJRCIoFIpRjUqr1TIO12igZCrtfPV6Pcxmc0rjrE6nQ05ODt59911MnDgRJ06cGJOGA+1k5XI5NBrNqByydDitWQrEbJTJZLBYLGyiFNFnSRInJycHQ0NDAEaYBAaDgfG8yRCys7OhVquZUclkMqxevRqzZs1ikjpAqrfes2cPNm3axAaBy+VyNgGUsvA5OTlsWlcyiF5TXV2NhQsXIhAI4OOPP4bNZsPu3btTgmSJRILp06ejsbERb775Jnw+H2644QYcPHgQLS0tzOBMJhNmzJgBlUqFqqoqnHfeebDb7Ww0cCa57mnTpjEGR3Z2Npv3Mx4ncFobFWXFFQoFK7vE43FIpVKWOzp06BB7gqkf8MSJE9Dr9dDr9bDb7WhubobVamW1PJPJhO9973uorKxkO0TaRVJikJZY8khdXV1sUGPyDmvr1q3o6OhgSVgqgufn52Pq1KnIyspCeXk5619cvXo1OI7Dgw8+iL6+PohEIiiVSlRWVmLOnDnIy8uDRqPB9773PfbwGAwGvPLKKygsLMSDDz6I/Px8llKgfNy8efPg9Xrxwgsv4Omnn2a7UXoozzrrLIRCIRw+fBiTJ0+GVqtFT08Pdu/ePa77clobFSUtqcZHyzO/R+/gwYPMIKhbt6WlhSnhUUOARCJBXl4eTpw4AZfLhQceeAD3338/Fi1axM7F/SsfFY1GWVIx+XrIcGi5pGIvX5Ng3rx5mDVrFitaEx2HrpNkGPV6Perq6rBo0SLo9XpWudBoNKzeSclKs9kMlUqFVatWoa6uDj6fj+XV4vE4S4hqtVrcfPPNeOutt9Db28va/vV6PSoqKrBp0yZ0dXUxAbStW7eOuwHitDYqeoIBMKI/f3J6PB5PkBXS6XSM1E9lCyq40sw/eh1lnqnjhf5fIBBg9+7d+OCDD3DrrbemfOHpaLrJaGpqgsvlgs1mYwVuSlBSCScWi2H37t2oqalBbW0ta/AkrtPQ0BCkUikrGdHG5ayzzmItW2TQTqcTFosFwIjBWq1WTJw4EWq1Gg0NDWzYksfjQUVFBaqrqxEMBtHc3IzzzjsP4XAYH330UcYRJ8k4rY2qo6ODjR/j/tV0QCWZeDyOlpYW7Nq1ix2vUCgYJYaeXpqNR6UbAnXc0BIIgMVJJpMJ559/Po4fP57CY0rX/pQcSw0MDMDhcKC5uTmhYTN5dEcgEMDAwAA6OjrQ0NAAjUYDoVDIiu5U0xsaGsK2bdvQ1dWFcDjMeg9ph5qTk8My8DQxFQCOHz+O/v5+RvajeiPFhBKJBIcPH2bc/LHitDaq9evXo62tjcVUbW1tbJdCdA8+LUej0TCKChWSVSoVGwpEXyzd2KeeegrPPfdcyq6XPFYsFkuhCCfPGqRm0WRQ8+bJQMZ97NixMX0nO3fuhEajgcFgSGj34v7FruA4DkNDQ0wJcGhoiG1iviic1kZlt9szNjikA00h/fDDD5mHCwaDGB4eRm1tLSOokVGM9cYng893Su4R/Hfjl7/8JX7729+yMhQZEjVakEcfR3py3DgjWrSAz6i7wMgyla5d22QyITc3F01NTazLmOKO/Px8xGKxjK1ahEzCHASlUsmYnhT/hMPhUzLO/yRouaSlmKgzyebxpWnRAoCVK1firrvuYkH21VdfnZKwczgcbH7yt771LRQUFKCtrQ2bNm3C9u3bMxqL1WqFyWRi2lRNTU0ZE5TxeBznnnsumwf4zDPPJLA4Py8ouBaLxThw4MDnHvtGvP2ysjJUV1czQyKi45EjR1Lo4CfDGWFUAoEA5513HpNbDIfDqKqqypgFNhgMjKhHHPJMS5RUKsXEiRNZ141Wq4Xf78e+ffvSHt/Y2Ihrr70WIpEIJSUl0Gg0uPvuu8ckczha8ykw4k1mzJiBlpYWuN1uTJ8+HZs3b/5cjacmkwl5eXngOA5Hjx5lzR7d3d2QSqUoLS09afNsynWe8tX8H4JcLmc9cZSUTDfAiBCNRtHd3Y3W1lbYbLZRRWE1Gg1LqsrlcjalPZPIRXl5OZPIlslkKC0tPSnHWyAYGeu2YMECnHfeeWxSezIMBgNyc3PR0dGBoaEhdHZ2jqqtxQfx8vkQCoWwWq3wer2QSCSwWCyw2+2w2+1MjD8cDmekJGXCGWFUer0eJpMJ0WiUuerRnniv14vBwUGEQiE4nU44nc6Mx5PWE2lJ5efnp71BhPz8fJbVF4vFMBqNJzUqkUiE+fPnY/bs2WxnqtPp2K6RapkFBQXIzs6G2WxmU+pra2vZ7zJdk0qlwrJly9j3NHHiRPYd2e12cByHrq4uNDc3o7u7m3U7h0IhplYzHpwRyx/tbI4cOYJNmzbh5ptvTuCxJ4NyMZFIBA6HgzU+pAPpiVIOSywWQ6PRsECWDxryPTQ0BJPJxFIOBoNhVKJbTk4Oamtr0dbWxhomQqEQpFIp7HY7LrvsMpx//vns/adPn87owTKZDGvXrkV/fz/uvPNONh1MJBJBq9Uy71ZWVoaamhpIpVK0tLTg0KFD7CF0u90oLCxMYGAQU/TEiRPjbp87I4zK7/fD6/Xi2LFjaG9vRzweH1UympKYgUAAQ0NDGbfYAoEABoMBSqWSzY5pb29nJY90OSrgM50parJI56ko7VBcXIzVq1fD6XQiPz8feXl5WLBgAZxOJ9atWweHw4GioqKE+ubw8DCCwSBLalJmneM45OXlobS0lDXI6vV61mhCBpWTk4N33nmHfUehUCiFokwPxKn0Y55RRuV2uzEwMMCestEgk8kwMDDAqDIDAwNpk5xEWyHeN/HTMwm9BoNBlq4grYZkaDQaWK1WFBcXs7obeURKhZSXl8NkMsHhcLBtvlKpZJLbVquV6YuSil40GkVFRQWmTJkCu92OoaEhzJ49G1OmTGHlpO7ubrz66qsZvx964D5PpumMMCrKEuv1esYFP9mXUl5eju7ubpjNZpSWlqK1tTXteYHPPI9IJEJnZycLxJMRiUTws5/9DC+99BLL3j/77LPYv38/O0YgEEClUqGoqAhlZWVQq9Ws3EKF8GAwCK/Xi1AoBIvFwrSiKJlJRWR+Q2koFEIwGMTHH3+Mzs5O1k5GcwgHBwfR398Pv9+PrKws9h4A0kpZ0kNxKgZ2xhhVe3s7qqqqMup3JoMvsUiaVunOG4lEUF1dDYfDAZFIhBMnTrBsdTp4vV7WbCAWi+F2u1MaHvx+P/bu3Ys9e/YwKWpiE/CpNrFYjBkAEQTpuui/fr+fLVVUHD927BjjlJ04cQI7duxALBZjv2tubk5IxtLsZv55OY5jPYPjnfpwRhgVAPT09KCqqgoA2A3JBPJs9P+jiaX29/czXQia/ulyuTLmbYhtQDvG5IQq969mUjI6fq0w2fsRNRkY2QHSKBOaHUNxGbWG8d9rPIbA7wqi5Y/+KZXKL69Rvfvuu2hvb0dXVxfuv//+UeUMOW5kChbd9NFKM4ODg/jZz34GuVwOvV6PwcHBUTcBsVgM69evh8/ng8FgyDhBi7zgyeD3+7F582aUlZXBaDRiaGgIra2tCQYkFApHHfE7GqhZlrqRySOSpNCpTJ8/7Wt/YrGYbZePHj0Kt9sNnU6HgoIC7N27F319fSk3ValUMp0D4LOh2pmg0+kwbdo0SCQStLW14dixYxk9oVAohMlkYpLWAwMDo6Y3Jk2axEa/xeNxbN68OS3NhCgvJSUlyMrKAsdx6OzsxIkTJ065nzAZ2dnZmDp1Knp6erB///4UKg7wJan9XXrppfjpT38KtVqNoaEhdHV1oaioiCnIXX/99di+fTs7furUqfjhD38IlUoFh8PBND2feeYZvPjii2nfY+rUqfj1r38NgUCAwcFBXH311QmTP/nQaDQoLS3FihUrsHfvXrS2tuLAgQMZJbVvvvlmzJ49mxW2b7vttrQTukpKSnD33XdjwYIFkMlkiMVi8Hq9ePfdd/GrX/2KiazxS04UZPMNg9/WT8ZIEpRXX301hEIh8vLysGHDBrzyyis4fPjwly9Qp7IGCUnk5eWxTtv8/HysWLEiwahyc3MxefJkRgGhHddZZ52V1qhqampw8cUXszSCSqXCzTffjEcffZTdSD5oDO5HH33E0g+Zcj0qlQqFhYUsPhIKhaiqqkprVNdffz0uvvjihDhLq9Xi0ksvxeDgIH7+859DIBCgtLQ0QdGGvxPmx0per5fteK+99lpcffXVTINLLpdj5cqVmDdvHm644QYcP358XPfktDYq6hoh0LAgvr4BX+0O+GxCAm2ZKTDt6elJOb9QKMTFF1+MKVOmoLu7myUg58+fj46ODvz+979PeQ0JqQWDQdbqlAm0ZSd1vnS8d+AzJoFcLk8hAEqlUjbxneM45Ofns2nx/B0l/z1FIhE6OjrQ2toKpVKJuXPnMgapXC5HIBDAiRMnUFZWhvz8/HEb1Wld+5PJZGx9z8QySGYgJNfH6G/pkoEikQh1dXVMEdjtdrPdFt3IZHAcB7FYzLRHR0MkEmElItospOsLVCqVsFqtrKZI2hKUCqCZ0MBnOznyfKRVRa+j15BeqVwuh81mg1gsZrTqaDSKgoICCIVCViccD05ro+LvyPhPI8UR6QyFisHJHiRdhpz639xuN5RKJRMPc7lcMJvNaQVWqf8vOzsbRqMxQUIoGYFAgBV0aZeYjq4jl8thNBqxceNGNhGLPO3x48eRnZ3NrmXWrFlYuHAhNBoNq4kS+1MoFCIrKwtr1qxBTU0NgM9SFZQKoVIQefFksdyx4LQ3KpVKhQ8//DBhdAi5fgqm+R6FtshjCT6p0eDAgQNQqVTIzs6GRCJBU1MTVCpVRkP0+XxQqVQn3ZZzHMfm4hw/fjzhmvkgETOaC01xIHVJ0+R6YGSnumjRItx555244YYb0NjYiEgkgtmzZ+PGG2/ETTfdhJqaGvb5DQYD86yUYiFvKxCMDHcab1rhtDYqSsy1tbUllBPI/Tc3N0OlUqV4FL7x8Y0wGbQzpDiJeu08Hg9bEpNBGWg+N320m9LV1YVQKIT+/n4Eg8G06QQ+a4CvyQkg7fHUll9UVISioiKEw2FkZWXBYrEkTASj75DUBylnRZRpilH/rerE/9dAwq3kUQiUrSaBM/5NTbcTSt52E6j1Kzs7mxlqPB5HYWEh8xbJoGCabgRpNWRCZ2cnay0LBoNps/vEZiVBfoqXgBFdrEAgwNID/GM4jkN1dTV+/vOfY8KECQnBO3+aF+2EyRj5n4U0IsaD03r3p1KpEAqF2G6HD4pP6Mnm/55uCsUZANImEOlvdH66KWazOePySRsDimeImpIJ/GFKfr8/rVFFo1GEQqGEAi99DgDw+Xzsvfbs2YPh4WHW40dMCWqYpet65513AIw8XD6fD0NDQ8jOzmaZerlcjqampnHrfQKnuVGp1Wo0NzfD6XQyim0sFoNWq4XJZEJbWxuGhoYStunDw8NMiY5uSjgcTstuJP1xr9fLhFY5jmMamekMkeIRinP48U460FIUCAQSclB8+P1+dHR0YOPGjdiwYQNbkqlxNCsri3Ginn/+eRZk84N0urZktLS0wOPx4LnnnkN+fj5CoRCLq7xeL1atWjXutvfTukwjlUoTBibyhyRqNBp4vV6mJ05PIGkm8EXqg8EgOjo6UrbzCoUC1dXVaG5uTrghEokEZWVl2Lt3b4oR0O4vJycHg4ODiEajcDgcGTt1zGYz6urqcPjwYZSWlmbs6qmurmZyi/zZg3K5HDk5ORm1p+hByBQ3ikQiVFVV4dixYwlxIMdxUKlUqKmpYSwHYGxlmtPaqPiYMWMGpk2bhqqqKmzcuBFvvvnmqFpQSqUSOTk5LFA+HaDVarFq1SpMmjQJfr8ff/zjH9HT05ORZWGz2XDJJZfg2LFjMJvNeO655zIat06nw6OPPsqaHI4cOYJvfetbKV7qS1H7A0a8Q1FREdu10FOWyVi0Wi0WLVrENArefvvtUTtqkt9Lq9VCp9OxHVsmEEeJAn7qpxsvBAIBLrjgAlx++eVsAJNCocC8efOwd+9ePPzww2zcCR9WqxU33ngjjh8/DoFAgBdffDEtjcVgMOCOO+7AxIkToVKpEI/HodPpcMkll+Avf/nLl5OjTjkWh8PBAurkbTB/e79q1SrY7XYcO3YM9fX1uPTSS5mcZKYvUKFQoLa2FnV1dZg8eTJaW1tx4sQJrFu3Lu3xEokE3/ve92Cz2VBXV4dwOIx3330XDz/88LhjFKPRiJtuugkmkwlCoRAKhYLFbrNmzcKll16Khx9+OOV18+fPh1wuR0lJCWKxGEpKSnDkyJGU41atWoXly5ejpaWFFZp1Oh1uv/12HDhwIIG5OhacEUbFFzuj4DjZzRcXF2P58uWQy+UsS3zOOeewzHROTg7C4TDef//9BPloQlVVFSoqKrB161a8+uqraGhowNq1a7F169aUDl6BQIBzzz0X1113HbRaLduBPvHEExkNipTrSJNq9+7drOaWm5ubNjNPwXhBQQFkMlmKFyR6D19uMR1oLO6ePXugUqkYg6OmpgY5OTlfTqOiwPXYsWNs+HQyh6m2thZlZWWs0EsSQkTHtVgskMvl8Hg8KUYlkUhgNpvx2muvsR3XJZdcAr1ej5KSkhSj0mg0uPbaa1nsQSyEdNpVfM96yy23oLi4GAqFAg0NDbj//vsRiUTQ3t6Ob3/727j55ptRV1eXMB3+8ccfx5tvvpl2WaORJqPt/oCRvkmJRIKLL744IY+XXLAfK84Io6K8UHd3N8u1JC9jZWVlbB4N8ZFISIxyOSqVCrW1tVCpVCkSRFR2AUZ0Q6k9PN3NrKqqYrpWZDT866GidE1NDSZMmACj0Yh//OMf+NWvfgWVSoWrr74ax48fZ6NOhoeH8cknn2D16tWoq6tLyFV1dHSk7QQCRkh3pNBH/YrJkEgkMJlMEIlECYlTynuNd9gRcAYZlVAohNfrhcvlYsMe+dBqtZDL5SxvE4lE4PP52I2XSCRQKpWMXUBGlZeXh2nTpmHixInYvXs3jEYjVq1ahffffx/FxcVoaWmBWCxGSUkJOI7D5MmTcffdd8NoNCZ4iHg8ztIYWq0WFRUVuOKKKxAOh6HX65GXl4fnnnuOebd0G4BkA06nj0WQyWTIy8tjnpvjOJSUlKSMfSHJRuCzZC//OzmViapnhFGFw2HWKUIy1slGdfz4cZSXlyMejzOqb3IpRSwWs3IJAJSWlmLSpEl4/fXX0d/fj7vuugterxePPfYYhoaGcMstt8Dr9WLGjBl47LHH2E2gGIaKv8CI4WdlZUEoFKKsrAwVFRWMCmy32+H1elFZWQmFQsGmWpnNZrYrjcVi6O/vT6lxZqIqK5VKFBcXs3guFouxxhA+KAY9cuRIQpcyNYeMxrLIhDPCqAKBAGN+piOmASMCabRL5MsbEr+Ifu7q6kIkEkFBQQFmzpyJ8vJybN68mc1Rfvzxx9m0MGquOHLkCJ544gmsXLkS5eXl4LiRwdldXV2MzxQKhdjswN27d2P37t1pS0sAEhKQ/L9R+xmfhJhJP0EqlTImBS3BNCkrmV4sEAjQ39/POFREgzl06NApNT6cEUZFZQ4+dSMZ8XiciZGR9DUtd3S8TCZjmgd5eXmsGXPNmjUIBoN45JFHWFAejUZx9OhRRl958skn8ec//xnZ2dlQKBQYHByE1+tlLeixWCxFPD9T4Jyp+ybZ6NKlTggGg4F9VvKWZrOZGTj/vNSIyp96KpFIRk2sjoYzwqgAMCGJgYGBtEOw9+3bh6effpqpqtCNIR0F+nK3bNmCSCSCrVu3jvp+8Xgc27ZtS/id3+9PKZccOHDgc36yz9DX14e9e/dCKpUy1mimDHk4HMb69evZ9C+Px4PW1tYUgw2FQti8eTPsdjub+wyMfC979+49JVG1M6ZMo1QqodPpEAgEIJfLYbfbMzIPLBYLfD4fC8ZJx2CsWfXRQN6DWKdqtXrU8woEAtTW1kKj0eDTTz8dNeNOdBv+EjaafqdSqURZWRkEghHV4ba2trTnp8w/3wMCn6kl8w33S1GmEYlEWLRoEVatWoXy8nK2I9q1axf+8pe/JKj6CgQCXH755bjuuuvgcrmwd+9eBINBTJ06FfF4HA899FBGhbyxQiAQoKCgAJ2dnRCLxQnBdjJ0Oh3mzZuH7373u9BoNHj11Vfx17/+Fc3NzWkfCKKtjBUTJ07E448/juHhYTgcDvzgBz/AoUOHUo47FWWX0XDaG9XEiRPxve99D8BnLecymQyzZ89GXl4e7rzzTrbtlkqlWLlyJXQ6HRQKBYqLiwGMeD5qd2pqavpczZkUj1CNLhPtRaPR4H//938xdepUdsyaNWuwfPly3HfffYzv9HlAzRkSiQRqtRqFhYVpjSrdxgAYqUJEIhF0d3ePq/fvtDcqGmBEQhh89eDi4mIYjUZ0d3cDGJlsXlhYyIJz6gqmFENdXR3L65wqOI6DxWKBx+MZVYeA5tIkM0+1Wi2WLFlyUqNSKpWYNWsWtm3bxpba5BtPE97b2tpQVVWVtkxjtVpx3nnnQa1WJ4ibiMViNDQ0IBaL4Z577hlVRiAZp71RKZVKDAwMsHQBADaIKBqNQqfTMaOiuhbtgGh8B90UvV4PnU6XUSuBtunE1lSpVCkccWJwkmFnipGUSiXTluLPLpZIJKNqbGo0GixduhSXXnop1Go17rvvPkilUsRisYSmWQBoa2uDUChk40iSO3UEAgEqKyuxePFi5OXlQSaTJbBhA4EAHA4H1Gr1l8uo/vznP+ODDz7AE088wZ5EoVCIY8eO4cEHH0zIOFutVnbT+Q0KSqWSTXzQ6/UJRsUfdhSJRKBQKLB06VL09fXB4/Fg7969CYEsXyCN2rvSITs7m1UC6PVCoRBDQ0Ms1UHbeaFQCIPBgCVLlmDNmjWoqKhgryN2wtGjR/HJJ58keKvh4WHEYjH09vbC7/en6EWIRCJmUOFwmCk2y2Qy+P1+yGQy6PV6FBUVnVRfno/TuvEBGAkySSCMGAcAmCod/0suKysDx3HYt28fxGIxa8qMxWJobm6GwWBgg4GEQiFkMhljDlD+hub2iUQiHDx4MGVLr9PpkJWVxeIqg8GQtvHBarUiEAgkBPGk1aDRaNgDYjKZcM899+CVV17Bvffei+LiYsY5F4vFyM3Nhc1mQ15eXkoilGqgPp+PxZt8FBYWQigUIhKJIDs7G4WFhXjiiSewe/du5OfnQ6/XIxgM4rzzzhtX88Np76mAzyggfKT7EkwmE3w+H/vy+RnpwcFBVmRWKpW46qqrGAuht7eXFZ7VajXb0S1atAivvPIKPv3004T3MBgMcDgc0Ov1UCqV0Gq1KUsPLSmRSCRh8qnD4UB+fj7LF5WVlWHJkiVsQqharWZxHxV+qY6ZHA9SeSYYDLJZPMnfm16vZ+UtALjoooug1+uZh5dIJJg3bx6sVmtaImA6nBFGBaS2XqUrX9BNy83NZaPc+M2ZlLfy+/147rnnEoRVgc/GrtESByBlPAj1yYnFYmi1WlZITjYqkUgEu92OrKyshGDd7XYjLy+PnX/Xrl247rrrkJuby1rXqc4ZiUTYSLWBgYGUTQF/GeaXpwi9vb3o7u5GIBBg7W42mw0ymQxutxvRaBQnTpzA22+/fdIRvHycMUYFJJYv1Gp1Qm1NIBBAq9XC5/OhqqqKkdaopmaz2eBwOFiQfDIh2nSg8g/dPNrOq1SqlGPdbjd6e3thsVhSmg2oeRUYWcbb2tpOaah2KBRik8KSHxBg5CH59NNP2YMllUoTuoyCwSBcLhc++uijceWyzgij4j+N5E2Sl0PSODebzYw/RfUuelLtdnvaEs9YQV085C1pp5mOk9TV1QWLxQKLxcIy1NFoFIcPH0ZLS8vnGg1CoO6Y5JFzhFAohLfeeutzv08yzogyjVKpxIIFCxLcu9vtxvvvv5+wLC5cuBASiYRNfCDtKJlMBrVaDZ1Oh76+vrQJwrGAlg+j0cgU/fr7+8FxXEqagh4CEvOgDuFMpZTk9xnLbdPr9Zg+fTqamppQU1ODnTt3jmsYZDp8aVq0NBoNCgoKWINlVlYWent70dXVlbbgmp2djeuvvx7xeBzBYBA+nw/PPPNM2kSlSCTCJZdcgry8PHg8HmRlZeHNN99MO6xaoVDgoosuwuzZs+FyuVje5+mnn0ZTU1PK8VKpFMuWLWOs1fz8fAwMDGDnzp0ZE7BWqxW33HILurq68MILL5zSMp0OVVVVKCkpwd69exEIBDB16lR0dHSwfkDCl6L2Z7FYUFVVlaAxwHEcKisrodVqcfDgwZSn2mAwYNq0aWznNDQ0hBdeeCHFqKRSKW688UbcdtttTBlYKBTinHPOwQ9+8AN88MEHCcefffbZuOGGG5iwRSQSgcfjgdVqxde//vUUL5Gfn4/LL78cMpkMGzduRDAYRFlZGY4ePZqSbJRIJFiwYAEWLFiAyZMnY2BgADqdDh9++CF27do1rjJKMsRiMS666CLIZDJMnjwZ8XicySX19fWN27ud1nkqyghTAEq8INIeKCwsTEvcp50eVeBJ8D4ZVVVVuOWWWxKq+NQkcc8996SwIidNmsQoNDQnORaLsTxSMtra2vDnP/8ZwWAQkUgE/f39+OMf/5g2e11cXIyZM2dCoVCgubmZaR9QA8bnAVGPs7OzEYlEmJoeDVUaL05rowJGAl65XM4ovCR1SOWbdEtaJBJhcYtAIEjQG+CjuLiYbbUp4UjLUm5uLsxmc8LxOTk5MJvNkEgkLDNNw8ALCwtTzh+Px7F//350d3cjJycHUqk04y6LNBPIiwwNDTEDyMrKGvU7UiqVLAsPfDaFlCASiZCVlQWr1crCCp1OB4vFMm7FF+A0NyqOGxEJo6FCOp0Oer2eJfoOHz6clsPt9XpZLEVqKOluZkFBAdOj2r9/Pzo7O3Hw4EEAI8lL/qw9mpdHzQ2UWKW+u0xz+dRqNbq7u9Hc3IylS5dmZHKSJ+E4DgaDAVqtFhqNhu1oM6GoqAg/+tGP8J3vfAdz586FTCbDqlWrUF1dzY6hYeUkHUTXTQ/IeHHax1SUZ9JqtXC5XBCJRNDpdHC73RkbN71eL+Ocq9XqjMfm5OSwNvdJkyYBGDE0EgTLyclhx6pUKmRlZaG9vZ3lhKhM0tfXh+Li4rQNGeTxZs6ciebmZpjNZgwODqZcS3Z2Ntrb21lNkNifHMehsLAwQdFYIBiZ1LB06VLMmjWLCeBeccUVWL58OYqLi/Hmm2+yXa5Op0M0GmV5O/51nsryd9obFTCS3/H7/UzfiZ72TDsoiruo7EHyOcnQ6/Us7uKzIYERY+Z/4STS2tHRwTqgiR/u8XhgMBjSGpXdbsfOnTtRVlY2auMm9TPS9fj9fqZ7kJ2dzY6rr69HXl4eiouLWW2PcnI0jFImkyUsmWSo9NnoATvVqfBnhFEBIzU38jwajQahUGjUxgIyRI7jMk5i50sWkmHxy0D8kbrUgUxirBTYU6KVSivJqKioQE1NDQ4cOICCgoK0y7VIJGIaByT8SmPiKGYkgyWaDwA4nU6IRCIEg0HY7Xb2IPEnuAKfVQJoWheVreh7Gi/OCKMSCEaGPRJ9hbhVoz1lRM3NNCWKBkWmqyXS7wwGQ0IiknTNqSxD3q2pqYm1RyWjt7cXCxcuhFAohMfjSZt3ohJTJBKB1+tlDwMViEmLNBaLYXBwEA6HA/v372fvR56I8ma0OSGQZw+Hw6y0Q7oUmVrARsMZYVRisZjFB4FAAIFAgBV1MzEvqWs3k/HRl0lLDl9clbwW36gokfqPf/wDb7/9NoDPNB6CwSAuuOCCtNfh9Xoxbdo0dHR0YNeuXWmNKplUaLFYGGu1ra0tJe5J1k6gc2Yq/SiVSqb4QsaXm5uLUCh0SmWrM8KowuEwm4lM5Y/RGi3j8Th6enqYzhN/ySBEo1H89re/xXvvvYd58+bBZDJBo9HA5/NhYGAAdrsd27ZtS2B4bt26Ff/85z9TkoUlJSWYMmVKWuN1OBzYs2cPy51l2lwMDw+jubkZlZWV8Pl8bL7yvn37UFNTk9YLjhVDQ0N45plnsHfvXvY7gUDAxNXGizOiTMOPAfhf7mg6ULRV5ou+ZkJywTrTPEE+i5MPgUCQ0sTJBy01o82xUSgUCIfDTM89Ho+jr68P4XAYYrE47TxnOi4SiUClUqGnpyftdWdqfEj3+7GUacB9Djz00EMcAO6b3/wm+10gEOC+/vWvc0ajkVOpVNwFF1zA9fX1JbzuxIkT3PLlyzmFQsFZLBbuW9/6FheJRMb8vm63mwNwxv2TSCRcYWEhV1RUxMlkspS/y2Qy7sILL+Teffdd7u233+YWLlzISaXSlOOkUilXVlbGnXfeedyzzz7LPfXUU9yNN97I1dTUcDqd7nNdo9vtPun9OeXlb+fOnXjyySdTZpfceeedWL9+PV555RXodDrcdtttuOCCC1geJRaLYcWKFbBardi6dSt6e3tx1VVXQSKR4Gc/+9mpXs5JIRQKUVpaCpVKhRkzZjCv1trair1797LxGX6/f1wk/9EwVjYBMOKtLr30UixatAgKhQKffPIJHn/8cbb8mM1m3HjjjZg4cSLrJL7++usxadIk/P73v0/YwarVauj1ejQ1NaG7uxsymQyDg4MwGo0oKCj4Qrum0+GUMuperxdXXHEF/vCHPyRsq91uN/74xz/i17/+NRYsWIApU6bg6aefxtatW1mnxzvvvINDhw7h+eefR0NDA5YtW4af/OQneOyxx8Y9VjUT0iXsiBFQVVUFt9sNn8+HUCiEvLw8LF++HMuXL8d5552HhQsXfiHXYDQacfXVV6OiooLtIkeDyWRCQ0MDy5yfffbZKC8vZ39vaGhATU0NYrEYOjs70dHRgXg8jilTpiRkx+mzNjQ0sFnPPp8ParUaRUVF0Ol0p7SjGw9O6ey33norVqxYgUWLFiX8fvfu3YhEIgm/r6qqQkFBAdMd2LZtGyZMmJCQsFu6dCk8Hg8rgSQjFArB4/Ek/EuGQCBAUVERbrrpJtx+++1MFpHm39GUA76qXSgUYolPys8k87h1Oh2mTJmCysrKMSugSCQSTJ06FXq9HitXrsStt96K22+/HUuWLMn4Giq7UGqA0gaEmpoaxiX3+/1slqBKpUqRCJLL5aioqMCsWbNQVFSEgoICzJo1C7NmzUJeXl7a/j/+aBL+72w227g3AeNe/l5++WV8+umnaXUxSWInuWqenZ3N1FT6+voSDIr+Tn9Lh4ceegg//vGP03+Af7WWL1u2DNOmTYPJZEJBQQFKS0vR1taG6dOn45vf/CYcDgcrn/DTCHwed3IuKjs7GzNnzkRXVxcuvvhiiMVi/L//9//YdXJpZIvEYjEWL16M8vJyKBQKSKVSNv9lNJ63RqNBIBBg6Qun04muri4An8lDAiM3mjqfqWEiJycnYanVaDQYHBxkCWDKT3m9XuTm5rIWLGBkA1BZWYmJEyeiu7sbGzduBDDi7a+++moYDAb8+te/HtcqMi6j6uzsxDe/+U28++67GUVJ/x249957cdddd7GfPR4P8vPzccEFFzBymV6vZ1QNnU6HZcuWIRwOw+12IycnB1qtFnq9nrVyAWDeiW4I8bQ1Gg2WLVvG/pabm4uuri5ccMEFePzxx3H48GH2+f/yl78whRiZTIbLL78cc+fOxfbt2xEKhSCVShEMBqHVahNqhXyo1WoUFxfD4XAgFArh+PHjaGlpYUZIDQ/AZ7oHAoGA9eqp1WqWxBUIRmb12O12tLS0ICcnh5WKdu/ezbqRXS4XcnNzceedd0IikSAajaK+vp51Di1fvhwzZ87EI488Ar1ez0bIjQXjMqrdu3djYGAAkydPZr+LxWL46KOP8Lvf/Q5vv/02wuEwhoaGErxVf38/ayiwWq3YsWNHwnmpZJCpM5f685Ixb948NDQ0QKFQsPZ1n8/HglMArOhKI3GpCs/3Mty/KDNkZD6fD3v37kVubi70ej2kUincbjdefPHFhCUTGHnQsrKyMHnyZOh0OpSWlqK9vR1Go5FNpCDddbfbzQrBNGg7OzsbeXl5yM3NhVqtRnt7O+x2O+uWHhwcZFQV0ixNbuggT0s/U3GdmjDooaFroe9GKpXixIkT7LOEw2HMnj2bERffeecdJsD7j3/8Y8xaVeMyqoULF6bsHK655hpUVVXhO9/5DvLz8yGRSLBx40ZceOGFAEY6Zzs6OtDY2AgAaGxsxE9/+lMMDAywoua7774LrVbLBhuOFf/zP/+D5cuXo6qqCgqFIqEXLhQKIRKJwOVyobu7GzU1NczQKXczPDycoOBLMwM5jkNzczOam5vHdB3Lly/HrbfeypZPik+odsjPDYnFYpSXl7M5eyqVCgUFBcjKymJ04nA4DIPBwJYxAn+Jo8+p0WhSOmWIS5afn8/+Hg6HYTQaMTg4yAywvb0dv/vd78b1nY8F4zIqjUaDurq6hN+pVCqYTCb2++uuuw533XUXjEYjtFotbr/9djQ2NmLmzJkAgCVLlqCmpgZf+9rX8PDDD6Ovrw8/+MEPcOutt46bZtHV1YWnnnoq4SkFEpN15I2ys7MZk4GUd0+cOMFmKtPyRzHXeAqpW7ZsQXd3N/R6Pav7UR2NL09Ny+Hq1auh1+vR3d2NaDQKq9XKCrp6vR5GoxEikQiFhYUJrVn8z5kp0QqATSW1Wq3Q6/UsaUv1xdEGMH0R+MLLNI888giEQiEuvPBChEIhLF26FI8//jj7u0gkwhtvvIFbbrkFjY2NUKlUWLt2LR544IFTfs+xqLTs27cPBw4cYCp65K2+CLjd7jHrWgkEAjz77LOorKxEfX09E5f1+Xxobm5mHHG3280MlIL3jz76CEePHmUkRI4bGcRts9kSHiSJRIJgMIjOzk44nU7GXLXb7SgtLf33x8NjTmP/H8KZmlEXCAScTqfjtFotJ5PJOIFAwAHgxGIx19jYyJWUlKQcn5uby82cOZMTi8Xs92VlZZxWq037Hg0NDVxubu6/NaN+RtT+xgOz2Yzly5fDZrOhv7+fNZlu2bIFn3zyyeeaqEVs0LPOOgslJSWIRCIs/fJ5++2Az3oXzWYz1Go1Dh06dFJt0rFALBZDqVSylnoKA2KxGJxOZ0K2/kvRojUe6HQ63HfffaioqGBqMdRRPH/+fLzxxht47LHHUpZTfoAsEomQl5eHnJwc2O12Nj9GJBLh1ltvxdVXX43s7GxG+eU4DkePHsXdd9897hkvfEgkEkyePBkLFy5EZWUl5HI5LBYLwuEw9uzZc8pCbSKRCNdeey0WLlzI9LFkMhmLLR0OB+6++2709vaO+ZxfKqOaMWMGJk6cCK/XyxReiC4DjOxu//a3v7GkIx8KhQITJkyAyWSCUqlEeXk5jhw5wozKZrPhjjvuYFND+/v7ceTIESxevBjV1dW48MILmVFlZ2djypQpjB3BpygDYCS5vXv3MlpOaWkpqqurceDAAchkMgSDQbS3t6OyshJ2uz3tlPixYPr06bjooouY5BGf7UEj78455xw888wz/5481ekA+mLSEdKampoQiURYQwL3rwD4xIkTKCkpQVNTU9qsPvevdENzczMzyPPOOw82mw0GgwEulwt6vZ6VhlpbW/HBBx+weS9GoxFVVVWQSqWsg7qkpIQpuVDrF/ev3BmlJCgdAoDNM4zFYmwpjUaj0Gg0KC4uRkdHR1rqSnZ2NgoKCjA8PIyjR48ykTeiPi9ZsoQtZ5Sa4Bu3SCTC0qVLsW7dOgwNDY3pHpwRRiUQCGCxWDBv3jwsXrwYg4OD+OlPf4pIJIL8/Hz09PQgEAigt7cXW7ZswbRp09jOj3hJxcXFow5MjEQicLvd7AsfHh6G3++H1WqFy+ViHSkdHR0oLS3FTTfdBGBEauitt95CaWkpMyoArNAcjUZZMpQMyuFwICcnh9F85XI5KzT39PSwTD+NuvX7/awBlA+lUokLL7wQa9asQTgcxjvvvAOHw4Hy8nLs2LED//jHP7Bv3z4sXbqULdX0+fkpC5pNPVac1kZFAxIbGxuxePFi5ObmQiQSQaVSobq6Gh6PBxaLBXfddReOHDkCjuPw0ksvYdKkScy9CwQCzJ49Gz6fDz09PaO+HxmUVCqFzWZDTk4OC2Jptt7+/ftRUlLCjo/FYjh48CDq6+sTNElJMJ+aYIn37vf72fJJx+t0OhiNRiiVStYEQbOjKbbKyspiSyANMZo/fz4WL14MsVjMxMuoAXV4eBj//Oc/2TSK5Cw9x0uyDg8Pf3nUiYuKinDhhRdi+vTp0Ol0bPnweDwoKChAOBxm0wwI/f39jO9Nc//IO2TqquHDYDBg6dKlTAOUlgTK6NMwcO5fWXribJGkDy1dNpsNQqGQtbyTUh+VVcRiMct9FRQUwGazYf/+/RCJRKxLBgB6enpQVVUFs9mMrq4u1NfXo7a2FpWVlSgtLWXXGI1GMTQ0xJbvpqamBO59coMH39D56oNjwWltVAKBAG+//TbeffddmM1mJk8YjUYxPDzMuo+TK+zUMsXvd6MWqGSQ3I/dbodGo8GSJUuwa9cudHV14fzzz2eGSB6HtuR81T0arKTRaNDX18fqc0Rp9nq9LLNPs/uopw8AazvjZ8KFQiFTYQbARsF1dHTA6XRi165dzGD8fj8CgQCkUil0Oh2byBWPx5GVlQWNRgOJRJKQGObXRWfOnIns7OyME7uScVob1eHDh9nWN/kL4Zcwkgc40rJExV0q0vKXABrdlpWVhcWLF+O1117DrFmz8Mknn6C9vZ3twPjGQ+Q/eh9ikwoEAtbkKhaLcdZZZ7FCsVgsRk9PDxPzoOOcTicL0kUiEYaHhzE4OAir1cqaOqim197ezjYgDoeDZdHTgTYitDE4cuQI684hShCpNtNnCwQC/z7qy/9F0IcfT8mFdn/8GhqJfFDfW319PYqLi2G1WmE2m3HRRRfhyJEjLG7h/lUrJCMNhUIYHBxEW1sbm+hFMVJLSwvcbjfEYjGi0Sj+8Ic/4LnnnmPLpMfjGfX633//fezduxdTpkxhFBelUolQKASNRoMXXniBFZ7Ju4wVs2bNgsvlgkqlSijI01zqSCSCdevWjStlcdob1XgRj8cxPDwMuVzOlgWaFkHLUTQaxaeffor9+/dDpVJBq9Wyqad0wyKRCDZv3sye5kOHDuHQoUN4+eWXsWPHDpaVDgaDaG5uxtGjRxnFhz9saazX7HA4mLQ3FapJD4LmHZ4Kjh49CpvNxmqQwWAQ7733Hg4cOMA8ZPK0sJPhjCnT0A6HXHgyHZcgEAhgMplgMpkgFArZxK1gMAin03nKN4fksDOVY/R6PTweT8r5icPFn/qQCTQHmZYqUg4crbRETa/UZJsM+g4opiL57EwphC9FmYYGF+Xk5LCAOBqNwu12o6mpKaUzJjc3F/fffz9jA6hUKvT19eEPf/gD/vKXv4z6XqN1x/CTkumQLnFoNpvxjW98AzNmzGDB8+9+97u00o85OTk4++yzMW/ePBYvtbe34/3338enn36advmkiQ4mkwlisRibNm1KWcaIu19aWorp06dDJpOho6MDkUgETU1N6OnpGbdIx2nvqaZMmcJ2NPRPpVKx+TC7du1igbpMJsMvf/lLrF69mqUEKMi32+24/PLLM4rICoVCzJgxA11dXejp6UkprYwFFFMR7rjjDtx3333MKwgEAuzfvx8rV65MWB61Wi1+/etfMy6/WCyGw+FgXTpPP/10WpXhiooK1NfX49VXX4VWq8Vll12GZ555JoXBKZfLsXTpUpSVleHss89GX18fIpEIdu3ahRdffDHh+LF4qtNa9Iy22sBIjBMIBBAKhXDfffehsbGRZaIJK1euxLnnnguBQIDXXnsNN910Ex555BFwHAej0YhvfvObGYmCRqMRFosFy5cvx7Jly7By5UomIpYOQqEQK1asQGNjI/OmK1euZMp0YrEY8+fPh1QqZeUQsViMmpqalJar4uJi1NXVsQFJtBEgLa65c+cmpBtUKhXMZjNmzJiB1tZW9gD29/fDZrNBp9MlXHdxcTEqKipgtVphtVoxceJEzJw5k83yGS9O6+VPIBBgYGAABQUFjNEZCoWwa9cuNDc3Y3h4mDUPELuA2JekEEPLBvcv8TCLxZK2oEwZbQBMatFqtWLXrl1pYxq1Wo3bbrsNJpMJ/f39MJlM+Otf/8q25llZWcjNzcW+ffsYa5Y8xNSpU7Fr1y52rpycHHg8HhiNRsRisYTBRBKJBFlZWVAoFPB6vRAIBFi8eDEWLlyIQ4cOYc2aNdBqtejt7cXevXtRXV0NjUaD/fv3s5a4CRMm4Prrr8fAwADz3sFgECtXrsTzzz8/5vEhhNPaUw0PD+PIkSNsijrlnx577DG0tLRg3759bAZwLBbD//7v/2Lt2rVwOp0oKChAfX09KisrEY/H8ac//Qlf+9rXMjIUzGYzsrKykJWVBa/XC4vFgoGBgYwCFvPnz0dRURGUSiUKCgqQn5+fQB/p7+/HJZdcksD5Hx4expVXXokXX3wx4VwbN27ENddcw/JPNNZNLpfj+eefx913352QhJXL5di2bRtLAFMsaLfbmQgIfwkrKytjgwSoIK/VapGfn4+zzjpr3PfltPZUAFiwTQVbyrN4vV4cPXo05dijR4/C6/Wivr4eFRUVrEF07969KfNjgJGbRJ0uZLgGgwFCoRATJ07E22+/nWJYkydPxt13381ovZTp5zd2xGIxtLe3Q6fTscA7NzcXfr8/JXMdiUTgcDiYGh7l0qj0wt8EGI1GqFQqdHV1wWazITs7m9Fr1Go1E/rgU4oNBkNCoE/9hJlGoJwMp71RAcCCBQtwxRVXJOgydXV1Yd26dWlTBDRckmIN2tbzUVhYiMrKSqbXVFZWBolEAo/Hwzqmp02bBovFghMnTkAsFkOv12PJkiW4/fbbWd6HXzObNGlSQrAul8sTJBmVSmWKjDYfRKJTqVQIBoNMo5OPlStXMmU8j8eDpqYmTJ06FV6vl4nBJXdaU7mKX3uka/zSGhVlkfk7skz5Jloigc8GRCYr5cnlclx++eWoq6tjomVNTU3MI5Ckz8svvwyBQICamhrcd999KCoqQk5ODuMlxWIxpnJH4Hfq0Gxj/mcYTQ2YrpPGs9HoEYJCoWD5JqoVDg0N4cMPP0QsFoPZbGZGSLQammvodrshk8mYV5VIJPD7/ez6xoMzwqjIICh2IEqISqVKmx+i6jy/U5n/+mg0ira2Nhw8eBDDw8OIRqMoKipi9Tq3281IccSHstvt0Ol0rNW8q6sL+/btg8vlQmlpKYCRQDy5Dpms5DeaXkMsFmNqejRNla9zHgwG0d3djQkTJkCpVLLyEfG1+HKTfK1QsViMzs5OTJgwgX0vNHwpkwT3aDgjjCp5ghY98aM99bQ0kVfj3+xoNIp169axKVdyuRxHjhxJyGt5vV5Eo1E4HA4EAgGWjqDWKSqdiESiBIH/5CGPCoUigbuU6ZrJQ9G0UCoo81MDHMdh8+bNuOCCCxCPx9nusLOzE21tbazeKZVKGa2GWrc2bNiARx55BGazGTKZDHa7HTNmzPhy6qgDIwVX2v2Qp+IzBvgIBAL43e9+h/z8fCiVSsYKOH78eILHCIVC6OrqSrsbTAeO49K2hcdisYzCHIFAAI8//jhLVQiFwoyziiORCJ544gk2AJNez5dUBEYUiX/6059i5syZMJvN8Pv92Lx5Mzo7O5lHDofD7Fo5jsNvf/tbuFyulNJWOtnKseC0z6gTysvLUVVVxWSf9+/ff0qJu3SQy+UoLy+HXC6Hw+FAW1tbxnMrFApcf/31rNPl5ZdfRktLyxdyHSqVChdccAEL1Ht7e7Fx48ZTkqVOB6FQiDlz5kAul2Pjxo1p639fitofobi4GMuWLUNXVxcqKipw7NixcYugphPPB4ArrrgCd955J4RCIRN8a2lpSWtYFosFq1evhlarZbHZF2VUNpsNl19+OcxmM4aHh7F3717s2rWL5eKAkXRAeXl5gg46cc6AEcPp7e1N0VG3Wq3Izc3FmjVrGHeru7ubkQbHgzPGqMrLy3H8+HG2WxlrLEDDfih/w1/uhEIhlixZgssuuwzBYBCDg4PgOA433XQT/vCHP6TkwYDPOlKokWC0FAEfBQUFqKioYOxMp9OJd955J+GGWiwW9Pf3Q61Ws4DdarUmGFVNTQ3uvfdeVjUIBoMsQPd6vQgEAvD5fPjVr37Fiu16vR41NTVYsGABdu/ejXA4jEsuuQSbNm3Ctm3bxt0Ie0YYlUAggNlsZlpUHMeNSkMhUG4pHA7D5XJBq9UyOSJgROjimmuugdvthsPhQEFBAfr6+lBfX4+LL74YDz74YMo5ae7w4OAgcnJyUiaSZsIll1yCyy67jG3njxw5gg8//DAhTqOckdPpZMp7/N2iVCrFueeey4RC4vF4wt+J1dnc3IyysjImXKdSqViqo7+/P4GNoVQqv1zz/vhwOp3QaDRoampiKnMnQzQaZRMSQqEQ5syZg+LiYvb3wsJC2Gw2lJeXo7q6GhzHMW55Xl5e2rFl1FzQ3NwMoVB40g4dAKwjiDL2fr8fer0+7YxAt9uNTZs2weFwwOFwJFB7iKFBM5OJX8anXCsUCvaPQKUkkn4Mh8OIRCJoaGg4JXmBM8aolEol2tvb0dfXB7PZPO7gNT8/n91QynvJZDI2XSoWi8HhcCA7OxtqtRqRSCStUQWDQQwPD6O3txcOh+OkDZhisRhXXHEFysvLGUmO2J3Jynt+vx8CgQDd3d3YsWMHXC5Xwg6XdqBEkeZLR/JlljiOS2jBp0Tpnj17EAqFEA6HsWXLlgQ27HhwxhgViaq2tbUxwdWxQiqVQq1Wo7m5OWGgZE5ODotdiA1AI2/VanVGnSeq3zkcjlF1PtVqNa666irMmjULHo+H1egGBwfh9XpTuqUDgQBMJhMCgQBOnDiBaDSawOZUKpXo7u5mn52qB3zeOjE9yZMLhULo9XoYDAY4nU7Gfujr60NfXx9kMtm49azOiJgKAOtvI/XiTDNVkhsldTodsrKy4Ha7IRAI2HR2YCQ9QEp71OFcWFjIJjNk6oWjbHdyspPe02AwoLi4GHPnzkVdXR1rr+rt7cXLL7+MTZs24aKLLkJ7e3vCa10uF+RyOftsyYZNU+xp+CMtY/wePuK787VDlUolo1Lzl0p+melL06HMh9lshsfjYU9uplggJycHgUAALpcLFosFGo0GbW1tiMVibGI8QavVsthDoVCw9iiNRsO6T9KBZv8Rs4EPiUSCFStWYPr06bBarcjKyoLD4cDu3bvxhz/8AR0dHZBKpdi1a1cKRdhut8PpdCI/Px/9/f0oKytLKetQ8pS6mUnPgSoH4XAYBw8eZP2ClGH3er1QKpWspUwmk8HlcrH4bDw4I4yKmg6oNpZuG6/RaKBUKmE0GhGNRmEymRAMBtHW1sY4R2q1OkGjk89eoJQCEQKpqp8MjuPgcrnQ398Pp9MJr9eb8PdwOIznn38ef/3rX5kkNnHq6X0jkUhanrrX62VaDT09PWzIJR+BQABHjhxh5MC9e/cyMRLa/W3atCnldTREidIPJK19Kqp7Z4RRUX8ddSc7HI4UwxoeHobP52P9cUKhMMETkI4U3/uEQiHs2LEDQ0NDUKlUkEgk6OjogNvthsFgSFuW4TgObW1trKUr3YaBCr2ZumAylXw4jsPHH3+MSy+9FBw3oouebBwHDx7E1772Nbb8nUzEjZZ2UoShGdQOhwORSAS1tbXj1r46Y8o0tPaHw2Ho9XoMDg6OKtFMfCfa8aULqCl94PV6WcBK3ScGgyEhK80HGRzpmY/2FRPLkuKd0XJCVECmiVsGgwG9vb2jnp9yWSQFkA70uSgRzHEc+2zUYEv4UpRpSINpxowZmDlzJmQyGXw+Hz766CPs2LEj7U2SSqX485//jOzsbIRCIbzyyiv45S9/mXIcMTDXrFkDm80GmUyGWCyGP/zhDxkNCgDLHWUa2shHXl4efvWrX7HRbV//+tfTDuYuKSnBQw89BKVSiQ0bNqC8vBw1NTXYvn07fv7znyeUpAQCAWw2G3Jzc1FZWYmzzz4bH374IQ4fPoy+vj50dXUlGKJIJML06dOxatUqLF68GNFoFDt27MCbb76JDz744KSfIRmnvVFJpVJcffXVsFqtiEQiiEQiUCgUOPfcc1FRUYEnnngixX1bLBbk5OTAYDAgFAqhqqoqpX0K+GyQYmFhIQKBAORyOUKhEBYtWoT169ef0oDFZJSUlCArKwtisRhyuRxVVVX4+OOPE44RCoX49re/jSVLloDjODQ2NjLvMnXqVNYZTdBoNKitrWVqMvv374darYZWq4XRaMTw8HBC/mz16tX49re/zfSyRCIR5s6di8bGRjz88MMn7YdMxmmfp9Lr9SgtLWWZYxqArdPpMGHChLSBJinekaQQKcYkQyKRYOnSpVCpVCz9IJVKUVZWhrKysjFdH58MmA4kfkayRhUVFSnHGAwGzJ49O4GESJsLukZ+esNsNiMvLw8KhQJLlixBXl4eqqqqYDAYYLPZUijMF110EQsdKF1CoiWrVq0a98jb095TGQwGWCwWxr4kWK1WDAwMpDWW/v5+xGIx7Nq1C2VlZRgaGkrb4Uu03NLSUmzatAnt7e2sfHKyQjENIlqwYAHkcjn279+Pnp6eFH6WSqWCWq1mua10uS/q4unp6UFeXl4CGfHQoUOoqqqCSqVi09pNJhOrBjz77LMsYUt6pWazGa2treA4Drm5uYwCndx6LxQKUVxcjOLi4owTztLhtPdUcrmcbYWJDSkWi1nDQjrivtfrhcPhQG9vLzo7O2G32zNqU4VCIfT09CArKwsWiwVqtTrtBkAgEDCDKygowMyZMzFlyhSYzWbW4JDudcFgEDqdDnv27IFUKk1LLKS6I01wJ9ZqJBLBxx9/nGDkHMexwZhUCaDZPqFQCAcPHkyg7XR0dOD+++9PmA5P/z1x4gR+9rOfobW1dRx35AzwVCR12NnZyai8wEiwTEnKZMRiMXR0dCAcDsPn82UcCUeiH8ePH2d1PofDAZVKlbIkUGwnlUrh8Xig1WphtVqRnZ2NwcFBtLa2puSsADDjP3bsGCZNmpQ2BZGfn4/W1lYMDQ2xXRowYvR9fX1svh8Vrx0OB7Zt24apU6dCp9OxpdLlcmHnzp0JQXo4HMbGjRtRWFiI6urqBJ2uPXv2YP369eNOKZz2nopKKSTzQ/UuimXSFX2Bkafx2LFjo9KFSaPJ7XazpCip3fHPm5WVhfLycjZQiYTE2tvboVAooNFo0NraCoPBkBJf+Xw+loSl3FPydVZUVMDhcKC2thbAZ40eUqkUWVlZcLlcTGeUQOUWvrZp8kxDPqgcRe9J+asvXds7IRQKMT4QGYLH42HKdckQCD6biZebm5sxa0ysAfJYVIwl0TGCz+eD1+vFhg0bAIwsr3/84x8RDofx9ttvo6KiApWVlRgYGEioLQIjeZ9wOIzW1taUuJBw6NAhNDU14Y477mCehJpmdTodm0GY/Bn5zbX8TUwmz0Nao3R91Bk0Xpz2RkXj14LBICOmUUY6nR4UMNKpTHHUaGwGkqhWKpUJhkdGTCARs+QCMCEdQ5RAgyDb29szZt8fe+wx1NfX47777kv5++TJk3H77benZM7JI6ZLk6QDeTNqX+P3R44Xp71RUWs5n4NNM4hJ3DUZ8XgcHR0d8Pl88Hg8GTPv0WiUHUMVfBL+/6KaDUKhEAKBABsbN5rqDPXyAZ/xomh5TTYqvgYqXxOddEKTkWxs5OG+lJ7q8OHDcDgcyM3NZaUa4mN3dHRkVCzZuXMn+vv7mSRhOvj9fhw5cgSbN29OWHJo0sMXARrkODQ0hCeffDIjU/TAgQO46aab2E5PIBDA4/GgtbU1beaeeFl8Ixxt6Wtvb8cbb7yR8DnHOkQzGWdM7a+kpARz5syBTCZDe3s7Pvroo4wSP1OnTmWUELfbjYqKCuzfv5/lbpKRl5fHdkYcx2HXrl0pCn18CIVCpvjrcDgyPu0ikQiVlZUoLy+H3W6HzWbDvn370NLSknbZlsvlTMZILBajr68voekh+VibzYbi4mJGuuvq6srYHSORSFBWVgaFQgGlUokDBw6kLXF9KWp/hClTpuAb3/gGIpEIjh49iqamppTJT3K5HA899BDmzJkDhUKBt956C+Xl5SgsLITX68UPf/hDvPvuuynnXrZsGVasWAGbzYb29nZ4vd6M4qpyuRx33XUXVq9ejXA4jCeffBLPP/98ipGo1Wr85Cc/wYoVK1iOiTYY69atw49//OOEhyInJwd33XUXm6AVi8Vgt9vx4osv4u23304xlLq6OuTm5rLqQjweR3FxMd5//33GpSLIZDKcffbZmDFjBoARr1ZWVoa3336b6a2PB6d9SoFgs9kQCASYyCpJGfKRnZ2NadOmMarM4sWLUVRUBGCkdDNr1qy0gaxYLMaePXtYs8Fo2fSSkhLccMMNyMrKgs1mw+23354wpJxQXFzMSiD8aVpqtRorV65Muf4ZM2agrq4OHMfB5/MhGo3CbDbj/PPPT8sjp+x5IBBgNGGr1ZrCe1coFFizZg3mzJkDq9WKw4cPQyKRMNGRysrKjJ81E84YoxocHITdbseRI0dgt9vTFntJPojSDNFolAXcUqkURUVFaVMQPT098Hq9CAaDaGlpSbvtJ5SUlDCim1AohNFoZIbLh9VqZcyEYDCY8F8aCsBHUVERo71IJBJWRcjLy0tZjoRCIcxmMyZPngyTycRKWUVFRTAajQnHWiwWVFRUsDEo0WgUMpmMTQUba40z4f3H/Yr/oxCLxfD7/diwYQNCoVDa3VlpaSlL/pFcNP0XGLnR6dicXq8Xw8PD+OSTT+Dz+UZlJ+zevZuN8BAIBPj000/TCtuTcBo/Sclvp0qOGfnjPihnJpFIYLPZYLVaE46VyWSwWCzw+XxQKpVQKBTs3MkbDGrX0mg07PxU3olEIigoKBjXXBrgDDKqaDSKEydOYN++fQlT1vmorKxMyDADn01uGK1DxuPxwOv1or29HR6PZ1Rh/f7+fvT09DDjOH78eNpgmrb8dD3EJ+cbGoGM7NixY6wDmr+TS9aQkkqlUCgUGB4ehlQqZaxVv9+fUl7S6XTMAAcGBhAOh9nY22AwCL1eP25K8RljVNTiREnQZOOgLmZa3ugm8mfIyOXytA0TgUAAEomEzfs7mRg+FbUpu58Ocrk8YRgT7SyDwSCrDPCPpVl/fAMMBoM4ceIELBZLwrkpe07noaWSxrLxQYwGu93OBGqp+5k6dzKVujLhjDGqwcFBqFQqcByXNo8kkUhgNpsT6ln8JcfpdEIqlaYNwqnVyuVypYiUpQP/xmeCRCLB4OBgynChSCSC4eHhBCoOeTS9Xs9iKnoIQqFQSkxFBkrH0cSwdD18tGkhzQViMzgcDrS0tECj0YzbqM6YlILf72fEtHS6lpFIBE8++ST27t2L6667jsUJtJS89dZb2Lt3b8bko1KpxPDwMKxW66ikOzI4foY/HSiHRYJowGfGkzwAic5JLWP8zmOhUJiWiUHnlEqlbKQuidryIZVKEYvFWELV7/fD7XZDLpeju7sbHMeNW/fzjPFUHR0dTAoaQEq5g+M4bN++nbU+8Zc+YMToXnnllbTNAXxVPkoBjAbKXo/G+qTuYlouyauIxWL4fL4UDjkVeslrkjIfNWLwQRx9CuZJU8Hj8WDLli0Jx5LcJL1HNBqF0WhEPB5HbW0tUwIcD84YTzU8PIzjx48jNzeXCfAng6glXV1dMJvNrCuFfpcJ0WiUySjSDcoEgUAAp9PJYqVMmXfqSFar1YyxSXOa+ZNHgZH64OHDh5Gfn4/h4WFGQvR4POjp6UlRlgmFQoyfRVwqoVDI9LL4+OCDDzA4OIg5c+ZgeHgYZWVl6Ovrg9frhd/vx/bt2zMWyjN+B2dKmUYgEKC4uBh+vx8GgwHd3d1pJ2kS7aW4uJi5fprmmSmoVigUbKq7xWJBb29vxvl8UqkUVqsVFosFIpEIPT09CbNsCHq9HjKZjO3IyJPEYjEYDIaEaRXAyG7RYDAwb0L/aJBlsveUSCTQ6/XMU9GUVv54ueTPSNdB40rsdntK8XksZZozxqhmzpyJu+++G/F4HDt27MAjjzyS1kikUilWrFiBxYsXIzs7G263G7t27cILL7wwbh0mPoRCIRYsWMDKOR0dHdBqtVCr1Th69Ciee+65FC8hkUhQW1uLCy64AJWVlfjTn/6ETz755KRKMf8O1NTU4Lvf/S4cDgcsFguam5vxi1/8ImWn+6Wq/S1btgwVFRUspfDss8+m7Z+rra3F6tWrEQqFUFhYiN27d2PmzJk4evQoNm7ceMrvb7Vacffdd7Od19SpU1mG+uyzz0YkEsHPf/7zBEM/66yzcPXVVwMYCdwvu+wyNDQ04JFHHhmXas0XgXPPPRcrVqxg+a/FixfjlVdeSSH/jQVnhFEJBALk5uYiFouxXBN1KSeDGhgGBwfR29vLkns1NTWfy6hKSkpYRwtJGfGHW9fV1UEmk7GgWiAQoKGhAXK5nA3rBkbKMbT8fFGYNm0ali9fDoFAgL6+Phw9ehStra0s0y8SiVBXV4doNMqy9Xa7HdXV1V9eowLAdkK0g8ok1kV6UzTKlVb/dAVoPmiXlimWysvLg0wmQzgcZt6KGj4VCgVrXuUbVV5eHvLy8hImSej1elgsllGpNWq1GmazGV1dXQna6JmSsmeffTZWrFjBEp8HDhzAt7/9bfb3WCyGn/70pzCbzZg5cyYEAgEeffRRvPfee6N+J5lwRhgVx3EYGhqCSCSCy+WCUqnMmB8iTU/+tj8ej0Oj0SQwJAlU3a+srIRGo8H69evTehESdyVRfpLu4Uvz5ObmsjwYNacS753kfORyeVoCIG0wamtrMWXKFOTm5uKTTz7BgQMHkJ2djbKyMvz5z39OeZ1IJEJhYWGCJFBXV1dK2z7JKfHfazTBttFwRhgVABZkt7W1YcKECRnzQ9SdS7OIKfstl8sTqLYkA93Y2AiXy4W2tjacf/75uPzyy/HKK6+wvI7H42ESjjR6g7qMybA2btyI5cuXJxgLjVDzer2sjKJQKFKkkFQqFaxWK/Ly8ljHjs1mg1qtxiWXXIJLLrkEsVgMfX19UCgUCTkr0kWvrKxk3djASB8hlYn475OTk8MSsPyJX+PFGWNUxGj0er2jtiIRVdbv9yMSiUAmkzH9BZLfkclkWLJkCRverVQqWRd0bm4uVq5ciVgsBrFYjO3bt6OtrQ1arZbxluh9gM96B+PxeIJRkQFR4hH4TD2YvxRXVlbiyiuvZGPV6B+/7YrGoIhEIhgMBhiNRiYEO3nyZAwMDMDtdkMqlaK8vBxKpRJKpTLBqIjuQtd9sh3eaDhjjIo8DBVFM1XWKYAmgwqFQmzZoxJGKBTChg0b2FNL+gUUdPPLJPS3vr4+bN++HXV1dayYS6+3Wq344IMPEnJJVI/zer1sjkw8Hk+QXwRGGjoPHz6M7OxsZGVlMc0Fvp4n9SaGQiHcdtttqK2tZTpdbrcbOp0ONpuNZeGpaMznhVFNEfhsstapYtxlmu7ublx55ZUwmUxQKBSYMGFCwmhWjuPwwx/+EDk5OVAoFFi0aBGOHTuWcA6n04krrrgCWq0Wer0e1113Xdru3fGAVPA8Hg9L4KXDyy+/jCNHjkAsFkOlUkEmk8Hr9eIXv/hFQgwRiUSYaBj/v5FIhLE0+bHTM888g9///vesgEtzkUmk9sEHH8Tf//53dn6fz4e//vWvLOFJ2LhxIz799NOE7zMQCKC9vR07duzAli1bsH37dnzyySfYtWsXdu/ejT179qC1tRWxWAxbtmxhS2FZWRnmzJmD3t5ePP/88+ju7mbTHpJLL/Sgkac6FQFZwrg8lcvlwuzZszF//ny89dZbsFgsOHbsWIJbf/jhh/Hoo4/i2WefRXFxMe677z4sXboUhw4dYt7jiiuuQG9vL959911EIhFcc801uPHGG1PGvI4HWVlZbAdEFf10oKHWNH2TPE53d/e4udh80Lg0euLJk1BGm7L3BK/Xi3Xr1qGmpgYWi4XVFD/44IO00oxjvYbNmzdjy5YtzJiBEW33aDSK5557jhlpMtGQurwtFgsrLJ8qxmVUv/jFL5Cfn4+nn36a/Y4vZs9xHH7zm9/gBz/4Ac4//3wAwHPPPYfs7Gz8/e9/x6WXXorDhw9jw4YN2LlzJ6ZOnQoA+O1vf4vly5fjl7/8JXJzc1PeN1nKMF35Zd++fVAoFNi+fTtUKlVGUYlYLIaPPvoIdrsdVqsVHo9n1LLLeOD3+/Huu++yCaA0HdTv96fd7geDQWzatImN0/V6vac8uYoPYiQkt56NRoN2uVz4zne+g7Vr10IikeCRRx459d5Gbhyorq7m7rjjDu6iiy7iLBYL19DQwD311FPs78ePH+cAcHv27El43dy5c7lvfOMbHMdx3B//+EdOr9cn/D0SiXAikYj729/+lvZ977//fg7AF/ZPKBRyVquVs9lsXEFBAWcymTiBQJDx+Pr6em7GjBlcY2MjV15e/oVei0wm46ZMmcJNnjyZmz9/PieRSL6wcwsEAm7u3LncNddcw82fP3/Uz5j8ukx/c7vdJ7WTcXmq1tZW/P73v8ddd92F733ve9i5cye+8Y1vQCqVYu3atUw9JTmRmJ2dzf7W19eXQuoXi8UwGo0Z1Vfuvfde3HXXXexnj8eD/Pz88Vw6g8ViwZVXXomJEyeyJdDpdLL6X3IOSiaT4Zvf/CZLE2zZsgX33nvv51oqAbCdmNFohMlkQklJCfx+P/r7+5nHGk/bebLWOXUU3XLLLZgyZQqOHz+Oo0ePMm2u0cB9znLwuAL1eDyOyZMn42c/+xkmTZqEG2+8ETfccAOeeOKJz3URJ4NMJoNWq034lwmUAsiEWbNmob6+HgKBgHWwqFQqnH322axRkw+aJG8ymWAymVIeiFNFcXExU9Hr6+vD+vXr8d5777FW9pPNhLFarQmphcWLF7PPLRAIcO211+K1115j00Vramrw2muv4corrzzptdHSfaoYl1Hl5OSkJMWqq6tZazl1dSRna/v7+9nfSOGOj2g0CqfTmdIVciqQy+UoKytj23ki4xHMZnPCJFDiGlFnSjKILkKjNjjeSI6TobKyEgsXLmQTHqqqqlj7lsViQXl5ORMJoXSGx+OBQCAYlW0plUrx85//HL/73e/wve99Dw888AAaGxtZ4lOtVmPNmjVsmABpR+j1elxyySWjPnRqtRqXX345Hn30UaxZswYGg2Hc82nGZVSzZ89OUTBpbm5mT3hxcTGsVmtCYdbj8eCTTz5BY2MjAKCxsRFDQ0MJO5z3338f8XicdcieKoqKirBq1Spce+21UKvVKCkpwfTp09muUyAQwGg0wufzJSxf0WgUUqk0pScOGMlrUbY+Ho+PWV5HIBBg4sSJqK+vx9KlS3H11VcjKyuLkeYCgQBCoRDMZjPT7qROmXSitnwUFBSguroaM2fOxNe+9jUsWrQowbNoNBrG56KHwGazQaFQoLCwMKVRgo9zzjkHN910EyorK3HHHXfg0UcfxW233TauvNW4Yqo777wTs2bNws9+9jNccskl2LFjB5566ik89dRTAEa+yDvuuAMPPvggysvLWUohNzcXq1atAjDi2c455xy2bEYiEdx222249NJL0+78xgKJRIKCggKsXr0as2fPRiAQwMUXX4xp06bB6XTik08+YcfqdDr09/ezbLlQKITP52Pt4cngD9bm/sX8HAtIbnpgYICNFeHHM59++ina2tqwZMkS5OfnM/GzYDCII0eOZDQquVyOa665BgaDgeXJKKdGoIHcLpcLOp2OeWy73Q6lUpn2MwiFQtTV1eGqq65i0t7ASFlrwoQJ49oJjsuopk2bhnXr1uHee+/FAw88gOLiYvzmN7/BFVdcwY6555574PP5cOONN2JoaAhz5szBhg0bEjLcL7zwAm677TYsXLgQQqEQF154IR599NHxXAoAMNrKOeecg9raWng8HrS0tOCss87C2rVr2ZR2jUbDBhppNBr09fUxowJGPBVJKvJzV8DIcllRUcGWkYKCgrTFVn5ZyGKxYO7cuRgcHER/fz/roeN7E+5frV6xWAwej4cNb8wk3yMQjIw5ufPOO7Fq1So2cImWdovFwrhQpAe6Z88eLFiwgHHaDxw4gEmTJiWcU6VSobCwEFdccQVmz57NRsORUcnlcjbuZCy68MAplGnOPfdcnHvuuRn/LhAI8MADD+CBBx7IeIzRaPxciU7CBRdcwModVA6x2+04duwYpFIpurq6sG3bNpYj4utLkfIeLT3EC082qoKCAhiNRlZjq6ioQGlpacKU9enTp2PBggUARmqQNpuNjZu1Wq1siUvmzXP/KvHo9Xq4XC6IxeKUyoJCocDy5cths9kwadIkxjunZRQYSaSSHHcgEGCdMfwxcwDYayh+q6iowA033ID8/HwWf5EaDr0H0Zrlcvm/z6j+L2HLli3Iy8tDIBDAzp07EQgE4HA44HQ6GeWEWpRouCLHcZDL5WhsbEQkEmF66m+++WbaAJamSJBXUCgUmD17doJRkZJfbW0tJk6cyMpFra2tkMlkUKvVGeMw6rWjZTaZflNWVobLLrsM+fn5CcYSDAZZN04sFkuQfaSOHHpfOsZgMMDv97Pyi1wuh8fjwY4dO1g/JH9zQ6Jp4XD4yzOarbm5mY0yUygUTBVPIBgZLUJlGyo50JITjUYTdlexWIwVmPk7RYFAgKKiIrZDJKOcNGlSQl7o2LFj+M1vfsPqfhTnUJmG+u7SSShKpVLIZDKm2c4vYgMjA4yuu+46lJeXo7a2Fnq9Hm1tbUyi+pxzzkE0GmWlGPqcLpcLWVlZTCA2Ho9Dq9XC4XAwj3ngwAF0d3ez5luOV6Sma/D7/RgcHExbxciE09qouH/VsUi8jI90TaEikQhOpxObNm3C66+/nvA0Zmdnp0gJCQQC9Pb2skmjYrEYLpcrY5KWryJDCIfDGanB3L/YoTRnjww9ecmi5gx+4Z7w+9//PuV3SqUSdrudhSDkqUKhEL773e+y1A2p7aWjXX8enDHdNADYlvxk4rCUQiCjoqWR+un4kMlkqKioYBM/29vbv1DNT4PBAJVKBY/HA5VKBafTOapWA+W5aBlMl9kn0Y3W1lY2b0YsFrM5PG63O0UQjs5N4Der8vGl6aYRCARYunQpzjrrLOh0OrS3t+Opp55K67LLy8uxaNEiNDQ0MC+xe/dubN26NWXQEDDiaQYGBnDNNddAo9HgqaeeyqgjOl6IxWLMmjUL8+fPx8DAAEpKSvDGG29g/fr1GSUUf/jDHyIrKwsqlQpPP/102mYNj8cDs9mMb33rW5g9eza6urqQl5eHd999F6+//npag7LZbFi7di3jZeXk5OCNN97A3r17x122OSOMSqVS4corr2Q6A5SkfeONNxK+ELVajbvuugslJSWIxWLw+XyQyWRYtGgR5s2bh+9///sJSVmBQIBp06Zhzpw5WLhwIcLhMG699VY0NTVh3bp1p8zhJlCvIsUwND/mww8/THtuo9GIGTNmwGg0QigUYvr06WmNqra2Fq+99hqys7NZnBSPxzF9+nRccMEFuOiii1IkGouLi5GTk8MmvIrFYhQUFCRsSMaK015LQaFQ4Nprr0VFRQVT7vX5fLjiiiswZcqUhBgpLy8PJSUljECnVqshl8shkUig0+kwbdq0hHNrtVrMnz8fer0e+/fvx4kTJ2A2m3H11Vdj0aJFn+u6VSoV1q5dC2DEA2VnZ0Mul7Mmi3SgXV0oFMooCAuM5BPJoAhkXDabDWazOe25aaQJNXEQ8XG8OO2NatKkSVi5ciXcbjcrvzidTnAch2984xsJCcfS0lKoVCqWbqDdHPCZPCOf7VhcXMziDJJNnDFjBlQqFebMmTOqxA7Fd3zwfy4rK0N1dTWbNioQCFBaWgqxWIxzzjkn7Tm9Xi+GhobQ1tbGRMrSIS8vjz1MfHYqxUfJ5SiRSASr1cra5GUyGTiOg9VqTStXeTKc9stfaWkp1Go121Jz/2q90ul0TCKHlhIS8qIAlzQIKCHIJ/4DI2xQl8uF8vJyRCIRpsT38ssv4/XXX8/YqpWbm4sZM2bg0KFDbIk1Go2w2+1oamoC8FlXDzENfD4f9Ho91Go16uvrodVqU2LC4eFhuFwuOJ1O1NbWZoztlEolwuEwM2IyqmPHjkGj0aQtnJeVlbHvifJUVGEYL057o6J8FPAZp4jfyZKTk5OQXvD7/ezpo10UgBSlFWCEKfnJJ5+gqKiILQW9vb145pln0jZ7NjQ0YMKECaipqUFbWxs0Gg1cLhcCgQCOHz+eQNGdNm0a26lSrsrn8+HAgQMoLi5GVlZWilHF43G4XC54PB4EAoG0mgs0B/nAgQMoLy9n/X4cx6Gzs5MZPR/kLUlvnbqGJBJJxlF0o+G0X/40Gg1LCfATlNR0kI6rTllj+n8AGbfnLpeLDT0aHh5Om0qQy+XIyspCVVUVqqur8f7772Pjxo0QCoWYMGECioqKUFtbi6lTp7JOn7q6OqaGJxAIEA6HMTw8jI8++ghisTgttwsY2dIPDw8jGAymDeZpKaPrBj4rB8lkMgwMDECv1ycsxfxUTGdnJ5qbm5mY2ngFz4AzwFOp1WrWdkTxERkN9cjxQR0j1PpOZRF+zMGHXC5n9JGhoSFYLBbodLoET0VB8KZNm7B161aWod+1axfEYjGKiopYuSMejzMZSIrXKDMfiUTQ1NQEn8+XwP1P/rxOpzNlCBOBSIqU0CTjEYlEsFgsTNWFXw6inkKJRIKOjg44HA4sXLgQsVjslFq1TmujEovFrDEyGo2yRlLyUgqFIuMQSQrUKZDNxA6wWq2MLhIOh6FQKDBp0qQEIbDkxoxkJG/fSdczWXPBbrfj+PHjaG1tRUlJSdo2fLvdzlir6ZZgKv3Y7XY4nU4YDAbGq+ru7sahQ4cwe/ZsVlAHRna5RHfp7e1lWqSZPP3JcFobFdW8jh8/jqqqKlZFp90d1eAIJO1DzAYKYAFkNAqhUMi8HTFECwoKUtgM40E4HMa2bduQk5PDvFU8HkdbWxsGBwfh9XoTtA/4OHHiBDQaDYaGhlKMlc7j9XpZT2BjYyMbo7tp0yb09/dj7ty5Ca9RqVSQSqVMCef/t/fl8VHV5/rP7Pu+ZDJJJvsyCSQBIpsYCCKL4EUBlSvdxILWrd5aW/Wn7bWt9er91OtVe+2mqFWxVa9UBblFVJR9xwgJS0I2si+TzCSZ/fz+SN+v52RmAom2NoHn88lHMzmcOTPnPd/lfd/neQYGBtDY2Ai32x3jEHEhGNdBBQzRxsxmM1asWMEELwYHB1FZWYmTJ08KKE+tra04duwY9uzZwxbpZOGRmprK6Ol8nDx5EkeOHGE6Bh9++CF27tz5pckBL7zwAjZu3CjYHIRCIYRCIWzbtg0DAwNxp+Pm5maYzWacOnUqLq0sFAqxTPjJkyexceNGAF+sHVNSUlBZWSk4d2dnJ95//33I5XJ4vV5EIhFUVVWhs7MzRqjtQjAhan8i0ZDDudPphEKhQE9PD/NI5oOmvEgkgszMTNhsNvj9frbNpyCLB77J4khMGolEgrlz50Imk6G/vx8dHR0jmkgqlUpYrVaWeGxtbT1vmwm/EJ7oWmntdvnll8Nms2HHjh2MOxBvWiW3dwoHotdftLW/7OxszJgxAwqFAmq1Gt3d3UhLS8OOHTsEXwrHcbDZbEzBLj09HX19fXjjjTewefNmnDlzJuF7XCgly2w248c//jHTV6isrIxLzJRIJCgvL8fUqVORmprKFvG1tbX49NNPUV1dnfA9zke+iEajsFqtePzxx3HVVVdBJBLhzJkzePHFF/HHP/4x5rOo1WpMnz4dn376KQvosrKy8/rwJMK4TykAQywfi8XC2MYcxyErKyum6c5iseC5557DT3/6U6SmpiISiUCr1eI73/kONmzYgMsvv3xU76tWq2N2R1qtFiKRCP39/VAqlYJdHh+FhYW4/vrrYTQaWRF3cHAQLpcLt956a9xSCh82m23E7f6iRYuwbNkypoyXlZWF9evXx2TTafRNTU1FZmYmUlJS4HK5mNH3RZn8JIaMWq1GSkoKSw2YTKaYMkpGRgbcbjdT0KP2WkpJlJWV4dNPPx3x/UpLS2GxWBAKhaDVanHgwAHB00xCF8FgkJETho8MEokECxYsYCrE/KSoWq1GVlYWLrvsMrz//vtxr0EikeDGG29ES0sL3n77bbaBoI2KWCxGRUUFvF4vK1NR33lJSYmgCH3ZZZehvLwcM2fOxPXXX8/6ujweD7KyslBdXY1t27aNStx2QgRVWloaU9Lz+/3shhsMBsEOKTMzk/Wk8zsiw+Ew9Ho9q70l6pXSaDR46KGHcPnll0MqlaK3txdXX321IKh8Ph/TQ7fb7QiFQnHPd/LkSZSUlMBms2FgYIAFtslkQnt7uyDlMBw2mw1lZWWQSqWM2VxfX4/33nsPwBdkjUOHDqGiogLRaBRdXV145513UFFRwYLKZrPhxz/+MVwuF+tDHxgYQHt7O3Q6Ha688kpceeWVUKlUePnlly/4nkyI6U8mkzESJjXfDQ4Oxgz1u3fvxqOPPsoy8LTgFYlEeOONN/DSSy+NuEieOXMmY/xS5j6ezrjX62Xi9h6PJ+ackUgE77//Pn7+858zpgoVcn0+Hx544IGEO0yZTIYVK1ZArVYjMzMTK1euxMyZMzFr1iy2k5w+fTo8Ho+gukC9/KWlpWzaLCsrQzgcxkcffYS+vj709vYiGAwiIyMDqamp8Pl8aG9vx/Tp00flGT0hgur06dOsFEE36dy5czEWas3Nzfi///s/RmPiU5wOHDiAAwcOCG4k/U0kEiE3NxcPPfSQoGOUOiz5IAYKCecnqptRhp7YyaRpRUE5HFRmWbRoERYvXsyEa0nfijTbxWIx5s2bh/3796OkpERQojEajZBKpYy6/8EHH2D9+vV49dVX4XA4EAgEoFarcfbsWRw5cgTNzc2488478eCDD44obDsc4376i0aj2LJlC771rW+xpz0ajaKysjLuDoqICbStpoo8P2EKDE0hTqcTGo0GM2bMwDe+8Q3WUkLdDWSANBwUUPGK1HzQDacaIBEN+IGt1+uxaNEi6HQ6XHbZZcjIyIDf72euXqFQCCaTiUlMEsNmx44dWLdunSAf53Q68de//pWlBEKhEPr6+pCUlITGxkY4HA40NjZiypQpaGpqYuu90QrSjfugIlAOihCPbkVPLH+0oZvK38VJpVKsX78eU6ZMYVaw1DJD/enk1xyPCxeNRtkudKR+JApQPoWL2ncIFRUV+N73vofe3l4WNCS9TRLZwWAQXV1dTAj3oYceYubjwBeOXikpKbj//vsFGwOO45ieOxXVW1paWP1Ur9dfnEHF/c1digKLOjvjgU+3opob8foI4XAYTz/9NAoKClBWVsaa96qqqnD48GGo1WosWbIEoVAoJuPMcUPG2W1tbWzUTAR6T6pdkhoffxu/a9cuqNVquN1u2O12qFQqBINB7N27F++//z5uueUW5sVDmxK/34+8vDwBjUwsFiMnJwdKpVIwvcrlcqSlpbFrJVoZMNShMbwgfyGYMEFFFXXqVoi3yCXen0gkYtMN0ceH33yqnx08eJDdZP459+zZE/MawePxoKOjAxaLZURmD0kk8bslhqdBOjs7sXHjRsHGgj4vADz66KPsOvjXMmnSJFbGoUw59cDzd6tisRi1tbXo6OiAz+djSoMGg4GVrkaLCVGmAYa6CSwWC6Mktba2xuWzKZVKzJ8/X9C8Fg6HcejQoRHt2UaD9PR0ZsQIICFPUKVSYdasWWyKJB7eZ5999qVF1WbOnAmz2SzQhpdKpdi1a1fMoptGbdrAUICS8yl/93rRlGmAoRuX6Obx4ff7sWXLlr/bdZAIyJVXXolwOHxev5tZs2axdMDevXvx5JNPfumAAoZMCqZOnQqNRsOSvGQkEO+a1Wo1iouLkZ2dDZ/Ph3379qG7u3tUdHfChAmqscJkMmHu3LnYu3cvent749LTRwOtVosnnniCMWKmTJmCH/7whzE3RyQaUr9bt24djEYjOI5DcXEx+vr68Nvf/vZLqQMDQwZMU6ZMgU6ng8ViQXNzM3p7e2Gz2WJSFqTPfvbsWXR0dLAEcnJyMlpaWkZ0t4+HizqoJBIJbr75ZixatAg33XQTvF4vmpub8ctf/jLuri4rK4vplwYCAchkMuZEBQylIZ544gm43W4AQ+uchQsX4vvf/z6efvppwSiRl5eHn/3sZyzFQbute+65Bz6fD7///e+/1GeTSqXweDxQq9XweDzgOE5QtuFDo9EwOSWfzyfonCUvn9FgQiQ/CVKpFJmZmYJuxZEctTQaDXONMhqNcDgckMvlcbluGo0G8+fPx8MPP4wtW7bg5Zdfxk033SQ4du7cuSgvL2ftxLQjvfnmm2N6zqdPnw6LxYJwOMy0R6lT4XyKgpQhH4mTR9n+trY2hMNhRl8bPmKSuwM5eiUlJcHpdCIjI4Ppu48WEyKoJBIJsrKy8KMf/Qj/9V//hZ/85CeYNm0asrOzsWLFioTdi3PmzGHuWaTOUl1dHVOZl0gkmDFjBiwWC5KTk6HRaGAwGGI6EGbPns1GBK/XyzTUiZjJh8VigVqtZjtSvoaW2WyO+yBIJBKkpKTgrrvuwpNPPon8/HxIpVIkJSVh5syZgmP9fj/TmuJrYw1fUxHBoaKiQqAaE41GmUbpaDGupz+LxYKUlBQsXrwYxcXFEImGlOHmzp2LefPmIRKJwGAw4K677orhyCUlJeHGG2+EXC5nOx+pVIqqqipBsk8qlaK4uBiFhYVQqVQskAYHB1lnBBkBJCUlCVzTA4EA4xoOVzWmZkH++4RCIRY41CkADLX2FBcXo7y8HEVFRYzlsnHjRpw5cwZarRbnzp3DwYMHWWCQkZLf78e5c+dgt9vjCpCQ+8XBgwdZLzt1z5KvTbyUy0gY10E1f/58uN1uaDQa1NTUICUlBT6fD8FgkMn6DFeTA4bSCv/2b/8Gm83GSJcSiYRxAkUiEbKyspjENZVixGIxzpw5g2g0inPnzkGr1bIRiEyEqAuC0hW0+yLyKP8agC9KNfw8E79PS6lUIiMjA3l5eYhGo6iurmZrHZ1OB47jUFtbi8rKSsEI29nZCbFYzIyNMjMzGW2eD7lcDrVajaamJtb9yddCjSfEdj6M66DatGkT3nnnHTZ1USadck/UDhsOh6FSqZCWlobU1FQsWLAAbreb3UQ6pq+vD5mZmaiursb69eshEong8XigVCphs9lgMpnQ2toKl8uFSZMmQSwWMyMCUhfu7+9no59MJmMMn+Gg4AO+yKzzuwro30QiEZw5cwaNjY0Coiw/GQogJp8UCoWgVquhUqnQ0tLC1pbDk7EymQwWi4WlY+gB47ghmzuNRjPqRr1xHVSUMb6QFMCNN96ItWvXwufzscy7VCqFSqViFrlEO+/p6cFvf/tbuN1u5OfnIxQKoaamBkeOHMG5c+ewdOlSNkpQCzKdr7q6GiUlJRCLxWy05HMSCbQoj0ajMQHC12EIhUKCDHiiGzz8/HRumtppxIyXd6JRka6B5CyDwSBb940G4zqoRoPu7m6cPn2aFW+DwSD6+/uxbds26HQ6lJeXw+PxoLOzE5FIBLW1taitrcXmzZtjzhWvYY0kGM+ePYvS0lIBUaKnpycmCx2NRlFfX8/KRDT9UddBoht5oTeYTI9UKhULbupuGH6+YDDIxGPFYjEbXRNpy58PF01Qbdu2DR988AGbKmkRS7XA3/3ud6y1eCwIBoPYtWsXdu3ahaamJlbP6+vrg9FojJGLrKmpwV/+8hc8//zzbN3HcRxzZRhtbmg4aHTNyMjAqVOnkJWVxYKYDwq21tbWGO2G1NRUGAyGUY9UE6b2ZzAY4Ha7WZvwiRMn4vLixGIxTCYTgsEgo0WZzWY0NTUlXIxSAXpwcDDGp3j4uaPRKAwGAxPFb2trY0HDPz+NZPGmo5GIqjqdjuXhzudRSOsoquP5fL64Dw0t0NPT01nqIRAIoKGhIWaXelHV/mbOnIkHH3wQXq8XXV1deOCBB+KKyS5cuBC33norJBIJjh8/jtTUVNhsNmzcuBEvvfRS3HPLZDJMnz4ddXV1cDgcAgcJPux2O+bPn4+SkhKkpKQgHA7j9OnTOHz4MLZv3y5YJI8UDIkCSq/XY+nSpVixYgXOnj2L559/HqdOnYp7vFKphNlsRkZGBqZMmYJ9+/bhzJkzcQMrHA5DrVbjySefRE5ODoCh2uGaNWvG5Bg7IYJKLBYzQQvyIHY4HDFBJRaLsWjRIgQCAeTk5MDlcjEhseuuuw5vv/12XJ1QsViM+fPnw+PxIBQKxQ0qsViM1atXo6CgAMCQAyr1MBUUFCAQCODDDz8c82cUiUS47bbbsGbNGjQ0NKC4uBivvPIKVqxYgcbGRsGxGo2GaTF0d3dj69atGBwchNFohEqlQltbW1zun9VqZQJsNpsNOp3u4g0qEugaGBiAWCwWWL3yoVarUVBQALPZzOxnjUYj6240Go1xg4qSnTNnzmSMleHQ6/XIysoSiH1QIlSr1SInJ4cFlVQqxdVXX82cSymlAYDlllQqFf76178y0VepVMq6DmbOnIloNIrW1lYUFRXFBJVOpwMA5mhPUy9fa2J4aoEE//k76bFw/oAJElTAFzoEhw4dYiyR4TCZTHC5XIIi6+DgIM6cOYOSkhLk5ubGVacTi8XMwjYjIyPu+6vVauh0OlbvozyTVCoFx3FMeJ/jOJhMJixevBiZmZmsWTAYDEIikcBgMLBFfmtrKwsq6qXfsWMHli1bBr/fj82bNyeUHKIUQn9/v0C+WqfTxc2bkXgJlZaoG3UsmBC1P2Km9PT0MJu3eCwWMr/u6enByZMnIRKJ0NvbC41GA7VandDFi0gQLS0tSE9PZyMBH2azmTX+8Z9w/mhFN5NUlPnZa+qD56cXhvfcUxadnzglb8PhaG1tZcHJd6ePRCJxg4rSCnx3+wv1ohmOCRNUVquVPY0ymSxub7XFYoHNZoNSqURKSgpb9yQnJyMcDid86tPT0xmVvrGxMa6ALAnUUq6Hnxmna6IAIk8c2v3xHeIDgYBAq5Mgl8vR1dWFzMxMNsVbLJaY1AMROfh+MhSkNA3GI2NQ4Hm9XpbbuqiDilpBTCYTSktLodVq4XK5Yo5zOp3w+Xzo7++Hw+FAV1cXmpqaWPmDnKSGgxgsPp8vIamSRMM6OzsZW0cmk0EikaC+vp7JMNL1koCaXq9nf1MqlQKvPX7ikZKoVquVBaTBYIjbAk3ZfBJ0o89EQRzvM9J7er1eFqhj6foEJsiaioytFQoFXC4XVCqVwKOPQAtxsViMxsZGtksChlpFSHVu+M6IRFapLSWRLCJNp06nk40yCoWCWZAQJauzs5M14RUWFrJFM63F6KbzJYiCwSBOnz6Nqqoqtk5raGiI2eEOlxmic/KJsYn6sEKhEDo7O+HxeJCbmztmYbcJEVQGgwFqtRqhUAhFRUUQi8VMq4o/hNO6xev1IicnB2fPnkU0GkVjYyPC4XCM3zKB4zh0dnbCYDAwPt9whMNhdHV1MYpTNBqFUqmEVCpFU1OTQMXF7/fj+PHjAMD+ez709PRgy5Yt7OEJBoOor6/Hli1bYq6HNhZUHAa+SHCST008kIKeXC5HcnLymIXdJkRQOZ1OiEQiFiwdHR1QqVRISkoSaHN2dnbiyJEjzOnBbDZjYGAA3d3dqKyshEajSTjk63Q6aDQaOByOuIlLqhdST5NEIoFSqWQy1nl5eV9afY9aj6n3Coi/7ae2FdrpUbMeiegm6ub0+/2s7tfV1TXm65wQZRqn04ns7Gx0dXXBYrFAp9Ohs7MTx48fFyxk+d56tBsbHBxkegSkfjcc1PFZWFiIhoYGVFdXx0jr0O4snkiYSqVCSkoKampq4gYWPzBGuh0mkwklJSXMS5ACdjiIWkXTtkQiQX9/P4LBIFvrxevsmDdvHlpbW8FxHJKSkvDJJ5/EHHMhZZoJEVRisRhXX301c3kQi8XYu3cv68jkQyaTISMjA9OmTWO7tLa2Npw8eRItLS0xo5BcLkdmZiby8vLYe1VWVqK+vn7MC1mC3W7H3LlzkZ+fz4LkvffeQ0NDQ0IqldlsxsqVK6HVavH6668nnI5lMhmcTifcbjesVisaGhpw5swZgTn4WHDR1P5EIhFWr16N3NxcAEOL2oGBgZigEolEmDt3LnMbNZlMCIfDrPH/6NGj+OyzzwT/xu12o7i4mI1uPp8PV1xxBUQiEWpqahJeE6UJEqm+KJVKTJkyBe3t7axvqbOzk1G7hl+7TqfDunXrcN111zGmy4oVK/DnP/8Zzz//vGCElclkWLBgAVOB0el0sNvtSEtLQ01NDfbu3XvhX+4YMCFSCrRVpqa0kQqyubm5MJlMqKioQE5ODoqKiuB2uyGXy5GTkyPYGZGgGk2TZPAok8nOKwWdlpaGu+66C7NmzUJqaiqzUyNoNBqWSiC390AgAIlEEjdtsXTpUtx+++1ISUmB3W6HVqtFSkoK7r77bsyZM0dwrMPhYAJug4ODOHfuHMvqj5XMMBpMmKA6evQoq6ORbPNw6PV6uFwuWCwWloGnfie1Wg2bzSbYGVEKwel0shwSbQCG95wPR35+Pux2O5566il8+OGHePHFFwVTtlqtZsFBiUly8opnNDRp0qSYXnbqby8sLBQcS735JpMJWq0WGo0GdrsdZrMZfr//vEo0WVlZrBxFAh7xaqmJMCGmv3A4jJSUFHR2dmJgYABdXV1xGb4ymYxpWVJzXjgcRjAYZItXpVIpyBsZjUakpKSgpaWFbc9NJhOsVmtCKUeq8vt8Pnz++edwOBzo7+8X9HcplUqmSsy/VrLWHd5PTolQfiKTEqbDXbFIC/T666+HUqlkWftz587hf//3fxOKhqjValx++eVYvXo1AoEA6uvrMWXKFCiVStx0000XdC+ACRJUHMehubkZVqsVHR0dkEqlccU5amtr8atf/QorVqyAxWJh5ZNgMIjKykqcOHFCQAknB/ba2lrWNy6TydDZ2Qmj0Siw4uCjqKgISUlJiEajGBgYQF1dHcRiMQs04ItuBGIE08hEFm38+hwdq1AoWFqA389OlnL8aZ/Ecvn97qTcxwepD5aVlWHevHmYPHkyXC4XOI6D2+1GT08PtFot1Gr1BVPxJ0RQAUBXVxfEYjHa29uZsGw8EFuXeHAKhQKBQADhcDjGMZ4W2z09Pewmq9Vq9PX1xRSVKVOdlpbGGMZarZadW6PRoKSkhOlZUfqCuggoALq6uliZhyCTyaDRaNgDwP8byQ/J5XJBmoCmSZrq4q01HQ4HysvLYTKZoFarkZeXxxy2iPtHI7der79gV/gJE1TEyB0YGIBOp4vbSgx8QY2i4i//iY+HSCTC6nShUAgajSbmidXpdJg6dSpbJxmNRphMJqblaTAYIJfLkZ2dLdCXGk63EovF8Hg80Gg0ggAgGWoqv/BHsUgkEpeqT4VlPv1reHeC3+/H4cOH2W5y165drBzFL0D39/cL7FjOhwkTVOFwmC0mdTrdiHYgMpmM2WQQ8TOeBRnZqVFXAL84yyepFhcXo6SkhFX3rVYrs44NBAIsuWo2m6HX69Hb28sKxMQQph0sX8mOQPVI2uHS9BcMBsFxHFQqVdx2FgpqCuDhn5EsgmlEpoAfPpVGIpFRKeFMmKCiXU1fXx+rc8UDPZXkpsVvVxkOkhX68MMP0dnZyYLKarXiqquuYscdOXIE/f39SE9PZx2odXV1aG5uhlgshtFoRCAQQG9vL8tbicVi1NfX49ixY6xsEo1GoVKpMGfOHMFNpa6D/fv3w2azsZ1na2srGhoaoNfr49b/+CMViezyJRuJEkZdn18VJkxQtba24sSJEzh79ixOnDiR8MniOA41NTWsZ1sqlaK7uzvueoGIC11dXTFrtIaGBpZ9HxgYwNGjR3H06NG4JZd41X6v14uzZ8/GtK6YzWYMDg4Kjh8cHMT777/PHOcpoz0wMACDwYAZM2bELMAjkQh6e3uZlifVA78KQbXzYUKUaYAvpjViBJ/vySN/4P7+ftZ7FA8UJJTvIRnokb42i8XC1kBtbW0XNArQQprjuLj0dLoOo9HINCAosBO1qOh0OuaK6vf7mal3IpCtCY1w8XZ7F02ZBvhiqgLO31wmFosxe/ZsSCQSgQN7IqxYsQIrV65kBIgNGzbgo48+inss5XRycnJgMpnw1ltv4S9/+UvCc+t0OlxxxRXIyspiBNCdO3fGHEedpw8//DAyMjIQDAbx+9//Hps2bUrYS+5yufDCCy+wAH3xxRcTiqnJZDLMmTMH/f394DgOvb29OHz48IjfSyJMiIz6cFC/eCIBM6PRiP7+fuzYsYO1+SaC0+nEN7/5TZa/KSkpwW233YbU1NS4x/O7OKlhLxE0Gg3uuOMO+P1+vPXWW3j33Xeh1+vxzW9+M2axrtfr8Y1vfIO1+LS3t+PKK6/E/PnzE55fLpdDo9GwDcJI1rWRSAQqlQqTJk1Cbm7ul1pjTZiRiqDVavGjH/0IycnJCIVCOH78OP7whz8IkprRaBQnT55EJBJBU1MTFApF3KFeoVDg4YcfRlFREVvUy2QylJSU4J577sH9998fM0qIRCJotVoYjUY0NDQkpDnJZDJcf/31mD9/PjMloo5OauzbunUrm9bS09PhcrnY+o92pDNmzMD27dvjjs50zR0dHTAYDCNqopMpksvlYuIjY8WoRqpIJIKHH34YmZmZUKlUyM7Oxs9//nPBBXAch5/85CdITk6GSqXCggULcPr0acF5uru7sWbNGvb03HLLLWMiLcZDVlYWCgsLUVhYiGnTpsHtdsesT8RiMXuN8lvxkJaWhpKSEgSDQbZz6+npgd/vx+TJk+PWw8hrprm5GWq1Gu3t7THHiEQiVFRU4N5770VqaipcLhdyc3ORnZ2N3NxcZGZm4pFHHmHaocBQeiA9PZ2tBYkSlp6entCRnQKaNBTOt0jX6XQoLi5GZmYmrrnmmhFH8JEwqn/1+OOP47nnnsOzzz6LqqoqPP7443jiiSfwzDPPsGOeeOIJPP300/jNb36Dffv2QaPRYNGiRYLhdM2aNTh+/Di2bduG9957D5988gnWr18/pg/Ah0ajwfLly2E2m1mSM97TSclHAKwzIB4sFgvLAfl8PgQCAQwMDECj0bDF+HDQ56yvr4dMJotLvU9KSmLeesPdKShJ6XQ6sWrVKvY6lYxoQS+RSKDRaGA0GuNSxvjnI52IkXJNCoUCpaWlCIVCaG5uZg1+Y8Gogmr37t1Yvnw5li5dioyMDKxatQoLFy7E/v37AQzdrKeeegoPPfQQli9fjuLiYrz88stobm7Gpk2bAABVVVXYunUr/vCHP2DGjBmYM2cOnnnmGbz++utxb8CFwmq14s4770R5eTkjD4TDYRiNRsFagk/0BMCa+uJNU06nkz2tlOOhXnipVBrX7IiSlCQiFo/mRFYglECl5COfdDFcvIOfGedbw9G1xQOxaWpra8+bwNRqtbDb7cwVwmQyjeh8OhJGFVSzZ8/G9u3bcerUKQDAsWPHsHPnTixZsgTAkKhDa2srFixYwP4N5VHIdmPPnj0wGo0oKytjxyxYsABisTih8AVlpfk/wNDuZsqUKVi+fDkefPBBVFRUsCeZWm6Tk5MFi2UiZNpsNjidTtjtdlgslrg6THwbEFr0U9CM1OtNwcGfZvkQiUSMc0i/80XKKLXAn151Oh1zgKfjBwYGmHdhPBD9nsyZRpr+9Ho9TCYTZDIZzGYza5sZC0a1UL///vvR19eHgoIClpl99NFHsWbNGgBf2GWQZCGBtMfpmOGiqlKpFGazOaFjw2OPPYZHHnkk5vWbbroJWVlZrFZG0kBUaqCRgD89LFq0CA8++CDa29vx8ccfw+Vyoby8HC+++CJ+9atfCc7PDxoKJH6tMNHClxbv4XA47gaANElpdOKTGKgTleM4Qb3NZrOx4jONUoFAACaTacSgAsA6G0Yaqex2O2NwBwIBGI1GTJ06VUAcuVCMKqj+/Oc/49VXX8Vrr72GoqIiHD16FPfccw+cTie+/e1vj/rNLxQPPPAAfvCDH7Df+/r6kJaWhrfeegvl5eVwOBwwmUzw+/347LPPcPr0adx0001QqVQIBAKCbLlSqYTVakVXVxeMRiNmzJjBisDDwXfX4hdYiUqeqIOS9BT8fn/coBocHERLSwtTIybfZaKIEdmU7+cslUqxd+9etLS0sHVXIBAYUXOdKFnUXjOSkFpSUhLb3Q4MDEAul8cl5F4IRhVU9913H+6//36sXr0aADB58mTU19fjsccew7e//W22xmhraxO027a1taG0tBTAULvF8B1ROBxGd3d33DUKgISLxtOnT+P06dOMhkQFWeALPh1fUYV+J630G264gTGU4y3W+/v70dzcDJfLxdYxUqmUTceJ+ouI1k5tNnwQmzoQCODAgQNIS0tjHn/kQF9bW4uXXnpJkJR94403IJfLY1Rp3nzzzRGvIxAIoL+/n/03HsRiMessJYVBaqWWyWQJOz4SYVRBRVI9fPAXl5mZmXA4HNi+fTsLor6+Puzbtw/f+973AAwZ/Hg8Hhw6dAjTpk0DACaqcT6ng0TgBxMhUXfjRx99JKCfBwIBRKNRHDt2LObYP/7xj3j77bdRVFSEzMxMps5y+vRp1NXVxS15cByHnTt3wufzoaOjI2ahHo1GceLECZw+fRqvvfaaQJdBpVKB4zh0d3fHBAoJuw7HSKmY5uZm/PznP8eJEyfw3HPP4ejRo3GPi0ajePnll7F//35kZGSgo6MDgUAAtbW1Y1J+GVVQXXPNNXj00UfhcrlQVFSEI0eO4Mknn8TatWsBDK0J7rnnHvziF79g+ZaHH34YTqcT1157LYAhdsrixYuxbt06/OY3v0EoFMKdd96J1atXj5h9HgkikQirVq1inZUksxPvyWxtbcXrr7+O7OxsXH/99fB6vXjxxRfj3hxybiA5nwtFe3s7SkpKAAy5gfLlifidAaPFqLTM/9b2UlVVBafTif37949Y9+vq6sLg4CDS0tJgs9mwadMm1NXV/f1p78888wwefvhh3H777Whvb4fT6cStt96Kn/zkJ+yYH/3oR+jv78f69evh8XgwZ84cbN26VaA/8Oqrr+LOO+/ElVdeCbFYjJUrV+Lpp58e9cUTtFotVqxYAavVCo/Hw0bCeBQqpVIJg8EAq9WKQCCApKQk1vnY0dHxpav46enp+NnPfga3282m2n//938f04L3QkFrLH6gFhQUwGKxsEU/acEfPXo0ZvScNm0a1q5dy8pbfr8fhYWFqK+vx4svvshkvS8UE6JLIT09Hb/5zW+YfA6NPvGKvmQZ29fXx7bvRHAYiw0ZHyaTCY888ggcDgfb9kulUjQ0NOChhx5i5y4rK0N2djYaGhowMDAgUN6jHBslToeLsJEYiUQigcPhwMmTJ3HNNdfg008/ZWkbuVyOmTNnoqysDCkpKaiqqoJOp8Pu3bvR1NQk2FUmJSXhP//zP2EymZjRNzA00lmtVpw8eRL/7//9PxaIF02XAnVxEikzEAgk/OAajQYZGRmorKxkT7fBYIDT6URPT8+IQUVtyIl0m1JSUpCZmclSBpFIBEqlEpmZmQItqauuugqDg4NMF4s2GdRIFwgEEAqFcPDgQUFQmUwmLF26FD09PSgsLEROTg6WL1+OyspKAf2dBEJIxtrv9zPX1uEJzd7eXhw4cAArVqxAf38/G/XUajXUajU++uijUfsfToigoi+KlFZ8Pl/CxJ1CocBll12G5uZmltcqKChASkqKQLqHQKNYaWkpysvLodFo8Oc//5nd2E8//ZQ18JHUNfV10YYgGAyy6Z8YNG1tbSgoKIBKpWKBEwwG0dfXxzx2+Os8YlRLJBJs3rwZW7Zswbp167BkyRI0NjZi+fLlePfddwUjME1jRqOR2e8O32j5/X48++yz8Pl8uO2229Df38/KW3fffTd279496vsxIYKK2k1I/U0sFscNKolEArvdjurqavj9fvZENzQ0CLztyDcwOTkZVqsVhYWFyM3NhV6vh1qtxowZMxjj5Qc/+AH74kkX1Ov1MgcFkUjEbNiAobabrq4uxjEkwgUZBtCUGYlEBDtAer/MzEwoFApYrVYUFxfD5/NhxowZOH78OKs7Uv4sHA6jvr4eLpdLQDMbDkq0Uh87HRNPbORCMCGCSqVSsfrdwMAAY5IMh1QqRVpaGjo6OljBmTLTKpUKVqsVdXV1kMvlKCoqQlFRERwOB5KSkqDX6yGXyxlFqq+vj1GYCAaDgdHMaZSixCMFeSQSQXd3t2Dao7UgJVX1ej38fj/LF1GbzODgINxuN2bPno1ly5ahpaWFjZR9fX0s5UBBQY5i77zzDurq6pgPz3AxOGDI3YGSr1R3HCs9fkIEFZVhaJQwGAysnZe/D6FgI1cIAIKSB+WMBgYGsHnzZmzbtg1yuZzJNvKLvfTDl5s2m81MhYXOS+p6FFRdXV0wmUw4d+4ckx8ym82MmEpqdpSwnDVrFrKysmCz2Zj33ooVK3Dw4EGcO3eOBQm/X4yCSqlUwuPxsCw8dUUML56TxyDx/agU5HK5cOLEiVHfjwkRVGq1mmXOu7u7YbVa4wYV3eze3l62w6ISSX9/v2DUoVxSf38/enp6Lug65HI5WltbYbPZBMVnWucAQ2uYHTt2ID8/H83NzQJHUprCe3t70dfXB6/Xi4aGBkyZMgVmsxk2mw3V1dU4deoU2tvbBeaX/Ew7BRCxioni73A42Eg+HOnp6QJpyGg0itzcXGzdunXU92NCBBWVcPhcukR9VFRYpZtOWWwKtC+DaDSKnp4eNrLR1NXR0SEI7ubmZrS0tGDXrl3sOPqh+iJxDpuamvDCCy8gOTkZBoMBfr8fHo+HjZpyuRwqlUqw/qEgUiqVjGEsEonQ2dkZV9NUJBIJ9NXpvyRFNNrc3YQIqkgkAr/fj66uLlZiiFfSIM0Fj8fD3LTIQPvcuXNj7h8iHD9+HNFoFG+++SaAL3qi3G53TK8YlXwuBIODg3EF3BKhp6cHe/bsQU5ODpKTk6HVauHxeLB//362geBDIpHg1KlTjD5Pu0f+wzcaTIjkp0KhgF6vF+gWDA4OjlgX468fEpkr8kHv0dXVdd4vmdwWqLY4lpLMSNdBrT7kWvFVgHJw/JEzHsn0okl+BgIBwfB/IeokNMWcD1KpFBaLBStXrsSqVavwzDPPYOfOnejp6YnrnJ6RkYFf/vKXcDgckEqleOutt/Daa6+hra1t9B9s2Ln1ej3mzJmD5cuXo729HS+//DIaGhrGLKLPx1jrkfEwIUaqvxdEIhFrnTYajZg0aRJOnDiB3t5eHD9+HB988IHg+KuuugqPP/44Sz+Qu0NDQwNWr1496sI0QSKRoLy8HC6XC0qlEgsXLkRdXR2OHDkCqVSKTz75ZFTT4/DPeL4Q4B9z0YxUw0E641/2edFqtZg0aRIGBgbQ3t6OnTt3IhQKsa7I3bt3C0bFa665hvWRUX94MBhERkYG3G73mIPKZrPh2muvRXNzM6RSKerr69Hf38+UmJctW4bnnntu1CONRqPBbbfdht7eXtTV1bGdKIl1RCIRpKamwuFw4Nlnn71gLuCECiqpVIqioiKsXLkSv/71r9HR0YHp06fj8OHDMYtiuVyO/Px8FBcX45133mF0b8pjAWDCtFarFe3t7WyXZDab4fF44HA4BCNESkoK5HI5mxZJO0oikaCwsDCh3x+VYFJSUlBbW8sc4ylNkpeXB5lMhsmTJ2P//v3weDwQiUSorq7GjTfeyNwq4tHBFAoFsrKykJ+fj6NHj6KtrQ1FRUU4deoUlEolbrnlFjidToGKDV/9RSaToaWlBRs2bLi4goooTeXl5SgrK0NaWhrWr1+P3t5e/Mu//AvWr1+P2tpa1tk4ZcoULF26FKWlpbBarXC73Th16hSKiorQ19eHxx57jJErP//8c0FZRqFQoK+vDx9//HFMfxKNkEQK5W8Ghkso0nVbLBY4HA5kZmayzgOv14t58+bhgw8+wMGDBxEKhdDY2Ai73Q63242Ojg5otVo4nU4cO3aM8QCBL9hCJCHpcDhQXFyMq6++Gk1NTayT9dlnn0VTUxMblYY70lMuiySLLnSnCozzoJLJZHC5XJg9ezYKCgqQlpYGs9kMjuMwZ84cSCQSaLVaFBcXIxAIYNGiRSgoKEBBQQFsNhsbCRYuXIglS5ZAo9Hg9OnTUCqVGBgYQH19PZqamhi5goLk1KlT2Lt3b4wwGTDUWbpgwQJBMXnv3r1Mg4rahg0GA+uXp6w5kSv6+vpQU1PD8kV79+7FgQMHsG7dOpSWlrLMuk6nwyuvvIIzZ84wv8CysjIUFRWhubkZOp0Oy5cvR1paGnp7e5n42vbt21nyl995mohNfSG7Yz7GdVB985vfRFpaGjQaDaxWK+x2u4AeZbVameovv5527tw5ZpNB5ASxWAyNRsPo8ATKgVGHAEnyDF+vKZVKqFQqbN26lWlX0U06fvw4srOzWf6H2lAkEglycnJYHkoikeDIkSOsDkjTKCVESUuLpqZIJAKHw8FKKTQqkqG20WhEdXU1C1CqLWo0GgQCARgMBlbiooCnWiWlLeh7IYrYhWBcB1VbWxtjCvf09KCjo4MFDsHr9WLHjh1ob29HXV0dm5qIB0c3jEANc3yQdjqVP+I90SSH3dbWJmDgSKVStLS0YNKkSYwHWFVVxSS0qfhMbB1q1KM8FP/8kydPZp0P9B4ZGRmsXBMKhXDkyBEYDAZotVq0tLSwdhdSYaa8XF1dHRsdaeoTi8UwGAzsWuh9yGH+QtMi4zqoqK+IzzCmL4SvNU4Yaz5ncHCQrT2ITTwcFAz8tQi9TsaMFAijXaMAYDSyvr4+xr6RSqXIyMhgIxAw9AC0t7fHXbQPR0NDA+6++26B5CS1DlELDFH949kAJ8K4DipAqIvw9wC105CIfiKzoFAohN27d0Or1QrEXqnL4dSpU18qxUHqxLRDpZ/hC+zRwOPxYOPGjWO+pkS4lPy8AFgsFrjdbhgMBsbXGy6PDYA94VOnTkVhYSHkcjkqKytx6NAh+P3+LxX8ZrMZc+fOZUbexOnzeDzYvXv3P0R2EbiIXLRGA6PRiIqKClRVVeHs2bMwGo2YNWsWTp06haqqqi81mlAzntPpRFZWFqLRKI4ePYqurq4xO6cPP/8111yD7u5ueDweNDc3j0i7kkgkuPzyy5GRkYGPPvpI0PvFh06nw6pVq5Cbm8v0UHNyclBVVYW//OUvgqn6os2oEzQaDfR6Pctki0QirF27FvPmzUNNTQ2r3qempkIqleKHP/wh6uvrRzznSNYhxF4JBoPMtGjSpEkIhUI4dOjQl2LqAEP97aWlpaivr0dPTw/TXU80SmVlZeGpp56C1WrFxo0b8cADD8Q9du3atbj33nvZJoXWjqFQCMFgcER5yXiYcPKMtFNRqVRIT09Hfn4+WzTLZDIUFBRALpcjLy8PeXl5KCkpgdFohNlsxuTJk0c8t0wmw7p163DllVfCZrPBbrezLblGo8GsWbPYTozkqGlzMFyUZCTwm+X4MBqNTMVFJpNBrVYn7AETi8Vwu904efIkPv/8c0bKiPdekydPFjQRkkCaTCZDRUXFBV83YUKNVDRa3HfffWwR+4tf/IJNaXq9HklJSYw6RTsd8hUeSZBCJBIhIyMDN910ExwOB86dO4cdO3bgiSeeAAAmPpaWlobm5mYMDg5CrVbD5/MxNsv5rt1oNLJA5TgOn332maCeR6NMKBSCSqUS7CiHIy8vD6tXr2adriqVCt/4xjfwP//zP4K1nVqtRlFREdvVUkWA0gn8LtYLxYQbqfLz85GdnY28vDykpKQIDLH1ej1sNhsOHTrEeoc4jkN7eztqampYqSQeOI6D0+lkOppmsxlpaWlsJxgOh9HQ0ICCggI2CqanpyM9PR0Wi4XJ+SQCSS36/X40NzdDpVLBZrMJjiF3hkAgwBK9iVBQUMCIGNQWnZ+fH6OppVAoYLfbBQJslFjlOA6ZmZkjKvXFw4QKqpSUFCxZsoSlGSKRCKZOncr+TgSDyspKtnagInFLSwvy8/MTMkhoKqCUAT+BCgxpNLz77rswmUzIyspCUVER8vPzUVhYiKqqKlRXVyfcBJDicHV1NRQKBZYtW4ZrrrkmxhybDJLIy28kpKWlARgKGr/fj0AgwOQc+aByESV36Yf699PS0ka9KZoQ059SqURBQQHKyspYeYFySm63GzqdDl6vl9mx2e12NgWIREPWZLW1tZg9ezbkcjnLyNPIolKpcN1112H27NlsoU4cPJPJxG4+xw0ZWhMzp7+/n02Dw6cPrVaL0tJStLa2suuTy+W49957IZPJmGs8H8FgkC32z549y/xy4sFut7N2aZVKBZlMxpjY/F2gzWZjnRS0HKARkMpFox2pxnVQUSGZpgm32w2n0ynor7ZarTCbzfB6vdBqtWhtbWVuVjT9qVQqJCcns9ELGCp/ZGZmIhwOIzc3FxUVFbDb7QKjJIPBIAgqSilotVomdBHPRHzy5MlwOp247rrr8PTTT2PatGmoq6uDy+WCwWDAb3/7Wxw7diwmr0VZ9GAwiLa2NqhUqrhrHaVSCbPZzJg2fLpYPKeMUCiE6upqZhBOgXXgwAEUFxePOG3Hw7gOqm9961vM6pW+MI7j2O6FGCVOpxP19fVQKpVoaWlhdrK0yOWbaBPD5vbbb0dmZiZ6e3uRkpKCvLw8Vg4hpsvg4GDMUyyTyaBUKqFWq9lUOjyvo9frmcTQvHnz2KgHAPv27cOUKVPQ0tISt9TCp1AlspNTqVRsSqNEKY0+w4XlSNS2vr4ehYWFrBxF3RJUXB4NxnVQvfzyy5g3bx6SkpJgMBgQiUTQ1dWF9vZ2uN1u9Pf3o6GhgTXSEa9OKpXC6/UiHA4jHA5DLBYz13WxeMg88dlnn8XChQvhdruh1Wrx6aef4vjx4zh48CBmz56NoqIitLe3C3ZnFNB8IX+lUikYTaLRKD799FPs2rVL8Fn4DXK0UI6HUCgEmUwGr9cLu90edxQhbSqahn0+HziOi7umot3e5s2bcezYMSgUCoTDYXi9Xpw+fRoVFRVIT09PKPIbD+M6qHbv3o3du3ezZB2/DkgiGfw1x8DAAJqamvDcc8+xKYGmBY1Gg2nTprGb1NDQgD/84Q+QSCSs5YQCaOfOnSwVMbxNpq6uDoODg0wPIRgMxs16J9qiny+jT7s/n88X47bFP3dHRwczRKLck1arjWk5lkqljNI1XEPLarWiqalp1G3KF1WZRqPRQC6Xx2Uck5BGd3f3l6qj6XQ65mLK1zQYSY6HdmA9PT3n7V4wm82sr0kqlSakodHaT6PRQKlUssAe3uqjVCpht9uZgD8fIpGISSzRe16UtT+DwYCsrCzGj6upqYmrYqfX62E2m5nqy/kgFothNpuRlZUFrVaL3t5e1NbWxgSo1WpFaWkprrzySpw4cQI1NTU4duxY3BKNw+HAkiVL8N3vfhd6vR4nTpzAK6+8gm3btn1pU0eVSoWMjAx85zvfQU5ODl599VWmbpzolstkMtxyyy0QiUR4/vnn4wb4RRlU2dnZSEpKYgtsq9WK1157jf1doVAgPz8f06dPh9lshlgsxt69e3H06NGEZt4ikQjLli3DY489BqvVytYr+/fvxw033MBGIUptRCIR1q9OU8epU6cEU6VIJMITTzyBm2++mTXO0a517dq12LZtW8LPz+8do9+HtzZff/31yM3NhdFohM/nQ0tLC7RaLTZt2hTjFSQSieB2u7Fq1SqsXLkS4XAYmzZtwqZNmxjrmnBRFpRrampYa25FRYVAtEskEmHBggWwWCzM26WzsxN5eXlwOp3405/+lHCBHAwGmewibe1pUUvQarVsM0AZ6UAgALPZDLlcLmgSFIvFKC0tFdjPEgFh8uTJMUGVlJSE7373u2xHRwo20WgUzz//vKAQrtfrUVRUBI/Hg+7ubthsNhbchYWFMUGVk5ODl156CVqtlm1cVq5ciWXLluHWW29NqGqcCBMuqIChJ3jmzJkoKirChg0bWMJSLpdjypQpGBwcZKL11M9eUFAAu90el5tHpRxAyDI5e/asYISgovKuXbvY1KFWq2EwGGK2/yLRkA1JY2MjY9pwHAev18uoYQSxWIzy8nLccMMNAjkjWrN5vV48+eSTLDh1Oh1cLhfL1YlEIvh8PuTn57MedP515+XlITk5mS386YcItKMNqglTpqGtOGHOnDmwWq34/ve/z3IzTqeTySfSU843rC4oKEh4fq/Xy6julKsavj6RSCRMhYXKHUTtGl7+oSx3Y2OjgMp18uRJZrxNSEpKws033xzTP0+5uFWrVjGJbQCM2EAG5N3d3UhKSmIMouHXUlBQwGxRiGFDa1K+PdyFYkIElUqlwpIlS7Bs2TLo9XoUFxcjIyMDe/fuxfHjx9laKScnhzX4E4ePpjFgiDyaKKFIx1EFPxqNwmKxCAJZJpMxAQ++C4VWq425kRTcR44cEbze2dkpcEsViURYuHAhsrKyoFKpoFAoIJfLIZPJGCNHp9NhzZo17NqJT0gFdL4IGpmR82E2m9HX18emdUqUNjU1JXThGAnjfvqzWCz4wQ9+gMWLF8Pn82HdunX48MMP8eqrr8Jut7MgkMlkyM7OZjeF7wVDN9Fut8NqtTLjJQoYh8OBlStXQqFQCEaLoqIiKBQKtlai7gUq49A6htZwfFA9MD09PSYwKQ0wMDAAi8WCadOmobGxkZls9/X1IRqNwmg0MqGRqVOnsmunqb2pqYlR70k7i0yT+JBKpaitrUV+fj574KRSKU6dOjUmza5xPVItXboU//Ef/4F//dd/xccff8y2+s8//zx6enoEQmKUQgCENCxKbnJ/E7DPyMgAMLSLvOGGG/DEE09g8+bNuO222+D1etHa2sqmtYKCAqSnp7ProTqbVCplJgBUWB4+UslkMrS1tTHDS0rg0r+h0VMmk6G0tBRdXV04cOAAIpEIDAYDLBYLq9HR9VAAaLVaBAIBdHd3IxgMQq/Xg+M4nDp1Cnq9PibAqaLAPxeJtanVagHh9EIwrkeqYDCIEydOsJv20UcfYcOGDfB6vQKjIz4rmKZCau+ghj3S28zMzMThw4dRWlqKOXPmMLvc1tZWliGn3iO/3y+oi5F7fEdHh6AvKSMjI+bGiMVidHR0sEI3Zeg/++wzTJ06lQVhb28v3njjDZSUlKCtrQ2tra2YPHmyYLQhvxxyC4tEIqivr2euq9QG5PP50N/fL9ixUhdCc3MzC2xgqD/s9OnTrFP2oqG9NzY2MnIBMZP5lhf05HEcB4PBgLa2NhYwFAyku37ixAmmRSUWi7Fnzx6cO3cORqMRarU6bouvRCKJ0dqsq6sTiHZQN+rwVAUF/X333cdSBNQx8corr7B1j9/vx69//WtB8ZsfUDTi8ktGXq8X9fX1rM1HLBajvr4eHo9H0PYDDI2uer0eR44cQV1dHWw2GyQSCbq7u1FTU8PU9UYFbhyit7eXA8AB4ORyOWexWLjk5GROrVaz1xUKBWexWDij0ciJRCJOLpdzSUlJ7P+Tk5O57Oxszul0ciqVihOLxZzJZOJMJhMnEonYecRiMSeVStmPRCLhJBIJ+51/bFJSEpeWlsZ+p59JkyZxer1e8JpWq+WKi4s5iUQieF0mk3FlZWWcQqGIOc+F/igUCs5sNnNyuZxLSUnhCgoKOJfLxSmVSs5mswmuWSqVcnfffTeXn5/P6XQ6Lj09ncvMzOTMZjNnNpu5H/7wh5xOp2PH9/b2nvf+TIiMulQqZVl0ms7UajWbsuJh+vTpKCwsREtLC/bt25cwm/73hFKpxJIlS3D27Fl0d3dDr9fDarXik08++Yfx+PiQy+VYu3YtqqqqsGPHjrjHXBQZdZVKhaVLlyI5ORmNjY2MbGk0GnHmzBl8/PHHMYHlcrnwzDPPwOFwQK1W45FHHsHvfve7UVPRvyz0ej1uvvlmtsGQyWSoq6vDnj17Ru0H82UglUoxY8YMLF26FGvXrkV7ezueffZZbN++Pa4T2XnP93e4xn8okpOTkZWVBQAoLi5GZ2cnBgYGYDabUVRUhIMHDwqYLEVFRfjd736HtLQ0JpR61113wWg04pe//OWII8Ts2bNRVlYGkWjIWPvIkSM4cODAmK+dJolQKMSsUL4KBcCRQLvWuro6th6cOXMmXnrpJSiVSkQiEVgsFvz0pz/FDTfcgBUrVsS4oZ4P4z6o0tPTYTAY0NHRwTSY6KYoFAoBPUokEuHGG29Ef38/3nrrLbbjEolEWLx4MZ5//vmEEooajQaLFy/G9OnTYTAYEAqFkJaWhsOHDyecYlUqFdsxxgMptfATqqRjNRJ0Oh1yc3NZy7FOp0N/f3/CB4I6YB0OB8rLy6FUKpnDFgDBZ+K/d35+PtLT0wUOXReCcR9UWq0WbW1tCIfDMJlMEIlEsNlsaGhoYJ2OVGyVSqWYOXMmHA4H3n//fbb2qqysxBVXXIHc3NyEQZWamoqMjAym6eTxeJgKHt8/TyQSISkpCQsWLMDUqVPR0dGBjRs3svZe/g2irT7V8UjOcaSgkkgkWLx4MebOnYt9+/ZBLpdDrVbjzTffjLl2arEuKytDZmYmVCoVenp6sHnzZkGrclpamsCjmq5HrVbD4XBcfEGlVquZxI5MJkNfXx88Hg9ycnLQ09MjsAax2WxITU2FWq3GmjVrEIlE0NbWhoqKCqSmpqKkpASffPJJzHtoNBrMmTOHZehra2sRCASg1WphMBhw7tw5iMViqFQqlJeXY+7cubDb7XA4HKioqMCqVasQjUaxb98+3H333YIRhRKYUqmUeQ/Gy2I7HA7k5eWhq6sLZWVlrJd+ypQp8Pl8MQ4XIpEIkydPRnl5OVpbW+HxeNDQ0IDq6mqkpKRAJPrCV0er1aKvr49NwdzfcnBUDxwtxn1QUSckTQGUa+ro6EBKSoqAPGkymWA0GuH3++FyuTA4OAiHw8Gq+cO5dNSYN3fuXLZuo1GJbNr8fj/sdjtmzpwJp9MJl8vFgo3jOLS0tLC25t7eXgEFTKFQQK1WQ6VSscw7iaHxVeuSk5Px0EMPMe0Hn8+Hnp4e9Pb2Qq1W47//+78Fo6VarYbFYsG1116LmpoavPvuu5g+fTpmz54Nt9vNfKYbGxshl8uZUVNGRgbL7vt8Pra+Gi3GdVAREZTU5KjU0NfXh6KiIrS2tsY4QwwODqKurg7FxcUsYRoKhdDT0wOdTse+1MmTJyMvLw8ZGRmw2+2MZWy32xmbxuPxwOv1wufzsb51ImGSTiat74iJw9/V5efnw2AwsPekJGtGRgbrKNXpdMjJyWFGRX6/H3/6058YDf+tt96CTCZDamoqmpubYTabkZ+fjylTprD15OrVq1mBOSsrC83NzTh8+DAAMF3U5uZmVqIChqbmpqamUXP+gHEeVMDQiEEFW51Oxxyl5HI5cnJyBA1pqampaG1tZaMAlWmi0SgqKyuh1+tZG4rb7Ybb7YZer2cE1c7OTrS1tWFgYADhcBgtLS3weDwIh8PMZAiIJS8MV/kjXHbZZdDpdBgcHGTMZ2CIv0jdCz6fD/v27cO+ffvY9EivnzlzBhaLhRk9RaNRLF++nI04wWAQs2fPhtfrhcvlgtvtRmdnJ6LRqIBhJBaLWQsOTX8ajQZnz54VrLUuFOM6qPg3iZ44QGhzxi/kulwu1NTUwOVysbILFYB3796NOXPmMHLlm2++yUQ3uL9Rr2hRTf92YGCAdSKMJQ1w4sQJbNmyBU6nk3WhdnR0CHrqOU5ojMRP0obD4Zje99deew35+flITk5mnanBYBCHDx/Gn/70J4TDYfT19bGdH31+v98Pr9fLNgxk6TbqEg3GeVABQ6aMW7duRX9/P0wmE2QyGfr7+9HZ2YkrrrhCcBNqa2ths9lYawrRqOrr61FVVSXwZwmHw+js7GRF2r8Htm7diq1bt8JoNMJqteLs2bNfWmrS4/GMiqNHBt4bNmzAU089xYIqHA5j3rx5yMnJGfU1jMsyTW9vLyNFWiwWxoMbDpvNhmAwyPJUNDJRCzGB4/V8f1kWy1hAPV3/6Iw+weFwoKOjIyagNRoNFAqFgLfo8XjOS48bl0FVW1uL7Ozsr/syLko0NjYiNTV1xGPG5fRHzXYNDQ3/UEHZ8Yy+vj6kpaWhsbHxvAXheOD+RspwOp3nPXZcBhVNXQaDYUxf0MUMvV4/5u/sQh/gcd1OfAn/nLgUVJfwlWNcBpVCocBPf/rTMSXmLlb8I7+zcbn7u4R/bozLkeoS/rlxKagu4SvHpaC6hK8cl4LqEr5yXAqqS/jKMS6D6te//jUyMjKgVCoxY8YM7N+//+u+pK8Fjz32GOvJstvtuPbaa3Hy5EnBMX6/H3fccQcsFgu0Wi1WrlwZY1vb0NCApUuXQq1Ww26347777vtyVnJfDWf4H4fXX3+dk8vl3AsvvMAdP36cW7duHWc0Grm2trav+9L+4Vi0aBG3YcMG7vPPP+eOHj3KXX311ZzL5eJ8Ph875rbbbuPS0tK47du3cwcPHuRmzpzJzZ49m/09HA5zkyZN4hYsWMAdOXKE27JlC2e1WrkHHnhgzNc17oJq+vTp3B133MF+j0QinNPp5B577LGv8ar+OdDe3s4B4Hbs2MFxHMd5PB5OJpNxb7zxBjumqqqKA8Dt2bOH4ziO27JlCycWi7nW1lZ2zHPPPcfp9XouEAiM6TrG1fQXDAZx6NAhLFiwgL0mFouxYMEC7Nmz52u8sn8OUN8YdXEcOnQIoVBI8H0VFBTA5XKx72vPnj2YPHkykpKS2DGLFi1CX18fjh8/PqbrGFdB1dnZiUgkIvgCgCH5QhIqu1gRjUZxzz334PLLL8ekSZMADDl7yeXyGJcH/vfV2toa9/ukv40F47L15RJicccdd+Dzzz/Hzp07v+5LGV8jldVqhUQiidm9tLW1jUmbcqLgzjvvxHvvvYePPvpI0JXpcDgYlYwP/vflcDjifp/0t7FgXAWVXC7HtGnTsH37dvZaNBrF9u3bMWvWrK/xyr4ecByHO++8E2+//TY+/PBD5sRFmDZtGmQymeD7OnnyJBoaGtj3NWvWLFRWVgpo8Nu2bYNer2cOZWO5sHGF119/nVMoFNyLL77InThxglu/fj1nNBoFu5eLBd/73vc4g8HAffzxx1xLSwv7GRgYYMfcdtttnMvl4j788EPu4MGD3KxZs7hZs2axv1NKYeHChdzRo0e5rVu3cjab7eJKKXAcxz3zzDOcy+Xi5HI5N336dG7v3r1f9yV9LUACJb0NGzawYwYHB7nbb7+dM5lMnFqt5q677jqupaVFcJ66ujpuyZIlnEql4qxWK3fvvfdyoVBozNd1qZ/qEr5yjKs11SWMD1wKqkv4ynEpqC7hK8eloLqErxyXguoSvnJcCqpL+MpxKagu4SvHpaC6hK8cl4LqEr5yXAqqS/jKcSmoLuErx/8Hb2X9Zcg4hQQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do not change this cell\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "# Show images\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "a1b02119-0bc1-44e9-b7e7-b71a8cd3b4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: you will design your model here\n",
    "class ConvModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # TODO: complete this method\n",
    "        super(ConvModel, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_size, out_channels=6, kernel_size=(5, 5))\n",
    "        self.batch_norm = nn.BatchNorm2d(6)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        \n",
    "        self.lin1 = nn.Linear(in_features=6*12*12, out_features=84)\n",
    "        self.lin2 = nn.Linear(in_features=84, out_features=output_size)\n",
    "        self.logSoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: complete this method\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.lin2(x)\n",
    "        x = self.logSoftmax(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "bb17772d-49e7-4d06-a14d-aff4bc9715a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, train_loader, loss_fn, optimizer, reg_param, device):\n",
    "    # TODO: implement one epoch of training\n",
    "    model.train()\n",
    "    \n",
    "    loss = np.zeros(len(train_loader))\n",
    "    correct = np.zeros(len(train_loader))\n",
    "\n",
    "    i = 0\n",
    "    for (X, y) in train_loader:\n",
    "        (X, y) = (X.to(device), y.to(device))\n",
    "        scores = model(X)\n",
    "        prediction = torch.argmax(scores, dim=1)\n",
    "\n",
    "        loss_step = loss_fn(scores, y)\n",
    "        correct[i] += torch.eq(prediction, y).sum().item()\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss_step.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        norms = np.array([np.linalg.norm(param.detach().numpy()) for param in model.parameters()])\n",
    "        loss[i] = loss_step.detach().numpy() + reg_param * sum(norms)\n",
    "\n",
    "        if i % 90 == 0:\n",
    "            print(f\"Batch: {i}\\tAccuracy: {((correct[i] / 256) * 100):.2f}\\tLoss: {loss[i]:.2f}\")\n",
    "        i += 1\n",
    "    accuracy = sum(correct) / (len(train_loader) * 256) * 100\n",
    "    return (model, sum(loss), accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "66b9baf8-6989-4789-9fb4-ecccaf1f2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_step(model, data_loader, loss_fn, reg_param, device):\n",
    "    # TODO: implement evaluation on a data set, data_loader\n",
    "    model.eval()\n",
    "    loss = np.zeros(len(data_loader))\n",
    "    correct = np.zeros(len(data_loader))\n",
    "    i = 0\n",
    "    with torch.inference_mode():\n",
    "        for (X, y) in data_loader:\n",
    "            (X, y) = (X.to(device), y.to(device))\n",
    "            scores = model(X)\n",
    "            prediction = torch.argmax(scores, dim=1)\n",
    "    \n",
    "            loss_step = loss_fn(scores, y)\n",
    "            correct[i] += torch.eq(prediction, y).sum().item()\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            norms = np.array([np.linalg.norm(param.detach().numpy()) for param in model.parameters()])\n",
    "            loss[i] = loss_step.detach().numpy() + reg_param * sum(norms)\n",
    "\n",
    "            i += 1        \n",
    "    accuracy = (sum(correct) / (len(data_loader) * 256)) * 100\n",
    "    print(f\"Accuracy: {accuracy}\\tLoss: {sum(loss)}\")\n",
    "    return (model, sum(loss), accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "0ecae0d8-d040-4e9a-8b3f-f64f97e988a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_conv_model(train_loader, valid_loader, test_loader, random_seed):\n",
    "    # Make device\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        if torch.backends.mps.is_available():\n",
    "            device = \"mps\"\n",
    "\n",
    "    torch.manual_seed(random_seed)  # do not change this\n",
    "\n",
    "    # TODO: write codes to train your model here\n",
    "    input_size = 1\n",
    "    output_size = 10\n",
    "    \n",
    "    n_epochs = 1000\n",
    "\n",
    "    model = ConvModel(input_size, output_size)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    lr = 0.05\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    reg_param = 0.4\n",
    "\n",
    "    train_loss = np.zeros(n_epochs)\n",
    "    test_loss = np.zeros(n_epochs)\n",
    "    validation_loss = np.zeros(n_epochs)\n",
    "    train_accuracy = np.zeros(n_epochs)\n",
    "    test_accuracy = np.zeros(n_epochs)\n",
    "    validation_accuracy = np.zeros(n_epochs)\n",
    "\n",
    "    for i in np.arange(n_epochs):\n",
    "        print(f\"Epoch {i}\")\n",
    "        print(\"Train: \")\n",
    "        model, train_accuracy[i], train_loss[i] = train_step(model, train_loader, loss_fn, optimizer, reg_param, device)\n",
    "        print(\"Validation: \")\n",
    "        model, validation_accuracy[i], validation_loss[i] = evaluation_step(model, valid_loader, loss_fn, reg_param, device)\n",
    "        print(\"Test: \")\n",
    "        model, test_accuracy[i], test_loss[i] = evaluation_step(model, test_loader, loss_fn, reg_param, device)\n",
    "\n",
    "    return model, train_loss, train_accuracy, validation_loss, validation_accuracy, test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "f2d29a37-67eb-44e2-971c-66b7817923ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (254378377.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[299], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    # TODO: plot accuracy curves, you can reuse your codes from the simple classification task\u001b[0m\n\u001b[0m                                                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def plot_accuracy_performance(train_accuracies, valid_accuracies, test_accuracies):\n",
    "   # TODO: plot accuracy curves, you can reuse your codes from the simple classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "ce449fa4-1519-40cc-a7d2-c36b3cffa068",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (771524813.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[282], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    # TODO: plot loss curves, you can reuse your codes from the simple classification task\u001b[0m\n\u001b[0m                                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def plot_loss_performance(train_losses, valid_accuracies, test_losses):\n",
    "# TODO: plot loss curves, you can reuse your codes from the simple classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "4fd20d6d-0bb1-48fc-8e9c-809f8a7b6c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 14.84\tLoss: 6.97\n",
      "Batch: 90\tAccuracy: 79.30\tLoss: 5.83\n",
      "Batch: 180\tAccuracy: 84.77\tLoss: 5.93\n",
      "Validation: \n",
      "Accuracy: 82.64627659574468\tLoss: 280.4419246733188\n",
      "Test: \n",
      "Accuracy: 80.1171875\tLoss: 239.08494669198976\n",
      "Epoch 1\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 83.98\tLoss: 5.91\n",
      "Batch: 90\tAccuracy: 82.81\tLoss: 6.12\n",
      "Batch: 180\tAccuracy: 84.38\tLoss: 6.14\n",
      "Validation: \n",
      "Accuracy: 85.33909574468085\tLoss: 289.01442603468894\n",
      "Test: \n",
      "Accuracy: 82.75390625\tLoss: 246.69606472551808\n",
      "Epoch 2\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 85.16\tLoss: 6.16\n",
      "Batch: 90\tAccuracy: 80.47\tLoss: 6.32\n",
      "Batch: 180\tAccuracy: 84.38\tLoss: 6.31\n",
      "Validation: \n",
      "Accuracy: 87.00132978723404\tLoss: 295.5604994088409\n",
      "Test: \n",
      "Accuracy: 84.501953125\tLoss: 252.38126233220083\n",
      "Epoch 3\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 85.94\tLoss: 6.32\n",
      "Batch: 90\tAccuracy: 87.89\tLoss: 6.31\n",
      "Batch: 180\tAccuracy: 87.11\tLoss: 6.47\n",
      "Validation: \n",
      "Accuracy: 86.08710106382979\tLoss: 303.073498415947\n",
      "Test: \n",
      "Accuracy: 83.623046875\tLoss: 258.87196364998834\n",
      "Epoch 4\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 85.94\tLoss: 6.50\n",
      "Batch: 90\tAccuracy: 89.45\tLoss: 6.51\n",
      "Batch: 180\tAccuracy: 89.84\tLoss: 6.51\n",
      "Validation: \n",
      "Accuracy: 87.69946808510637\tLoss: 307.6966882675886\n",
      "Test: \n",
      "Accuracy: 85.224609375\tLoss: 262.7150493115188\n",
      "Epoch 5\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 88.28\tLoss: 6.53\n",
      "Batch: 90\tAccuracy: 87.89\tLoss: 6.63\n",
      "Batch: 180\tAccuracy: 90.23\tLoss: 6.61\n",
      "Validation: \n",
      "Accuracy: 88.2313829787234\tLoss: 313.4418861329558\n",
      "Test: \n",
      "Accuracy: 85.947265625\tLoss: 267.56202574074285\n",
      "Epoch 6\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 91.02\tLoss: 6.63\n",
      "Batch: 90\tAccuracy: 89.45\tLoss: 6.73\n",
      "Batch: 180\tAccuracy: 88.28\tLoss: 6.77\n",
      "Validation: \n",
      "Accuracy: 87.4251994680851\tLoss: 319.7431895524261\n",
      "Test: \n",
      "Accuracy: 85.087890625\tLoss: 273.01351976394636\n",
      "Epoch 7\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 87.50\tLoss: 6.81\n",
      "Batch: 90\tAccuracy: 85.94\tLoss: 6.87\n",
      "Batch: 180\tAccuracy: 91.41\tLoss: 6.82\n",
      "Validation: \n",
      "Accuracy: 87.49168882978722\tLoss: 325.68641446232806\n",
      "Test: \n",
      "Accuracy: 85.078125\tLoss: 278.06456102430815\n",
      "Epoch 8\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 90.23\tLoss: 6.88\n",
      "Batch: 90\tAccuracy: 92.58\tLoss: 6.85\n",
      "Batch: 180\tAccuracy: 90.23\tLoss: 6.97\n",
      "Validation: \n",
      "Accuracy: 87.84906914893617\tLoss: 330.2563445657489\n",
      "Test: \n",
      "Accuracy: 85.400390625\tLoss: 281.754765242338\n",
      "Epoch 9\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 90.23\tLoss: 6.95\n",
      "Batch: 90\tAccuracy: 90.23\tLoss: 7.02\n",
      "Batch: 180\tAccuracy: 88.67\tLoss: 7.09\n",
      "Validation: \n",
      "Accuracy: 87.66622340425532\tLoss: 335.54994817376127\n",
      "Test: \n",
      "Accuracy: 85.15625\tLoss: 286.2318640649319\n",
      "Epoch 10\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 91.80\tLoss: 7.07\n",
      "Batch: 90\tAccuracy: 91.80\tLoss: 7.15\n",
      "Batch: 180\tAccuracy: 89.45\tLoss: 7.22\n",
      "Validation: \n",
      "Accuracy: 87.9654255319149\tLoss: 340.5361876100305\n",
      "Test: \n",
      "Accuracy: 85.302734375\tLoss: 290.5003164857628\n",
      "Epoch 11\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 91.80\tLoss: 7.21\n",
      "Batch: 90\tAccuracy: 90.62\tLoss: 7.23\n",
      "Batch: 180\tAccuracy: 91.02\tLoss: 7.30\n",
      "Validation: \n",
      "Accuracy: 87.9155585106383\tLoss: 345.49508022069915\n",
      "Test: \n",
      "Accuracy: 85.634765625\tLoss: 294.7742606550455\n",
      "Epoch 12\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 91.80\tLoss: 7.24\n",
      "Batch: 90\tAccuracy: 91.41\tLoss: 7.31\n",
      "Batch: 180\tAccuracy: 92.97\tLoss: 7.39\n",
      "Validation: \n",
      "Accuracy: 89.1373005319149\tLoss: 349.2595320433375\n",
      "Test: \n",
      "Accuracy: 87.060546875\tLoss: 297.88463020324684\n",
      "Epoch 13\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 94.14\tLoss: 7.33\n",
      "Batch: 90\tAccuracy: 88.67\tLoss: 7.47\n",
      "Batch: 180\tAccuracy: 89.06\tLoss: 7.48\n",
      "Validation: \n",
      "Accuracy: 88.2064494680851\tLoss: 355.0735132187602\n",
      "Test: \n",
      "Accuracy: 85.712890625\tLoss: 303.1879192292688\n",
      "Epoch 14\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 92.19\tLoss: 7.42\n",
      "Batch: 90\tAccuracy: 92.97\tLoss: 7.47\n",
      "Batch: 180\tAccuracy: 92.19\tLoss: 7.60\n",
      "Validation: \n",
      "Accuracy: 87.35871010638297\tLoss: 361.34764947295173\n",
      "Test: \n",
      "Accuracy: 85.46875\tLoss: 308.2072090655565\n",
      "Epoch 15\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 92.19\tLoss: 7.55\n",
      "Batch: 90\tAccuracy: 89.06\tLoss: 7.65\n",
      "Batch: 180\tAccuracy: 91.80\tLoss: 7.71\n",
      "Validation: \n",
      "Accuracy: 89.38663563829788\tLoss: 363.0448124825956\n",
      "Test: \n",
      "Accuracy: 87.36328125\tLoss: 309.66520451009274\n",
      "Epoch 16\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 90.62\tLoss: 7.70\n",
      "Batch: 90\tAccuracy: 90.62\tLoss: 7.73\n",
      "Batch: 180\tAccuracy: 92.97\tLoss: 7.71\n",
      "Validation: \n",
      "Accuracy: 89.5279255319149\tLoss: 367.9016957014797\n",
      "Test: \n",
      "Accuracy: 87.36328125\tLoss: 313.7705193459987\n",
      "Epoch 17\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 92.58\tLoss: 7.73\n",
      "Batch: 90\tAccuracy: 91.80\tLoss: 7.78\n",
      "Batch: 180\tAccuracy: 94.53\tLoss: 7.80\n",
      "Validation: \n",
      "Accuracy: 87.99867021276596\tLoss: 374.3771997690201\n",
      "Test: \n",
      "Accuracy: 85.3515625\tLoss: 319.48134146630764\n",
      "Epoch 18\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 88.28\tLoss: 7.89\n",
      "Batch: 90\tAccuracy: 88.67\tLoss: 7.90\n",
      "Batch: 180\tAccuracy: 92.97\tLoss: 7.94\n",
      "Validation: \n",
      "Accuracy: 89.54454787234043\tLoss: 377.05431267917135\n",
      "Test: \n",
      "Accuracy: 87.3828125\tLoss: 321.69072009623045\n",
      "Epoch 19\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 94.53\tLoss: 7.89\n",
      "Batch: 90\tAccuracy: 94.14\tLoss: 7.94\n",
      "Batch: 180\tAccuracy: 92.19\tLoss: 8.05\n",
      "Validation: \n",
      "Accuracy: 87.93218085106383\tLoss: 384.9192091912028\n",
      "Test: \n",
      "Accuracy: 85.72265625\tLoss: 328.59100198745705\n",
      "Epoch 20\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 92.97\tLoss: 8.05\n",
      "Batch: 90\tAccuracy: 94.53\tLoss: 8.07\n",
      "Batch: 180\tAccuracy: 94.92\tLoss: 8.08\n",
      "Validation: \n",
      "Accuracy: 85.80452127659575\tLoss: 393.4611555218694\n",
      "Test: \n",
      "Accuracy: 83.896484375\tLoss: 335.97604620456684\n",
      "Epoch 21\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 90.23\tLoss: 8.22\n",
      "Batch: 90\tAccuracy: 94.92\tLoss: 8.18\n",
      "Batch: 180\tAccuracy: 92.58\tLoss: 8.24\n",
      "Validation: \n",
      "Accuracy: 88.77992021276596\tLoss: 392.2737512499097\n",
      "Test: \n",
      "Accuracy: 86.6015625\tLoss: 334.57356803119205\n",
      "Epoch 22\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 91.41\tLoss: 8.22\n",
      "Batch: 90\tAccuracy: 91.02\tLoss: 8.29\n",
      "Batch: 180\tAccuracy: 95.70\tLoss: 8.27\n",
      "Validation: \n",
      "Accuracy: 89.43650265957447\tLoss: 396.0667423695323\n",
      "Test: \n",
      "Accuracy: 87.1875\tLoss: 337.92612113058544\n",
      "Epoch 23\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 92.58\tLoss: 8.34\n",
      "Batch: 90\tAccuracy: 93.75\tLoss: 8.37\n",
      "Batch: 180\tAccuracy: 91.80\tLoss: 8.44\n",
      "Validation: \n",
      "Accuracy: 89.3533909574468\tLoss: 400.8870466798544\n",
      "Test: \n",
      "Accuracy: 86.97265625\tLoss: 341.9984635710716\n",
      "Epoch 24\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 95.31\tLoss: 8.36\n",
      "Batch: 90\tAccuracy: 94.53\tLoss: 8.43\n",
      "Batch: 180\tAccuracy: 92.58\tLoss: 8.57\n",
      "Validation: \n",
      "Accuracy: 89.90192819148936\tLoss: 404.5729140728709\n",
      "Test: \n",
      "Accuracy: 87.607421875\tLoss: 345.26667852699734\n",
      "Epoch 25\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 92.97\tLoss: 8.51\n",
      "Batch: 90\tAccuracy: 96.09\tLoss: 8.50\n",
      "Batch: 180\tAccuracy: 94.53\tLoss: 8.56\n",
      "Validation: \n",
      "Accuracy: 90.10139627659575\tLoss: 408.4207363665101\n",
      "Test: \n",
      "Accuracy: 87.958984375\tLoss: 348.3764702826736\n",
      "Epoch 26\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 93.36\tLoss: 8.63\n",
      "Batch: 90\tAccuracy: 92.97\tLoss: 8.65\n",
      "Batch: 180\tAccuracy: 94.14\tLoss: 8.66\n",
      "Validation: \n",
      "Accuracy: 89.17054521276596\tLoss: 414.99232162237195\n",
      "Test: \n",
      "Accuracy: 87.03125\tLoss: 354.1939957141878\n",
      "Epoch 27\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 93.36\tLoss: 8.69\n",
      "Batch: 90\tAccuracy: 95.31\tLoss: 8.70\n",
      "Batch: 180\tAccuracy: 96.48\tLoss: 8.75\n",
      "Validation: \n",
      "Accuracy: 89.38663563829788\tLoss: 419.30972138643295\n",
      "Test: \n",
      "Accuracy: 86.796875\tLoss: 357.9506554901602\n",
      "Epoch 28\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 95.31\tLoss: 8.74\n",
      "Batch: 90\tAccuracy: 95.31\tLoss: 8.79\n",
      "Batch: 180\tAccuracy: 94.14\tLoss: 8.83\n",
      "Validation: \n",
      "Accuracy: 88.90458776595744\tLoss: 424.55385610461235\n",
      "Test: \n",
      "Accuracy: 86.357421875\tLoss: 362.6664940714836\n",
      "Epoch 29\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 93.75\tLoss: 8.88\n",
      "Batch: 90\tAccuracy: 96.09\tLoss: 8.86\n",
      "Batch: 180\tAccuracy: 90.23\tLoss: 8.99\n",
      "Validation: \n",
      "Accuracy: 89.4032579787234\tLoss: 427.1305902719501\n",
      "Test: \n",
      "Accuracy: 87.1875\tLoss: 364.3782303482296\n",
      "Epoch 30\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 96.88\tLoss: 8.90\n",
      "Batch: 90\tAccuracy: 94.53\tLoss: 9.03\n",
      "Batch: 180\tAccuracy: 97.66\tLoss: 8.99\n",
      "Validation: \n",
      "Accuracy: 88.11502659574468\tLoss: 433.8266701728102\n",
      "Test: \n",
      "Accuracy: 86.005859375\tLoss: 370.4330303668974\n",
      "Epoch 31\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 94.53\tLoss: 9.02\n",
      "Batch: 90\tAccuracy: 93.36\tLoss: 9.14\n",
      "Batch: 180\tAccuracy: 94.14\tLoss: 9.15\n",
      "Validation: \n",
      "Accuracy: 90.06815159574468\tLoss: 435.1846400827166\n",
      "Test: \n",
      "Accuracy: 88.10546875\tLoss: 371.0520178526638\n",
      "Epoch 32\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 95.70\tLoss: 9.10\n",
      "Batch: 90\tAccuracy: 94.53\tLoss: 9.15\n",
      "Batch: 180\tAccuracy: 94.14\tLoss: 9.21\n",
      "Validation: \n",
      "Accuracy: 88.97938829787235\tLoss: 441.76365735232866\n",
      "Test: \n",
      "Accuracy: 86.796875\tLoss: 377.32404685020475\n",
      "Epoch 33\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 94.53\tLoss: 9.22\n",
      "Batch: 90\tAccuracy: 92.19\tLoss: 9.31\n",
      "Batch: 180\tAccuracy: 96.88\tLoss: 9.27\n",
      "Validation: \n",
      "Accuracy: 88.12333776595744\tLoss: 447.9142823249098\n",
      "Test: \n",
      "Accuracy: 85.68359375\tLoss: 382.4980839490888\n",
      "Epoch 34\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 94.92\tLoss: 9.29\n",
      "Batch: 90\tAccuracy: 97.66\tLoss: 9.29\n",
      "Batch: 180\tAccuracy: 94.92\tLoss: 9.40\n",
      "Validation: \n",
      "Accuracy: 89.07081117021278\tLoss: 449.85355229079687\n",
      "Test: \n",
      "Accuracy: 86.89453125\tLoss: 383.51623664796324\n",
      "Epoch 35\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 92.97\tLoss: 9.42\n",
      "Batch: 90\tAccuracy: 94.53\tLoss: 9.46\n",
      "Batch: 180\tAccuracy: 95.31\tLoss: 9.44\n",
      "Validation: \n",
      "Accuracy: 89.9684175531915\tLoss: 452.47425130605734\n",
      "Test: \n",
      "Accuracy: 87.998046875\tLoss: 385.8973457813266\n",
      "Epoch 36\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 96.09\tLoss: 9.43\n",
      "Batch: 90\tAccuracy: 96.48\tLoss: 9.47\n",
      "Batch: 180\tAccuracy: 96.48\tLoss: 9.51\n",
      "Validation: \n",
      "Accuracy: 87.51662234042553\tLoss: 460.99818548858127\n",
      "Test: \n",
      "Accuracy: 85.4296875\tLoss: 393.04783570766426\n",
      "Epoch 37\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 93.75\tLoss: 9.57\n",
      "Batch: 90\tAccuracy: 96.48\tLoss: 9.57\n",
      "Batch: 180\tAccuracy: 95.31\tLoss: 9.63\n",
      "Validation: \n",
      "Accuracy: 88.73836436170212\tLoss: 464.71366355121097\n",
      "Test: \n",
      "Accuracy: 86.748046875\tLoss: 396.2163378745315\n",
      "Epoch 38\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 91.80\tLoss: 9.69\n",
      "Batch: 90\tAccuracy: 95.70\tLoss: 9.65\n",
      "Batch: 180\tAccuracy: 99.22\tLoss: 9.67\n",
      "Validation: \n",
      "Accuracy: 89.80219414893617\tLoss: 465.57527073323763\n",
      "Test: \n",
      "Accuracy: 87.470703125\tLoss: 397.79831916093855\n",
      "Epoch 39\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 94.92\tLoss: 9.71\n",
      "Batch: 90\tAccuracy: 94.92\tLoss: 9.76\n",
      "Batch: 180\tAccuracy: 93.75\tLoss: 9.82\n",
      "Validation: \n",
      "Accuracy: 89.33676861702128\tLoss: 470.25645570158997\n",
      "Test: \n",
      "Accuracy: 87.03125\tLoss: 401.44114241004013\n",
      "Epoch 40\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 96.88\tLoss: 9.77\n",
      "Batch: 90\tAccuracy: 95.31\tLoss: 9.83\n",
      "Batch: 180\tAccuracy: 96.48\tLoss: 9.87\n",
      "Validation: \n",
      "Accuracy: 89.97672872340425\tLoss: 474.1745077311989\n",
      "Test: \n",
      "Accuracy: 87.509765625\tLoss: 404.6358274817464\n",
      "Epoch 41\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 94.53\tLoss: 9.90\n",
      "Batch: 90\tAccuracy: 94.53\tLoss: 9.94\n",
      "Batch: 180\tAccuracy: 95.70\tLoss: 9.94\n",
      "Validation: \n",
      "Accuracy: 90.2094414893617\tLoss: 478.1098853379484\n",
      "Test: \n",
      "Accuracy: 87.705078125\tLoss: 408.1535467356441\n",
      "Epoch 42\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 97.27\tLoss: 9.91\n",
      "Batch: 90\tAccuracy: 97.27\tLoss: 9.97\n",
      "Batch: 180\tAccuracy: 96.48\tLoss: 10.03\n",
      "Validation: \n",
      "Accuracy: 90.05984042553192\tLoss: 482.34468378722704\n",
      "Test: \n",
      "Accuracy: 87.4609375\tLoss: 411.8637573570016\n",
      "Epoch 43\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 95.70\tLoss: 10.06\n",
      "Batch: 90\tAccuracy: 95.31\tLoss: 10.10\n",
      "Batch: 180\tAccuracy: 98.05\tLoss: 10.08\n",
      "Validation: \n",
      "Accuracy: 90.14295212765957\tLoss: 485.50928426981017\n",
      "Test: \n",
      "Accuracy: 88.076171875\tLoss: 414.2893138080838\n",
      "Epoch 44\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 97.66\tLoss: 10.09\n",
      "Batch: 90\tAccuracy: 96.88\tLoss: 10.14\n",
      "Batch: 180\tAccuracy: 98.05\tLoss: 10.16\n",
      "Validation: \n",
      "Accuracy: 90.02659574468085\tLoss: 490.3114721029997\n",
      "Test: \n",
      "Accuracy: 87.71484375\tLoss: 418.3233166337013\n",
      "Epoch 45\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 96.88\tLoss: 10.16\n",
      "Batch: 90\tAccuracy: 98.44\tLoss: 10.21\n",
      "Batch: 180\tAccuracy: 97.66\tLoss: 10.24\n",
      "Validation: \n",
      "Accuracy: 89.9185505319149\tLoss: 494.0318773776289\n",
      "Test: \n",
      "Accuracy: 87.685546875\tLoss: 421.8722094297406\n",
      "Epoch 46\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 98.83\tLoss: 10.22\n",
      "Batch: 90\tAccuracy: 96.09\tLoss: 10.31\n",
      "Batch: 180\tAccuracy: 97.66\tLoss: 10.35\n",
      "Validation: \n",
      "Accuracy: 89.17885638297872\tLoss: 499.21432407200336\n",
      "Test: \n",
      "Accuracy: 86.572265625\tLoss: 426.5122347921133\n",
      "Epoch 47\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 94.92\tLoss: 10.36\n",
      "Batch: 90\tAccuracy: 97.66\tLoss: 10.37\n",
      "Batch: 180\tAccuracy: 98.44\tLoss: 10.40\n",
      "Validation: \n",
      "Accuracy: 89.92686170212765\tLoss: 501.37894819676876\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 428.3085088431835\n",
      "Epoch 48\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 96.48\tLoss: 10.43\n",
      "Batch: 90\tAccuracy: 96.09\tLoss: 10.46\n",
      "Batch: 180\tAccuracy: 97.27\tLoss: 10.50\n",
      "Validation: \n",
      "Accuracy: 89.77726063829788\tLoss: 505.9019091293212\n",
      "Test: \n",
      "Accuracy: 87.763671875\tLoss: 431.51411767303915\n",
      "Epoch 49\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 97.66\tLoss: 10.47\n",
      "Batch: 90\tAccuracy: 96.88\tLoss: 10.52\n",
      "Batch: 180\tAccuracy: 96.88\tLoss: 10.58\n",
      "Validation: \n",
      "Accuracy: 89.22872340425532\tLoss: 511.5663175165649\n",
      "Test: \n",
      "Accuracy: 86.904296875\tLoss: 436.45698589086504\n",
      "Epoch 50\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 97.66\tLoss: 10.58\n",
      "Batch: 90\tAccuracy: 97.66\tLoss: 10.60\n",
      "Batch: 180\tAccuracy: 96.09\tLoss: 10.65\n",
      "Validation: \n",
      "Accuracy: 88.63863031914893\tLoss: 515.6400403618809\n",
      "Test: \n",
      "Accuracy: 86.328125\tLoss: 440.42936028540106\n",
      "Epoch 51\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 97.27\tLoss: 10.66\n",
      "Batch: 90\tAccuracy: 96.88\tLoss: 10.70\n",
      "Batch: 180\tAccuracy: 97.27\tLoss: 10.71\n",
      "Validation: \n",
      "Accuracy: 89.8686835106383\tLoss: 517.576954126358\n",
      "Test: \n",
      "Accuracy: 87.578125\tLoss: 441.47615149617195\n",
      "Epoch 52\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 98.83\tLoss: 10.69\n",
      "Batch: 90\tAccuracy: 98.05\tLoss: 10.74\n",
      "Batch: 180\tAccuracy: 97.66\tLoss: 10.79\n",
      "Validation: \n",
      "Accuracy: 88.58876329787235\tLoss: 524.3460790574553\n",
      "Test: \n",
      "Accuracy: 86.34765625\tLoss: 447.32226698100595\n",
      "Epoch 53\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 96.88\tLoss: 10.81\n",
      "Batch: 90\tAccuracy: 98.05\tLoss: 10.82\n",
      "Batch: 180\tAccuracy: 98.05\tLoss: 10.84\n",
      "Validation: \n",
      "Accuracy: 88.92952127659575\tLoss: 526.5894943088296\n",
      "Test: \n",
      "Accuracy: 86.572265625\tLoss: 449.9405232518914\n",
      "Epoch 54\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 96.48\tLoss: 10.87\n",
      "Batch: 90\tAccuracy: 98.05\tLoss: 10.88\n",
      "Batch: 180\tAccuracy: 98.44\tLoss: 10.92\n",
      "Validation: \n",
      "Accuracy: 90.00997340425532\tLoss: 528.0322532475\n",
      "Test: \n",
      "Accuracy: 87.87109375\tLoss: 450.7733468115333\n",
      "Epoch 55\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 98.44\tLoss: 10.91\n",
      "Batch: 90\tAccuracy: 99.22\tLoss: 10.93\n",
      "Batch: 180\tAccuracy: 98.44\tLoss: 10.98\n",
      "Validation: \n",
      "Accuracy: 89.99335106382979\tLoss: 531.3452707618472\n",
      "Test: \n",
      "Accuracy: 87.44140625\tLoss: 453.83755142986746\n",
      "Epoch 56\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 98.05\tLoss: 10.98\n",
      "Batch: 90\tAccuracy: 98.05\tLoss: 11.00\n",
      "Batch: 180\tAccuracy: 98.05\tLoss: 11.04\n",
      "Validation: \n",
      "Accuracy: 89.63597074468085\tLoss: 535.9971908032894\n",
      "Test: \n",
      "Accuracy: 87.490234375\tLoss: 457.82614731788635\n",
      "Epoch 57\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 98.05\tLoss: 11.05\n",
      "Batch: 90\tAccuracy: 97.66\tLoss: 11.09\n",
      "Batch: 180\tAccuracy: 96.09\tLoss: 11.16\n",
      "Validation: \n",
      "Accuracy: 89.41156914893617\tLoss: 539.1205009192233\n",
      "Test: \n",
      "Accuracy: 86.865234375\tLoss: 460.36569845676456\n",
      "Epoch 58\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 99.22\tLoss: 11.11\n",
      "Batch: 90\tAccuracy: 98.05\tLoss: 11.16\n",
      "Batch: 180\tAccuracy: 99.22\tLoss: 11.17\n",
      "Validation: \n",
      "Accuracy: 88.52227393617021\tLoss: 545.0019224166866\n",
      "Test: \n",
      "Accuracy: 86.474609375\tLoss: 465.14450740814175\n",
      "Epoch 59\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 97.27\tLoss: 11.20\n",
      "Batch: 90\tAccuracy: 98.44\tLoss: 11.23\n",
      "Batch: 180\tAccuracy: 98.05\tLoss: 11.26\n",
      "Validation: \n",
      "Accuracy: 89.17054521276596\tLoss: 547.7723391354084\n",
      "Test: \n",
      "Accuracy: 86.93359375\tLoss: 467.65254102647305\n",
      "Epoch 60\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 97.27\tLoss: 11.29\n",
      "Batch: 90\tAccuracy: 99.22\tLoss: 11.28\n",
      "Batch: 180\tAccuracy: 99.22\tLoss: 11.31\n",
      "Validation: \n",
      "Accuracy: 88.31449468085107\tLoss: 552.8650995463138\n",
      "Test: \n",
      "Accuracy: 86.259765625\tLoss: 472.3746806979183\n",
      "Epoch 61\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 97.66\tLoss: 11.36\n",
      "Batch: 90\tAccuracy: 94.53\tLoss: 11.47\n",
      "Batch: 180\tAccuracy: 99.61\tLoss: 11.38\n",
      "Validation: \n",
      "Accuracy: 89.81050531914893\tLoss: 552.1619311034681\n",
      "Test: \n",
      "Accuracy: 87.666015625\tLoss: 471.36983670294313\n",
      "Epoch 62\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 98.44\tLoss: 11.39\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 11.40\n",
      "Batch: 180\tAccuracy: 98.05\tLoss: 11.47\n",
      "Validation: \n",
      "Accuracy: 90.1346409574468\tLoss: 554.4553795486684\n",
      "Test: \n",
      "Accuracy: 87.734375\tLoss: 473.9132275730368\n",
      "Epoch 63\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 98.44\tLoss: 11.46\n",
      "Batch: 90\tAccuracy: 99.22\tLoss: 11.47\n",
      "Batch: 180\tAccuracy: 98.44\tLoss: 11.50\n",
      "Validation: \n",
      "Accuracy: 89.17054521276596\tLoss: 559.3975102663042\n",
      "Test: \n",
      "Accuracy: 86.953125\tLoss: 477.7118160277608\n",
      "Epoch 64\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 97.27\tLoss: 11.52\n",
      "Batch: 90\tAccuracy: 99.22\tLoss: 11.53\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 11.55\n",
      "Validation: \n",
      "Accuracy: 90.20113031914893\tLoss: 561.0751046359544\n",
      "Test: \n",
      "Accuracy: 87.509765625\tLoss: 479.7410066425804\n",
      "Epoch 65\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 98.83\tLoss: 11.56\n",
      "Batch: 90\tAccuracy: 98.83\tLoss: 11.59\n",
      "Batch: 180\tAccuracy: 99.61\tLoss: 11.61\n",
      "Validation: \n",
      "Accuracy: 89.34507978723404\tLoss: 564.4266166538\n",
      "Test: \n",
      "Accuracy: 87.1484375\tLoss: 482.5752956569195\n",
      "Epoch 66\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 99.61\tLoss: 11.62\n",
      "Batch: 90\tAccuracy: 99.22\tLoss: 11.64\n",
      "Batch: 180\tAccuracy: 99.22\tLoss: 11.66\n",
      "Validation: \n",
      "Accuracy: 89.7440159574468\tLoss: 566.6486905753611\n",
      "Test: \n",
      "Accuracy: 87.36328125\tLoss: 484.63861687481375\n",
      "Epoch 67\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 98.05\tLoss: 11.69\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 11.68\n",
      "Batch: 180\tAccuracy: 99.61\tLoss: 11.71\n",
      "Validation: \n",
      "Accuracy: 89.51961436170212\tLoss: 570.6475831031801\n",
      "Test: \n",
      "Accuracy: 87.138671875\tLoss: 487.76494063437013\n",
      "Epoch 68\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 98.83\tLoss: 11.73\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 11.74\n",
      "Batch: 180\tAccuracy: 98.83\tLoss: 11.77\n",
      "Validation: \n",
      "Accuracy: 89.73570478723404\tLoss: 572.4604281336069\n",
      "Test: \n",
      "Accuracy: 87.2265625\tLoss: 489.07452726364136\n",
      "Epoch 69\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 99.61\tLoss: 11.77\n",
      "Batch: 90\tAccuracy: 99.22\tLoss: 11.80\n",
      "Batch: 180\tAccuracy: 96.88\tLoss: 11.90\n",
      "Validation: \n",
      "Accuracy: 89.24534574468085\tLoss: 576.6822271287441\n",
      "Test: \n",
      "Accuracy: 87.12890625\tLoss: 492.31579899787874\n",
      "Epoch 70\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 96.88\tLoss: 11.89\n",
      "Batch: 90\tAccuracy: 98.83\tLoss: 11.90\n",
      "Batch: 180\tAccuracy: 99.61\tLoss: 11.91\n",
      "Validation: \n",
      "Accuracy: 88.21476063829788\tLoss: 582.840414834023\n",
      "Test: \n",
      "Accuracy: 86.357421875\tLoss: 497.4758903980259\n",
      "Epoch 71\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 97.66\tLoss: 11.97\n",
      "Batch: 90\tAccuracy: 99.61\tLoss: 11.92\n",
      "Batch: 180\tAccuracy: 95.70\tLoss: 12.05\n",
      "Validation: \n",
      "Accuracy: 89.4032579787234\tLoss: 580.0788397490978\n",
      "Test: \n",
      "Accuracy: 86.97265625\tLoss: 495.5553145259619\n",
      "Epoch 72\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 98.05\tLoss: 11.97\n",
      "Batch: 90\tAccuracy: 99.22\tLoss: 11.98\n",
      "Batch: 180\tAccuracy: 97.66\tLoss: 12.03\n",
      "Validation: \n",
      "Accuracy: 89.85206117021278\tLoss: 583.8681023269892\n",
      "Test: \n",
      "Accuracy: 87.71484375\tLoss: 499.0372431129217\n",
      "Epoch 73\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 99.61\tLoss: 12.01\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 12.03\n",
      "Batch: 180\tAccuracy: 99.22\tLoss: 12.06\n",
      "Validation: \n",
      "Accuracy: 89.23703457446808\tLoss: 588.4442264676094\n",
      "Test: \n",
      "Accuracy: 87.05078125\tLoss: 502.8311549127099\n",
      "Epoch 74\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 98.83\tLoss: 12.09\n",
      "Batch: 90\tAccuracy: 99.61\tLoss: 12.08\n",
      "Batch: 180\tAccuracy: 99.22\tLoss: 12.11\n",
      "Validation: \n",
      "Accuracy: 89.07081117021278\tLoss: 590.2664804816246\n",
      "Test: \n",
      "Accuracy: 87.40234375\tLoss: 503.86121401190786\n",
      "Epoch 75\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 98.44\tLoss: 12.11\n",
      "Batch: 90\tAccuracy: 99.22\tLoss: 12.14\n",
      "Batch: 180\tAccuracy: 99.22\tLoss: 12.15\n",
      "Validation: \n",
      "Accuracy: 89.48636968085107\tLoss: 593.2336252272129\n",
      "Test: \n",
      "Accuracy: 87.138671875\tLoss: 507.0664006471631\n",
      "Epoch 76\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 99.61\tLoss: 12.16\n",
      "Batch: 90\tAccuracy: 99.61\tLoss: 12.17\n",
      "Batch: 180\tAccuracy: 99.61\tLoss: 12.19\n",
      "Validation: \n",
      "Accuracy: 89.46143617021278\tLoss: 594.2628893613816\n",
      "Test: \n",
      "Accuracy: 87.041015625\tLoss: 508.3037491440776\n",
      "Epoch 77\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 98.83\tLoss: 12.20\n",
      "Batch: 90\tAccuracy: 99.61\tLoss: 12.20\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 12.23\n",
      "Validation: \n",
      "Accuracy: 89.72739361702128\tLoss: 595.2114236742258\n",
      "Test: \n",
      "Accuracy: 87.20703125\tLoss: 509.005798667669\n",
      "Epoch 78\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 99.61\tLoss: 12.22\n",
      "Batch: 90\tAccuracy: 99.61\tLoss: 12.25\n",
      "Batch: 180\tAccuracy: 98.83\tLoss: 12.30\n",
      "Validation: \n",
      "Accuracy: 88.95445478723404\tLoss: 598.1154833197594\n",
      "Test: \n",
      "Accuracy: 86.767578125\tLoss: 510.7689976096156\n",
      "Epoch 79\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 97.66\tLoss: 12.31\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 12.29\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 12.31\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 600.6321298718452\n",
      "Test: \n",
      "Accuracy: 87.265625\tLoss: 513.8398659378287\n",
      "Epoch 80\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 99.61\tLoss: 12.31\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 12.33\n",
      "Batch: 180\tAccuracy: 99.61\tLoss: 12.35\n",
      "Validation: \n",
      "Accuracy: 89.51130319148936\tLoss: 602.9935162395234\n",
      "Test: \n",
      "Accuracy: 87.197265625\tLoss: 515.4650332629677\n",
      "Epoch 81\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 99.61\tLoss: 12.34\n",
      "Batch: 90\tAccuracy: 99.61\tLoss: 12.37\n",
      "Batch: 180\tAccuracy: 99.61\tLoss: 12.38\n",
      "Validation: \n",
      "Accuracy: 90.22606382978722\tLoss: 603.5474428534508\n",
      "Test: \n",
      "Accuracy: 87.509765625\tLoss: 516.0838479101658\n",
      "Epoch 82\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 12.39\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 12.40\n",
      "Batch: 180\tAccuracy: 99.61\tLoss: 12.42\n",
      "Validation: \n",
      "Accuracy: 90.00166223404256\tLoss: 605.5099311769009\n",
      "Test: \n",
      "Accuracy: 87.451171875\tLoss: 517.7928372174501\n",
      "Epoch 83\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 12.41\n",
      "Batch: 90\tAccuracy: 99.61\tLoss: 12.43\n",
      "Batch: 180\tAccuracy: 99.61\tLoss: 12.45\n",
      "Validation: \n",
      "Accuracy: 89.7190824468085\tLoss: 607.8224425554275\n",
      "Test: \n",
      "Accuracy: 87.3828125\tLoss: 519.7659083306786\n",
      "Epoch 84\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 12.44\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 12.46\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 12.47\n",
      "Validation: \n",
      "Accuracy: 90.03490691489363\tLoss: 610.2199677050119\n",
      "Test: \n",
      "Accuracy: 87.314453125\tLoss: 521.9924180507663\n",
      "Epoch 85\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 12.47\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 12.49\n",
      "Batch: 180\tAccuracy: 99.22\tLoss: 12.54\n",
      "Validation: \n",
      "Accuracy: 88.3560505319149\tLoss: 612.6044054210181\n",
      "Test: \n",
      "Accuracy: 86.5625\tLoss: 523.0975973755118\n",
      "Epoch 86\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 99.61\tLoss: 12.53\n",
      "Batch: 90\tAccuracy: 99.61\tLoss: 12.54\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 12.56\n",
      "Validation: \n",
      "Accuracy: 89.46143617021278\tLoss: 615.3297092854976\n",
      "Test: \n",
      "Accuracy: 86.9140625\tLoss: 526.2971435189245\n",
      "Epoch 87\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 99.61\tLoss: 12.56\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 12.57\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 12.59\n",
      "Validation: \n",
      "Accuracy: 89.98503989361703\tLoss: 614.8836673319345\n",
      "Test: \n",
      "Accuracy: 87.34375\tLoss: 526.0312803387645\n",
      "Epoch 88\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 12.58\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 12.60\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 12.61\n",
      "Validation: \n",
      "Accuracy: 89.56948138297872\tLoss: 617.4846993386745\n",
      "Test: \n",
      "Accuracy: 87.294921875\tLoss: 527.7237716317177\n",
      "Epoch 89\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 99.61\tLoss: 12.62\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 12.62\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 12.64\n",
      "Validation: \n",
      "Accuracy: 89.9684175531915\tLoss: 617.5597044259305\n",
      "Test: \n",
      "Accuracy: 87.48046875\tLoss: 528.3482006043192\n",
      "Epoch 90\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 12.64\n",
      "Batch: 90\tAccuracy: 99.61\tLoss: 12.65\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 12.66\n",
      "Validation: \n",
      "Accuracy: 89.64428191489363\tLoss: 620.1551376223565\n",
      "Test: \n",
      "Accuracy: 87.28515625\tLoss: 530.5196951031683\n",
      "Epoch 91\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 12.67\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 12.68\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 12.69\n",
      "Validation: \n",
      "Accuracy: 89.83543882978722\tLoss: 620.0647533953195\n",
      "Test: \n",
      "Accuracy: 87.353515625\tLoss: 530.7447717636827\n",
      "Epoch 92\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 12.69\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 12.70\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 12.71\n",
      "Validation: \n",
      "Accuracy: 90.10139627659575\tLoss: 622.0320398569106\n",
      "Test: \n",
      "Accuracy: 87.412109375\tLoss: 532.3282294869425\n",
      "Epoch 93\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 12.71\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 12.72\n",
      "Batch: 180\tAccuracy: 95.70\tLoss: 12.86\n",
      "Validation: \n",
      "Accuracy: 88.27293882978722\tLoss: 627.8947715640063\n",
      "Test: \n",
      "Accuracy: 85.712890625\tLoss: 536.636750489473\n",
      "Epoch 94\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 93.75\tLoss: 12.95\n",
      "Batch: 90\tAccuracy: 98.83\tLoss: 12.81\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 12.81\n",
      "Validation: \n",
      "Accuracy: 89.8936170212766\tLoss: 627.2787373125552\n",
      "Test: \n",
      "Accuracy: 87.529296875\tLoss: 535.8199442327025\n",
      "Epoch 95\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 99.61\tLoss: 12.82\n",
      "Batch: 90\tAccuracy: 99.61\tLoss: 12.82\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 12.83\n",
      "Validation: \n",
      "Accuracy: 90.07646276595744\tLoss: 627.485129147768\n",
      "Test: \n",
      "Accuracy: 87.44140625\tLoss: 536.466659039259\n",
      "Epoch 96\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 12.83\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 12.84\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 12.85\n",
      "Validation: \n",
      "Accuracy: 89.93517287234043\tLoss: 628.7183774054051\n",
      "Test: \n",
      "Accuracy: 87.451171875\tLoss: 537.7837604433296\n",
      "Epoch 97\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 12.85\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 12.87\n",
      "Batch: 180\tAccuracy: 99.61\tLoss: 12.88\n",
      "Validation: \n",
      "Accuracy: 89.73570478723404\tLoss: 629.7584456920629\n",
      "Test: \n",
      "Accuracy: 87.5390625\tLoss: 538.7743969857696\n",
      "Epoch 98\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 12.87\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 12.89\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 12.90\n",
      "Validation: \n",
      "Accuracy: 89.15392287234043\tLoss: 632.6130582988262\n",
      "Test: \n",
      "Accuracy: 86.982421875\tLoss: 540.7927273064852\n",
      "Epoch 99\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 99.61\tLoss: 12.90\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 12.91\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 12.92\n",
      "Validation: \n",
      "Accuracy: 90.03490691489363\tLoss: 632.0683401525021\n",
      "Test: \n",
      "Accuracy: 87.5\tLoss: 540.519684702158\n",
      "Epoch 100\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 12.91\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 12.92\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 12.93\n",
      "Validation: \n",
      "Accuracy: 89.77726063829788\tLoss: 633.174334987998\n",
      "Test: \n",
      "Accuracy: 87.412109375\tLoss: 541.4595416486263\n",
      "Epoch 101\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 12.93\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 12.94\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 12.95\n",
      "Validation: \n",
      "Accuracy: 89.99335106382979\tLoss: 634.1034343302249\n",
      "Test: \n",
      "Accuracy: 87.59765625\tLoss: 542.380277708173\n",
      "Epoch 102\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 12.95\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 12.96\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 12.97\n",
      "Validation: \n",
      "Accuracy: 89.86037234042553\tLoss: 635.3451155424118\n",
      "Test: \n",
      "Accuracy: 87.373046875\tLoss: 543.5190145671368\n",
      "Epoch 103\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 12.97\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 12.98\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 12.99\n",
      "Validation: \n",
      "Accuracy: 89.82712765957447\tLoss: 636.1072184294462\n",
      "Test: \n",
      "Accuracy: 87.48046875\tLoss: 544.2011179327965\n",
      "Epoch 104\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 12.98\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 12.99\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.00\n",
      "Validation: \n",
      "Accuracy: 89.80219414893617\tLoss: 637.079878878594\n",
      "Test: \n",
      "Accuracy: 87.451171875\tLoss: 545.0458370447162\n",
      "Epoch 105\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.00\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.01\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.02\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 637.7200607657433\n",
      "Test: \n",
      "Accuracy: 87.1875\tLoss: 545.8637869656086\n",
      "Epoch 106\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.02\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.03\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.03\n",
      "Validation: \n",
      "Accuracy: 89.86037234042553\tLoss: 639.0850279331207\n",
      "Test: \n",
      "Accuracy: 87.4609375\tLoss: 546.7533155679703\n",
      "Epoch 107\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.03\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.04\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.05\n",
      "Validation: \n",
      "Accuracy: 89.7190824468085\tLoss: 640.2822614848615\n",
      "Test: \n",
      "Accuracy: 87.51953125\tLoss: 547.5826955437658\n",
      "Epoch 108\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.05\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.06\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.06\n",
      "Validation: \n",
      "Accuracy: 89.83543882978722\tLoss: 640.3594720482831\n",
      "Test: \n",
      "Accuracy: 87.470703125\tLoss: 548.0774946212772\n",
      "Epoch 109\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.07\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.07\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.08\n",
      "Validation: \n",
      "Accuracy: 89.80219414893617\tLoss: 641.554872637987\n",
      "Test: \n",
      "Accuracy: 87.5390625\tLoss: 548.8488309979441\n",
      "Epoch 110\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.08\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.09\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.09\n",
      "Validation: \n",
      "Accuracy: 89.7938829787234\tLoss: 642.1786835342646\n",
      "Test: \n",
      "Accuracy: 87.5390625\tLoss: 549.6123957931995\n",
      "Epoch 111\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.09\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.10\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.11\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 643.3404067039485\n",
      "Test: \n",
      "Accuracy: 87.4609375\tLoss: 550.5606678426262\n",
      "Epoch 112\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.11\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.11\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.12\n",
      "Validation: \n",
      "Accuracy: 89.69414893617021\tLoss: 643.7758266359563\n",
      "Test: \n",
      "Accuracy: 87.431640625\tLoss: 551.0915435850617\n",
      "Epoch 113\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.12\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.13\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.13\n",
      "Validation: \n",
      "Accuracy: 89.75232712765957\tLoss: 644.6207547247409\n",
      "Test: \n",
      "Accuracy: 87.59765625\tLoss: 551.4792647361758\n",
      "Epoch 114\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.13\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.14\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.15\n",
      "Validation: \n",
      "Accuracy: 89.76063829787235\tLoss: 645.4026156127452\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 552.1324629187586\n",
      "Epoch 115\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.15\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.15\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.16\n",
      "Validation: \n",
      "Accuracy: 89.95179521276596\tLoss: 645.8510416150093\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 552.6785642206669\n",
      "Epoch 116\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.16\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.17\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.17\n",
      "Validation: \n",
      "Accuracy: 89.90192819148936\tLoss: 646.861095792055\n",
      "Test: \n",
      "Accuracy: 87.59765625\tLoss: 553.5210398733618\n",
      "Epoch 117\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.17\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.18\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.18\n",
      "Validation: \n",
      "Accuracy: 89.7689494680851\tLoss: 647.3306280314923\n",
      "Test: \n",
      "Accuracy: 87.548828125\tLoss: 554.0263295173643\n",
      "Epoch 118\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.18\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.19\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.20\n",
      "Validation: \n",
      "Accuracy: 89.7689494680851\tLoss: 647.9533987104892\n",
      "Test: \n",
      "Accuracy: 87.548828125\tLoss: 554.5072917044164\n",
      "Epoch 119\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.20\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.20\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.21\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 649.1112820118665\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 555.3447417020799\n",
      "Epoch 120\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.21\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.21\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.22\n",
      "Validation: \n",
      "Accuracy: 89.7938829787234\tLoss: 649.3579332619906\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 555.7669647037982\n",
      "Epoch 121\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.22\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.22\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.23\n",
      "Validation: \n",
      "Accuracy: 89.99335106382979\tLoss: 649.9250596523284\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 556.412842899561\n",
      "Epoch 122\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.23\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.23\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.24\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 650.6475477933889\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 556.8353670537475\n",
      "Epoch 123\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.24\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.25\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.25\n",
      "Validation: \n",
      "Accuracy: 89.83543882978722\tLoss: 651.0967488229277\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 557.2837089002131\n",
      "Epoch 124\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.25\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.26\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.26\n",
      "Validation: \n",
      "Accuracy: 89.83543882978722\tLoss: 651.9309476613998\n",
      "Test: \n",
      "Accuracy: 87.548828125\tLoss: 558.0200103521347\n",
      "Epoch 125\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.26\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.27\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.27\n",
      "Validation: \n",
      "Accuracy: 89.91023936170212\tLoss: 652.2021722376342\n",
      "Test: \n",
      "Accuracy: 87.578125\tLoss: 558.2583364844319\n",
      "Epoch 126\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.27\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.28\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.28\n",
      "Validation: \n",
      "Accuracy: 89.82712765957447\tLoss: 653.190743285417\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 559.0937522649762\n",
      "Epoch 127\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.28\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.29\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.29\n",
      "Validation: \n",
      "Accuracy: 89.66921542553192\tLoss: 653.5500638127329\n",
      "Test: \n",
      "Accuracy: 87.578125\tLoss: 559.3503664135932\n",
      "Epoch 128\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.29\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.30\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.30\n",
      "Validation: \n",
      "Accuracy: 89.83543882978722\tLoss: 654.1110204458237\n",
      "Test: \n",
      "Accuracy: 87.666015625\tLoss: 560.0446927249432\n",
      "Epoch 129\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.30\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.31\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.31\n",
      "Validation: \n",
      "Accuracy: 89.7689494680851\tLoss: 654.4833848953242\n",
      "Test: \n",
      "Accuracy: 87.548828125\tLoss: 560.2170393466946\n",
      "Epoch 130\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.31\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.32\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.32\n",
      "Validation: \n",
      "Accuracy: 89.7689494680851\tLoss: 655.1928311944006\n",
      "Test: \n",
      "Accuracy: 87.578125\tLoss: 560.9178704321386\n",
      "Epoch 131\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.32\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.33\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.33\n",
      "Validation: \n",
      "Accuracy: 89.7190824468085\tLoss: 655.7096273660658\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 561.2551758289338\n",
      "Epoch 132\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.33\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.34\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.34\n",
      "Validation: \n",
      "Accuracy: 89.8686835106383\tLoss: 656.1853272855284\n",
      "Test: \n",
      "Accuracy: 87.587890625\tLoss: 561.6873150467871\n",
      "Epoch 133\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.34\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.34\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.35\n",
      "Validation: \n",
      "Accuracy: 89.81050531914893\tLoss: 657.0050609111786\n",
      "Test: \n",
      "Accuracy: 87.578125\tLoss: 562.3585289716721\n",
      "Epoch 134\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.35\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.35\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.36\n",
      "Validation: \n",
      "Accuracy: 89.7440159574468\tLoss: 657.2943601191039\n",
      "Test: \n",
      "Accuracy: 87.744140625\tLoss: 562.6323156058785\n",
      "Epoch 135\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.36\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.36\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.37\n",
      "Validation: \n",
      "Accuracy: 89.7190824468085\tLoss: 657.9071583032603\n",
      "Test: \n",
      "Accuracy: 87.607421875\tLoss: 563.1601595282551\n",
      "Epoch 136\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.37\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.37\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.37\n",
      "Validation: \n",
      "Accuracy: 89.85206117021278\tLoss: 658.3578938722605\n",
      "Test: \n",
      "Accuracy: 87.490234375\tLoss: 563.7036761343476\n",
      "Epoch 137\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.37\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.38\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.38\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 658.7795284986496\n",
      "Test: \n",
      "Accuracy: 87.67578125\tLoss: 563.8939779698849\n",
      "Epoch 138\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.38\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.39\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.39\n",
      "Validation: \n",
      "Accuracy: 89.71077127659575\tLoss: 659.0783870339399\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 564.2340733706955\n",
      "Epoch 139\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.39\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.40\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.40\n",
      "Validation: \n",
      "Accuracy: 89.8188164893617\tLoss: 659.7772963643074\n",
      "Test: \n",
      "Accuracy: 87.59765625\tLoss: 564.8990102410316\n",
      "Epoch 140\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.40\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.40\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.41\n",
      "Validation: \n",
      "Accuracy: 89.76063829787235\tLoss: 660.169828957319\n",
      "Test: \n",
      "Accuracy: 87.51953125\tLoss: 565.2516178190709\n",
      "Epoch 141\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.41\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.41\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.42\n",
      "Validation: \n",
      "Accuracy: 89.7440159574468\tLoss: 660.5853342592711\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 565.462142884731\n",
      "Epoch 142\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.42\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.42\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.42\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 661.0021586596961\n",
      "Test: \n",
      "Accuracy: 87.55859375\tLoss: 565.9439236223694\n",
      "Epoch 143\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.42\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.43\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.43\n",
      "Validation: \n",
      "Accuracy: 89.77726063829788\tLoss: 661.4397745251661\n",
      "Test: \n",
      "Accuracy: 87.5\tLoss: 566.3610511124137\n",
      "Epoch 144\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.43\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.44\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.44\n",
      "Validation: \n",
      "Accuracy: 89.77726063829788\tLoss: 661.7801560163498\n",
      "Test: \n",
      "Accuracy: 87.587890625\tLoss: 566.5522770285606\n",
      "Epoch 145\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.44\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.44\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.45\n",
      "Validation: \n",
      "Accuracy: 89.7440159574468\tLoss: 662.262468075752\n",
      "Test: \n",
      "Accuracy: 87.59765625\tLoss: 567.0684021115304\n",
      "Epoch 146\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.45\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.45\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.45\n",
      "Validation: \n",
      "Accuracy: 89.69414893617021\tLoss: 662.77893461585\n",
      "Test: \n",
      "Accuracy: 87.607421875\tLoss: 567.4374103844161\n",
      "Epoch 147\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.45\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.46\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.46\n",
      "Validation: \n",
      "Accuracy: 89.86037234042553\tLoss: 663.2265237271786\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 567.8551973104477\n",
      "Epoch 148\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.46\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.47\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.47\n",
      "Validation: \n",
      "Accuracy: 89.65259308510637\tLoss: 663.5529381096361\n",
      "Test: \n",
      "Accuracy: 87.59765625\tLoss: 568.1598112583162\n",
      "Epoch 149\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.47\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.47\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.48\n",
      "Validation: \n",
      "Accuracy: 89.76063829787235\tLoss: 663.8300052583218\n",
      "Test: \n",
      "Accuracy: 87.548828125\tLoss: 568.5229533016682\n",
      "Epoch 150\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.48\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.48\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.48\n",
      "Validation: \n",
      "Accuracy: 89.7689494680851\tLoss: 664.3776274442678\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 568.8812450766568\n",
      "Epoch 151\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.48\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.49\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.49\n",
      "Validation: \n",
      "Accuracy: 89.81050531914893\tLoss: 664.7611550927164\n",
      "Test: \n",
      "Accuracy: 87.607421875\tLoss: 569.3648659586905\n",
      "Epoch 152\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.49\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.49\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.50\n",
      "Validation: \n",
      "Accuracy: 89.84375\tLoss: 665.3303389370446\n",
      "Test: \n",
      "Accuracy: 87.65625\tLoss: 569.8336116373544\n",
      "Epoch 153\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.50\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.50\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.50\n",
      "Validation: \n",
      "Accuracy: 89.7440159574468\tLoss: 665.554360646009\n",
      "Test: \n",
      "Accuracy: 87.607421875\tLoss: 569.9333994686599\n",
      "Epoch 154\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.50\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.51\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.51\n",
      "Validation: \n",
      "Accuracy: 89.75232712765957\tLoss: 665.9437818884848\n",
      "Test: \n",
      "Accuracy: 87.59765625\tLoss: 570.2021898329259\n",
      "Epoch 155\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.51\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.51\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.52\n",
      "Validation: \n",
      "Accuracy: 89.76063829787235\tLoss: 666.3259874880314\n",
      "Test: \n",
      "Accuracy: 87.587890625\tLoss: 570.5535628199577\n",
      "Epoch 156\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.52\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.52\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.52\n",
      "Validation: \n",
      "Accuracy: 89.78557180851064\tLoss: 666.745060801506\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 570.9776696264744\n",
      "Epoch 157\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.52\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.53\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.53\n",
      "Validation: \n",
      "Accuracy: 89.71077127659575\tLoss: 667.0546883046627\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 571.1806738972664\n",
      "Epoch 158\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.53\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.53\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.54\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 667.6293640255933\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 571.6894550323491\n",
      "Epoch 159\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.54\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.54\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.54\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 667.7983256638048\n",
      "Test: \n",
      "Accuracy: 87.587890625\tLoss: 571.9913423061372\n",
      "Epoch 160\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.54\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.55\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.55\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 668.2400938153262\n",
      "Test: \n",
      "Accuracy: 87.548828125\tLoss: 572.2956398427482\n",
      "Epoch 161\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.55\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.55\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.56\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 668.5433056533332\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 572.5294376313682\n",
      "Epoch 162\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.56\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.56\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.56\n",
      "Validation: \n",
      "Accuracy: 89.64428191489363\tLoss: 669.0123048365118\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 572.9336443841456\n",
      "Epoch 163\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.56\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.56\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.57\n",
      "Validation: \n",
      "Accuracy: 89.73570478723404\tLoss: 669.0933092594145\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 573.023285239935\n",
      "Epoch 164\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.57\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.57\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.57\n",
      "Validation: \n",
      "Accuracy: 89.75232712765957\tLoss: 669.526057803631\n",
      "Test: \n",
      "Accuracy: 87.666015625\tLoss: 573.2427736818789\n",
      "Epoch 165\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.57\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.58\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.58\n",
      "Validation: \n",
      "Accuracy: 89.7440159574468\tLoss: 669.7138083100325\n",
      "Test: \n",
      "Accuracy: 87.587890625\tLoss: 573.5998390316968\n",
      "Epoch 166\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.58\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.58\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.59\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 670.241692817211\n",
      "Test: \n",
      "Accuracy: 87.578125\tLoss: 574.0436091721059\n",
      "Epoch 167\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.59\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.59\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.59\n",
      "Validation: \n",
      "Accuracy: 89.76063829787235\tLoss: 670.471701323986\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 574.2821063697338\n",
      "Epoch 168\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.59\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.59\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.60\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 670.7922475576403\n",
      "Test: \n",
      "Accuracy: 87.705078125\tLoss: 574.4606225490569\n",
      "Epoch 169\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.60\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.60\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.60\n",
      "Validation: \n",
      "Accuracy: 89.7689494680851\tLoss: 671.3219422519213\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 574.8646213710313\n",
      "Epoch 170\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.60\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.60\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.61\n",
      "Validation: \n",
      "Accuracy: 89.73570478723404\tLoss: 671.3908172667033\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 575.106484651566\n",
      "Epoch 171\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.61\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.61\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.61\n",
      "Validation: \n",
      "Accuracy: 89.69414893617021\tLoss: 671.887457436323\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 575.457752197981\n",
      "Epoch 172\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.61\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.62\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.62\n",
      "Validation: \n",
      "Accuracy: 89.64428191489363\tLoss: 672.2606527686113\n",
      "Test: \n",
      "Accuracy: 87.607421875\tLoss: 575.7854249775405\n",
      "Epoch 173\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.62\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.62\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.62\n",
      "Validation: \n",
      "Accuracy: 89.65259308510637\tLoss: 672.4897652029989\n",
      "Test: \n",
      "Accuracy: 87.578125\tLoss: 576.0320945680143\n",
      "Epoch 174\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.62\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.63\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.63\n",
      "Validation: \n",
      "Accuracy: 89.69414893617021\tLoss: 672.7062063038355\n",
      "Test: \n",
      "Accuracy: 87.65625\tLoss: 576.1316092610364\n",
      "Epoch 175\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.63\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.63\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.63\n",
      "Validation: \n",
      "Accuracy: 89.71077127659575\tLoss: 673.1606941223145\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 576.521695971489\n",
      "Epoch 176\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.64\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.64\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.64\n",
      "Validation: \n",
      "Accuracy: 89.65259308510637\tLoss: 673.4747255206106\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 576.8131612241269\n",
      "Epoch 177\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.64\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.64\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.65\n",
      "Validation: \n",
      "Accuracy: 89.64428191489363\tLoss: 673.6740249752996\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 577.0274721086026\n",
      "Epoch 178\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.65\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.65\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.65\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 673.8863575100901\n",
      "Test: \n",
      "Accuracy: 87.5390625\tLoss: 577.3159576058387\n",
      "Epoch 179\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.65\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.65\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.66\n",
      "Validation: \n",
      "Accuracy: 89.71077127659575\tLoss: 674.0207940354945\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 577.3085106909274\n",
      "Epoch 180\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.66\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.66\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.66\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 674.440281575918\n",
      "Test: \n",
      "Accuracy: 87.67578125\tLoss: 577.6225768923761\n",
      "Epoch 181\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.66\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.66\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.67\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 674.6949213802812\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 577.9147048592569\n",
      "Epoch 182\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.67\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.67\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.67\n",
      "Validation: \n",
      "Accuracy: 89.7190824468085\tLoss: 675.0217770278456\n",
      "Test: \n",
      "Accuracy: 87.568359375\tLoss: 578.1519615054129\n",
      "Epoch 183\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.67\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.67\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.68\n",
      "Validation: \n",
      "Accuracy: 89.72739361702128\tLoss: 675.3406285405157\n",
      "Test: \n",
      "Accuracy: 87.587890625\tLoss: 578.5103080570699\n",
      "Epoch 184\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.68\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.68\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.68\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 675.5836072802546\n",
      "Test: \n",
      "Accuracy: 87.587890625\tLoss: 578.7215799987315\n",
      "Epoch 185\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.68\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.68\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.69\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 675.8829612255103\n",
      "Test: \n",
      "Accuracy: 87.578125\tLoss: 579.007092058659\n",
      "Epoch 186\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.69\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.69\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.69\n",
      "Validation: \n",
      "Accuracy: 89.7190824468085\tLoss: 676.1424890577799\n",
      "Test: \n",
      "Accuracy: 87.5390625\tLoss: 579.2837347090249\n",
      "Epoch 187\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.69\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.69\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.69\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 676.4545797944071\n",
      "Test: \n",
      "Accuracy: 87.59765625\tLoss: 579.4559084475039\n",
      "Epoch 188\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.69\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.70\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.70\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 676.8166592657568\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 579.7121789753436\n",
      "Epoch 189\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.70\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.70\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.70\n",
      "Validation: \n",
      "Accuracy: 89.64428191489363\tLoss: 677.0443912565714\n",
      "Test: \n",
      "Accuracy: 87.59765625\tLoss: 579.9041073620324\n",
      "Epoch 190\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.70\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.71\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.71\n",
      "Validation: \n",
      "Accuracy: 89.7190824468085\tLoss: 677.150402140618\n",
      "Test: \n",
      "Accuracy: 87.578125\tLoss: 580.0631538331513\n",
      "Epoch 191\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.71\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.71\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.71\n",
      "Validation: \n",
      "Accuracy: 89.71077127659575\tLoss: 677.7863565325739\n",
      "Test: \n",
      "Accuracy: 87.71484375\tLoss: 580.6142171323298\n",
      "Epoch 192\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.71\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.72\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.72\n",
      "Validation: \n",
      "Accuracy: 89.76063829787235\tLoss: 677.807363063097\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 580.6852725148201\n",
      "Epoch 193\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.72\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.72\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.72\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 677.9582415282724\n",
      "Test: \n",
      "Accuracy: 87.666015625\tLoss: 580.7021657824517\n",
      "Epoch 194\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.72\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.72\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.73\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 678.3904930293562\n",
      "Test: \n",
      "Accuracy: 87.578125\tLoss: 581.0782460272311\n",
      "Epoch 195\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.73\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.73\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.73\n",
      "Validation: \n",
      "Accuracy: 89.73570478723404\tLoss: 678.6119250237936\n",
      "Test: \n",
      "Accuracy: 87.67578125\tLoss: 581.2981900274749\n",
      "Epoch 196\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.73\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.73\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.74\n",
      "Validation: \n",
      "Accuracy: 89.7440159574468\tLoss: 678.9591845870018\n",
      "Test: \n",
      "Accuracy: 87.578125\tLoss: 581.7074343860149\n",
      "Epoch 197\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.74\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.74\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.74\n",
      "Validation: \n",
      "Accuracy: 89.7440159574468\tLoss: 679.0415841937065\n",
      "Test: \n",
      "Accuracy: 87.578125\tLoss: 581.7580814957619\n",
      "Epoch 198\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.74\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.74\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.74\n",
      "Validation: \n",
      "Accuracy: 89.69414893617021\tLoss: 679.3848307669165\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 581.9563177525996\n",
      "Epoch 199\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.74\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.75\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.75\n",
      "Validation: \n",
      "Accuracy: 89.69414893617021\tLoss: 679.7338794469833\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 582.2551255226135\n",
      "Epoch 200\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.75\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.75\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.75\n",
      "Validation: \n",
      "Accuracy: 89.64428191489363\tLoss: 679.6924774348736\n",
      "Test: \n",
      "Accuracy: 87.685546875\tLoss: 582.2324173748493\n",
      "Epoch 201\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.75\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.75\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.76\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 680.1836414098738\n",
      "Test: \n",
      "Accuracy: 87.6953125\tLoss: 582.6166346967221\n",
      "Epoch 202\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.76\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.76\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.76\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 680.1928282201296\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 582.6458494663243\n",
      "Epoch 203\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.76\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.76\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.76\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 680.5507928907865\n",
      "Test: \n",
      "Accuracy: 87.666015625\tLoss: 582.9525325000282\n",
      "Epoch 204\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.77\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.77\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.77\n",
      "Validation: \n",
      "Accuracy: 89.69414893617021\tLoss: 680.6605503469704\n",
      "Test: \n",
      "Accuracy: 87.666015625\tLoss: 583.0615508258344\n",
      "Epoch 205\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.77\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.77\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.77\n",
      "Validation: \n",
      "Accuracy: 89.66921542553192\tLoss: 680.8844560742376\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 583.2303729355336\n",
      "Epoch 206\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.77\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.77\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.78\n",
      "Validation: \n",
      "Accuracy: 89.7689494680851\tLoss: 681.1004201948637\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 583.4135126769538\n",
      "Epoch 207\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.78\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.78\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.78\n",
      "Validation: \n",
      "Accuracy: 89.75232712765957\tLoss: 681.2974404990673\n",
      "Test: \n",
      "Accuracy: 87.705078125\tLoss: 583.6753937900066\n",
      "Epoch 208\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.78\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.78\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.78\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 681.685228723287\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 583.8683149218555\n",
      "Epoch 209\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.79\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.79\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.79\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 681.8469901919367\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 584.0635347068309\n",
      "Epoch 210\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.79\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.79\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.79\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 682.2258577346802\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 584.4114360809326\n",
      "Epoch 211\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.79\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.79\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.80\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 682.3842114508152\n",
      "Test: \n",
      "Accuracy: 87.578125\tLoss: 584.5813589692116\n",
      "Epoch 212\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.80\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.80\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.80\n",
      "Validation: \n",
      "Accuracy: 89.65259308510637\tLoss: 682.4358653485773\n",
      "Test: \n",
      "Accuracy: 87.568359375\tLoss: 584.6535116434098\n",
      "Epoch 213\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.80\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.80\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.80\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 682.5393109083174\n",
      "Test: \n",
      "Accuracy: 87.607421875\tLoss: 584.7615847289563\n",
      "Epoch 214\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.80\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.81\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.81\n",
      "Validation: \n",
      "Accuracy: 89.66090425531915\tLoss: 682.900113761425\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 585.0301142036915\n",
      "Epoch 215\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.81\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.81\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.81\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 683.2524535179144\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 585.334555536509\n",
      "Epoch 216\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.81\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.81\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.82\n",
      "Validation: \n",
      "Accuracy: 89.73570478723404\tLoss: 683.4821037590506\n",
      "Test: \n",
      "Accuracy: 87.59765625\tLoss: 585.5343351662158\n",
      "Epoch 217\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.82\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.82\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.82\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 683.5668113589285\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 585.6439139842988\n",
      "Epoch 218\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.82\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.82\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.82\n",
      "Validation: \n",
      "Accuracy: 89.64428191489363\tLoss: 683.6811369001867\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 585.699253052473\n",
      "Epoch 219\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.82\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.82\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.83\n",
      "Validation: \n",
      "Accuracy: 89.75232712765957\tLoss: 684.0210175335413\n",
      "Test: \n",
      "Accuracy: 87.666015625\tLoss: 586.0385322272782\n",
      "Epoch 220\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.83\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.83\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.83\n",
      "Validation: \n",
      "Accuracy: 89.71077127659575\tLoss: 684.5052016079426\n",
      "Test: \n",
      "Accuracy: 87.666015625\tLoss: 586.4339646100998\n",
      "Epoch 221\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.83\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.83\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.83\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 684.2434649646276\n",
      "Test: \n",
      "Accuracy: 87.685546875\tLoss: 586.2322864532466\n",
      "Epoch 222\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.83\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.84\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.84\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 684.4884119033813\n",
      "Test: \n",
      "Accuracy: 87.666015625\tLoss: 586.4469507336617\n",
      "Epoch 223\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.84\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.84\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.84\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 684.779180967808\n",
      "Test: \n",
      "Accuracy: 87.67578125\tLoss: 586.6202743649483\n",
      "Epoch 224\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.84\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.84\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.84\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 684.9788692533972\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 586.8537402153015\n",
      "Epoch 225\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.84\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.85\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.85\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 685.2342734694479\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 587.0744262039661\n",
      "Epoch 226\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.85\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.85\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.85\n",
      "Validation: \n",
      "Accuracy: 89.66090425531915\tLoss: 685.2558106005198\n",
      "Test: \n",
      "Accuracy: 87.607421875\tLoss: 587.1486677825455\n",
      "Epoch 227\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.85\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.85\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.86\n",
      "Validation: \n",
      "Accuracy: 89.66090425531915\tLoss: 685.6971596896655\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 587.5608904957776\n",
      "Epoch 228\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.86\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.86\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.86\n",
      "Validation: \n",
      "Accuracy: 89.65259308510637\tLoss: 685.7850046098234\n",
      "Test: \n",
      "Accuracy: 87.666015625\tLoss: 587.6482079327106\n",
      "Epoch 229\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.86\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.86\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.86\n",
      "Validation: \n",
      "Accuracy: 89.63597074468085\tLoss: 686.25318865776\n",
      "Test: \n",
      "Accuracy: 87.587890625\tLoss: 587.9357781708236\n",
      "Epoch 230\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.86\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.86\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.87\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 686.2322553753851\n",
      "Test: \n",
      "Accuracy: 87.6953125\tLoss: 587.8045237958431\n",
      "Epoch 231\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.87\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.87\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.87\n",
      "Validation: \n",
      "Accuracy: 89.65259308510637\tLoss: 686.4457030415541\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 588.128750532866\n",
      "Epoch 232\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.87\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.87\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.87\n",
      "Validation: \n",
      "Accuracy: 89.7190824468085\tLoss: 686.4243616759777\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 588.1082101166248\n",
      "Epoch 233\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.87\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.87\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.88\n",
      "Validation: \n",
      "Accuracy: 89.69414893617021\tLoss: 686.9107522845262\n",
      "Test: \n",
      "Accuracy: 87.59765625\tLoss: 588.5246712267394\n",
      "Epoch 234\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.88\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.88\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.88\n",
      "Validation: \n",
      "Accuracy: 89.66921542553192\tLoss: 687.0156319975856\n",
      "Test: \n",
      "Accuracy: 87.6953125\tLoss: 588.5341591536999\n",
      "Epoch 235\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.88\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.88\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.88\n",
      "Validation: \n",
      "Accuracy: 89.64428191489363\tLoss: 687.2121563851833\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 588.7466440498829\n",
      "Epoch 236\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.88\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.88\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.89\n",
      "Validation: \n",
      "Accuracy: 89.71077127659575\tLoss: 687.2168424606317\n",
      "Test: \n",
      "Accuracy: 87.6953125\tLoss: 588.8051395416255\n",
      "Epoch 237\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.89\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.89\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.89\n",
      "Validation: \n",
      "Accuracy: 89.69414893617021\tLoss: 687.4451634824279\n",
      "Test: \n",
      "Accuracy: 87.666015625\tLoss: 589.0283957123756\n",
      "Epoch 238\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.89\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.89\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.89\n",
      "Validation: \n",
      "Accuracy: 89.60272606382979\tLoss: 687.8095840275284\n",
      "Test: \n",
      "Accuracy: 87.607421875\tLoss: 589.2870157659054\n",
      "Epoch 239\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.89\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.89\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.89\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 687.987445944548\n",
      "Test: \n",
      "Accuracy: 87.607421875\tLoss: 589.5668378770351\n",
      "Epoch 240\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.89\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.90\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.90\n",
      "Validation: \n",
      "Accuracy: 89.66090425531915\tLoss: 687.8695886909956\n",
      "Test: \n",
      "Accuracy: 87.666015625\tLoss: 589.3689632415767\n",
      "Epoch 241\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.90\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.90\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.90\n",
      "Validation: \n",
      "Accuracy: 89.69414893617021\tLoss: 687.9704627990723\n",
      "Test: \n",
      "Accuracy: 87.607421875\tLoss: 589.4724705815315\n",
      "Epoch 242\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.90\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.90\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.90\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 688.2241014540195\n",
      "Test: \n",
      "Accuracy: 87.65625\tLoss: 589.6615832149982\n",
      "Epoch 243\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.90\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.91\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.91\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 688.6592562377447\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 590.0408413112159\n",
      "Epoch 244\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.91\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.91\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.91\n",
      "Validation: \n",
      "Accuracy: 89.65259308510637\tLoss: 688.7135247886181\n",
      "Test: \n",
      "Accuracy: 87.67578125\tLoss: 590.163418918848\n",
      "Epoch 245\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.91\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.91\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.91\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 688.9886965513226\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 590.3129993081093\n",
      "Epoch 246\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.91\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.92\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.92\n",
      "Validation: \n",
      "Accuracy: 89.7190824468085\tLoss: 689.1374873936182\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 590.54384765029\n",
      "Epoch 247\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.92\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.92\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.92\n",
      "Validation: \n",
      "Accuracy: 89.71077127659575\tLoss: 689.3122714281088\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 590.584160804749\n",
      "Epoch 248\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.92\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.92\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.92\n",
      "Validation: \n",
      "Accuracy: 89.65259308510637\tLoss: 689.4296755194664\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 590.7703102231026\n",
      "Epoch 249\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.92\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.92\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.93\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 689.6683538913724\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 590.9758206903934\n",
      "Epoch 250\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.93\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.93\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.93\n",
      "Validation: \n",
      "Accuracy: 89.64428191489363\tLoss: 689.9656705617901\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 591.2159033715725\n",
      "Epoch 251\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.93\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.93\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.93\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 689.9694111943245\n",
      "Test: \n",
      "Accuracy: 87.65625\tLoss: 591.1385163068771\n",
      "Epoch 252\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.93\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.93\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.93\n",
      "Validation: \n",
      "Accuracy: 89.71077127659575\tLoss: 690.2525743246078\n",
      "Test: \n",
      "Accuracy: 87.71484375\tLoss: 591.5296404361725\n",
      "Epoch 253\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.93\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.94\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.94\n",
      "Validation: \n",
      "Accuracy: 89.63597074468085\tLoss: 690.2884601056576\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 591.510647892952\n",
      "Epoch 254\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.94\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.94\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.94\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 690.6513455212116\n",
      "Test: \n",
      "Accuracy: 87.67578125\tLoss: 591.866432249546\n",
      "Epoch 255\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.94\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.94\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.94\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 690.7237971663478\n",
      "Test: \n",
      "Accuracy: 87.65625\tLoss: 591.957712084055\n",
      "Epoch 256\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.94\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.94\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.95\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 690.8153921037919\n",
      "Test: \n",
      "Accuracy: 87.666015625\tLoss: 591.9527249932294\n",
      "Epoch 257\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.95\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.95\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.95\n",
      "Validation: \n",
      "Accuracy: 89.69414893617021\tLoss: 691.0148261010653\n",
      "Test: \n",
      "Accuracy: 87.685546875\tLoss: 592.0987687408929\n",
      "Epoch 258\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.95\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.95\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.95\n",
      "Validation: \n",
      "Accuracy: 89.7190824468085\tLoss: 691.0288330435759\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 592.201784729958\n",
      "Epoch 259\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.95\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.95\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.95\n",
      "Validation: \n",
      "Accuracy: 89.66921542553192\tLoss: 691.2947514832014\n",
      "Test: \n",
      "Accuracy: 87.587890625\tLoss: 592.2901716530318\n",
      "Epoch 260\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.95\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.96\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.96\n",
      "Validation: \n",
      "Accuracy: 89.63597074468085\tLoss: 691.645979124307\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 592.6373397409911\n",
      "Epoch 261\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.96\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.96\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.96\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 691.5769979119298\n",
      "Test: \n",
      "Accuracy: 87.666015625\tLoss: 592.6522730886936\n",
      "Epoch 262\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.96\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.96\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.96\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 691.8463944196701\n",
      "Test: \n",
      "Accuracy: 87.65625\tLoss: 592.9395055770874\n",
      "Epoch 263\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.96\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.96\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.97\n",
      "Validation: \n",
      "Accuracy: 89.66090425531915\tLoss: 691.9655247032642\n",
      "Test: \n",
      "Accuracy: 87.568359375\tLoss: 592.9734090864658\n",
      "Epoch 264\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.97\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.97\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.97\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 692.2661342263225\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 593.2136149406433\n",
      "Epoch 265\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.97\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.97\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.97\n",
      "Validation: \n",
      "Accuracy: 89.66921542553192\tLoss: 692.2654918670657\n",
      "Test: \n",
      "Accuracy: 87.607421875\tLoss: 593.1437467038631\n",
      "Epoch 266\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.97\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.97\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.97\n",
      "Validation: \n",
      "Accuracy: 89.63597074468085\tLoss: 692.3418034732348\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 593.2062191665177\n",
      "Epoch 267\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.97\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.98\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.98\n",
      "Validation: \n",
      "Accuracy: 89.71077127659575\tLoss: 692.7872591018677\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 593.7281770408154\n",
      "Epoch 268\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.98\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.98\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.98\n",
      "Validation: \n",
      "Accuracy: 89.71077127659575\tLoss: 692.8990863502029\n",
      "Test: \n",
      "Accuracy: 87.67578125\tLoss: 593.7185465991497\n",
      "Epoch 269\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.98\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.98\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.98\n",
      "Validation: \n",
      "Accuracy: 89.69414893617021\tLoss: 692.8655717313293\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 593.7819963693619\n",
      "Epoch 270\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.98\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.98\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.99\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 693.1487512588501\n",
      "Test: \n",
      "Accuracy: 87.666015625\tLoss: 593.9976317882538\n",
      "Epoch 271\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.99\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.99\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.99\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 693.3368851900098\n",
      "Test: \n",
      "Accuracy: 87.578125\tLoss: 594.2022957801819\n",
      "Epoch 272\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.99\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.99\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.99\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 693.3619505882269\n",
      "Test: \n",
      "Accuracy: 87.65625\tLoss: 594.11903834343\n",
      "Epoch 273\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.99\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.99\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 13.99\n",
      "Validation: \n",
      "Accuracy: 89.66921542553192\tLoss: 693.4799365103245\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 594.3094671070576\n",
      "Epoch 274\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 13.99\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 13.99\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.00\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 693.5987293124199\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 594.444790750742\n",
      "Epoch 275\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.00\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.00\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.00\n",
      "Validation: \n",
      "Accuracy: 89.69414893617021\tLoss: 693.9482307732105\n",
      "Test: \n",
      "Accuracy: 87.587890625\tLoss: 594.8036075234413\n",
      "Epoch 276\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.00\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.00\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.00\n",
      "Validation: \n",
      "Accuracy: 89.66090425531915\tLoss: 693.7393822729584\n",
      "Test: \n",
      "Accuracy: 87.67578125\tLoss: 594.4885958433151\n",
      "Epoch 277\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.00\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.00\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.00\n",
      "Validation: \n",
      "Accuracy: 89.64428191489363\tLoss: 694.2916185855865\n",
      "Test: \n",
      "Accuracy: 87.67578125\tLoss: 594.9731114804745\n",
      "Epoch 278\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.00\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.00\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.01\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 694.1948621690279\n",
      "Test: \n",
      "Accuracy: 87.666015625\tLoss: 594.926491439343\n",
      "Epoch 279\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.01\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.01\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.01\n",
      "Validation: \n",
      "Accuracy: 89.66090425531915\tLoss: 694.2634170055389\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 595.0192663967609\n",
      "Epoch 280\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.01\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.01\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.01\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 694.4356756925589\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 595.0783981084828\n",
      "Epoch 281\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.01\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.01\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.01\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 694.6833887517458\n",
      "Test: \n",
      "Accuracy: 87.685546875\tLoss: 595.3260287940507\n",
      "Epoch 282\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.01\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.01\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.02\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 695.0487776100636\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 595.7225436866283\n",
      "Epoch 283\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.02\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.02\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.02\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 694.9824954748148\n",
      "Test: \n",
      "Accuracy: 87.59765625\tLoss: 595.5747638940807\n",
      "Epoch 284\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.02\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.02\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.02\n",
      "Validation: \n",
      "Accuracy: 89.63597074468085\tLoss: 695.2898259520528\n",
      "Test: \n",
      "Accuracy: 87.67578125\tLoss: 595.8846883773804\n",
      "Epoch 285\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.02\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.02\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.02\n",
      "Validation: \n",
      "Accuracy: 89.7440159574468\tLoss: 695.327528095245\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 595.9797956943512\n",
      "Epoch 286\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.02\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.02\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.03\n",
      "Validation: \n",
      "Accuracy: 89.72739361702128\tLoss: 695.4551626741886\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 596.0197359919548\n",
      "Epoch 287\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.03\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.03\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.03\n",
      "Validation: \n",
      "Accuracy: 89.66921542553192\tLoss: 695.4794115424156\n",
      "Test: \n",
      "Accuracy: 87.685546875\tLoss: 596.0436896979809\n",
      "Epoch 288\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.03\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.03\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.03\n",
      "Validation: \n",
      "Accuracy: 89.66090425531915\tLoss: 695.8117304027074\n",
      "Test: \n",
      "Accuracy: 87.685546875\tLoss: 596.2371525168414\n",
      "Epoch 289\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.03\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.03\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.03\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 695.9574590623379\n",
      "Test: \n",
      "Accuracy: 87.705078125\tLoss: 596.4306245744228\n",
      "Epoch 290\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.03\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.03\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.04\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 696.098720347881\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 596.6495625376701\n",
      "Epoch 291\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.04\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.04\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.04\n",
      "Validation: \n",
      "Accuracy: 89.66921542553192\tLoss: 696.0061848104003\n",
      "Test: \n",
      "Accuracy: 87.65625\tLoss: 596.541499465704\n",
      "Epoch 292\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.04\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.04\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.04\n",
      "Validation: \n",
      "Accuracy: 89.66090425531915\tLoss: 696.3442903697494\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 596.729546546936\n",
      "Epoch 293\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.04\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.04\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.04\n",
      "Validation: \n",
      "Accuracy: 89.66921542553192\tLoss: 696.522605329752\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 596.9550657272339\n",
      "Epoch 294\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.04\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.04\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.05\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 696.6183599650866\n",
      "Test: \n",
      "Accuracy: 87.67578125\tLoss: 596.9674759805207\n",
      "Epoch 295\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.05\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.05\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.05\n",
      "Validation: \n",
      "Accuracy: 89.66921542553192\tLoss: 696.5851804673669\n",
      "Test: \n",
      "Accuracy: 87.685546875\tLoss: 597.0272854268551\n",
      "Epoch 296\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.05\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.05\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.05\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 696.7058062076575\n",
      "Test: \n",
      "Accuracy: 87.65625\tLoss: 597.1286026835446\n",
      "Epoch 297\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.05\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.05\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.05\n",
      "Validation: \n",
      "Accuracy: 89.73570478723404\tLoss: 696.864166671038\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 597.2651850283146\n",
      "Epoch 298\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.05\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.05\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.05\n",
      "Validation: \n",
      "Accuracy: 89.66921542553192\tLoss: 696.9868469715112\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 597.3956830501552\n",
      "Epoch 299\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.05\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.06\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.06\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 697.1587034046644\n",
      "Test: \n",
      "Accuracy: 87.705078125\tLoss: 597.5179006755347\n",
      "Epoch 300\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.06\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.06\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.06\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 697.5159178256982\n",
      "Test: \n",
      "Accuracy: 87.666015625\tLoss: 597.842684924602\n",
      "Epoch 301\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.06\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.06\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.06\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 697.691380447149\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 598.0103047192097\n",
      "Epoch 302\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.06\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.06\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.06\n",
      "Validation: \n",
      "Accuracy: 89.65259308510637\tLoss: 697.4658954918378\n",
      "Test: \n",
      "Accuracy: 87.607421875\tLoss: 597.8155666291709\n",
      "Epoch 303\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.06\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.06\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.07\n",
      "Validation: \n",
      "Accuracy: 89.65259308510637\tLoss: 697.6148910760883\n",
      "Test: \n",
      "Accuracy: 87.59765625\tLoss: 597.8937074840069\n",
      "Epoch 304\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.07\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.07\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.07\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 697.772931462526\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 598.1339536011219\n",
      "Epoch 305\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.07\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.07\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.07\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 697.7606745898727\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 598.0765364468098\n",
      "Epoch 306\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.07\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.07\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.07\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 698.0130249500278\n",
      "Test: \n",
      "Accuracy: 87.666015625\tLoss: 598.2984731793404\n",
      "Epoch 307\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.07\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.07\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.07\n",
      "Validation: \n",
      "Accuracy: 89.65259308510637\tLoss: 698.2555337309834\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 598.44740280509\n",
      "Epoch 308\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.07\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.08\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.08\n",
      "Validation: \n",
      "Accuracy: 89.66921542553192\tLoss: 698.3973013877875\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 598.5832385122781\n",
      "Epoch 309\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.08\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.08\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.08\n",
      "Validation: \n",
      "Accuracy: 89.65259308510637\tLoss: 698.6350857675072\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 598.6964132785797\n",
      "Epoch 310\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.08\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.08\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.08\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 698.6018032252795\n",
      "Test: \n",
      "Accuracy: 87.666015625\tLoss: 598.7361671030526\n",
      "Epoch 311\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.08\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.08\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.08\n",
      "Validation: \n",
      "Accuracy: 89.66921542553192\tLoss: 698.6619941711432\n",
      "Test: \n",
      "Accuracy: 87.578125\tLoss: 598.784211814404\n",
      "Epoch 312\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.08\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.08\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.09\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 699.0604273796088\n",
      "Test: \n",
      "Accuracy: 87.587890625\tLoss: 599.0901233553891\n",
      "Epoch 313\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.09\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.09\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.09\n",
      "Validation: \n",
      "Accuracy: 89.7190824468085\tLoss: 698.9483740568164\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 599.0800975263119\n",
      "Epoch 314\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.09\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.09\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.09\n",
      "Validation: \n",
      "Accuracy: 89.72739361702128\tLoss: 699.044809007645\n",
      "Test: \n",
      "Accuracy: 87.67578125\tLoss: 599.146819382906\n",
      "Epoch 315\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.09\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.09\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.09\n",
      "Validation: \n",
      "Accuracy: 89.69414893617021\tLoss: 699.2536474823949\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 599.3713990747929\n",
      "Epoch 316\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.09\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.09\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.09\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 699.4175039768222\n",
      "Test: \n",
      "Accuracy: 87.6953125\tLoss: 599.5134427249432\n",
      "Epoch 317\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.09\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.10\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.10\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 699.5156112432486\n",
      "Test: \n",
      "Accuracy: 87.6953125\tLoss: 599.5532760918145\n",
      "Epoch 318\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.10\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.10\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.10\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 699.6781624734396\n",
      "Test: \n",
      "Accuracy: 87.666015625\tLoss: 599.6741933822627\n",
      "Epoch 319\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.10\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.10\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.10\n",
      "Validation: \n",
      "Accuracy: 89.64428191489363\tLoss: 699.7154970586306\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 599.6795864701276\n",
      "Epoch 320\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.10\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.10\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.10\n",
      "Validation: \n",
      "Accuracy: 89.7190824468085\tLoss: 699.8164192914957\n",
      "Test: \n",
      "Accuracy: 87.67578125\tLoss: 599.8678408861156\n",
      "Epoch 321\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.10\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.10\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.10\n",
      "Validation: \n",
      "Accuracy: 89.7190824468085\tLoss: 700.0454734444612\n",
      "Test: \n",
      "Accuracy: 87.587890625\tLoss: 600.0605625510211\n",
      "Epoch 322\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.10\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.11\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.11\n",
      "Validation: \n",
      "Accuracy: 89.71077127659575\tLoss: 699.9993493080145\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 599.9530144631867\n",
      "Epoch 323\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.11\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.11\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.11\n",
      "Validation: \n",
      "Accuracy: 89.66921542553192\tLoss: 700.2560816466805\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 600.2095625400543\n",
      "Epoch 324\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.11\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.11\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.11\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 700.2532201468941\n",
      "Test: \n",
      "Accuracy: 87.6953125\tLoss: 600.1672324538231\n",
      "Epoch 325\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.11\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.11\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.11\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 700.5367208182809\n",
      "Test: \n",
      "Accuracy: 87.6953125\tLoss: 600.4964376688004\n",
      "Epoch 326\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.11\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.11\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.12\n",
      "Validation: \n",
      "Accuracy: 89.63597074468085\tLoss: 700.4951409399509\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 600.4028331637383\n",
      "Epoch 327\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.12\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.12\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.12\n",
      "Validation: \n",
      "Accuracy: 89.69414893617021\tLoss: 700.7375797510141\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 600.6511040627952\n",
      "Epoch 328\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.12\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.12\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.12\n",
      "Validation: \n",
      "Accuracy: 89.65259308510637\tLoss: 700.664912605286\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 600.5136235952377\n",
      "Epoch 329\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.12\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.12\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.12\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 700.8204215586192\n",
      "Test: \n",
      "Accuracy: 87.685546875\tLoss: 600.6670475602155\n",
      "Epoch 330\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.12\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.12\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.12\n",
      "Validation: \n",
      "Accuracy: 89.69414893617021\tLoss: 701.1843938827515\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 601.0489493906498\n",
      "Epoch 331\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.12\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.12\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.13\n",
      "Validation: \n",
      "Accuracy: 89.66090425531915\tLoss: 701.0475107729435\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 600.8447311222553\n",
      "Epoch 332\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.13\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.13\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.13\n",
      "Validation: \n",
      "Accuracy: 89.66921542553192\tLoss: 701.3633449912074\n",
      "Test: \n",
      "Accuracy: 87.578125\tLoss: 601.2143961191177\n",
      "Epoch 333\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.13\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.13\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.13\n",
      "Validation: \n",
      "Accuracy: 89.66921542553192\tLoss: 701.5829024076459\n",
      "Test: \n",
      "Accuracy: 87.666015625\tLoss: 601.3500028550625\n",
      "Epoch 334\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.13\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.13\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.13\n",
      "Validation: \n",
      "Accuracy: 89.66921542553192\tLoss: 701.4911482453352\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 601.3566326797013\n",
      "Epoch 335\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.13\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.13\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.13\n",
      "Validation: \n",
      "Accuracy: 89.66921542553192\tLoss: 701.5399351775643\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 601.3148455619812\n",
      "Epoch 336\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.13\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.13\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.14\n",
      "Validation: \n",
      "Accuracy: 89.73570478723404\tLoss: 701.7527422845367\n",
      "Test: \n",
      "Accuracy: 87.59765625\tLoss: 601.5146423280239\n",
      "Epoch 337\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.14\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.14\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.14\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 701.8778792619705\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 601.6784328520298\n",
      "Epoch 338\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.14\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.14\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.14\n",
      "Validation: \n",
      "Accuracy: 89.63597074468085\tLoss: 701.7950373709208\n",
      "Test: \n",
      "Accuracy: 87.607421875\tLoss: 601.5710566937928\n",
      "Epoch 339\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.14\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.14\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.14\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 701.9980600893504\n",
      "Test: \n",
      "Accuracy: 87.59765625\tLoss: 601.7276978194718\n",
      "Epoch 340\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.14\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.14\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.14\n",
      "Validation: \n",
      "Accuracy: 89.66090425531915\tLoss: 702.1296928346163\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 601.8626987040047\n",
      "Epoch 341\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.14\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.14\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.14\n",
      "Validation: \n",
      "Accuracy: 89.66921542553192\tLoss: 702.3145292043689\n",
      "Test: \n",
      "Accuracy: 87.6953125\tLoss: 601.9937317967415\n",
      "Epoch 342\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.14\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.15\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.15\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 702.2826094329357\n",
      "Test: \n",
      "Accuracy: 87.65625\tLoss: 601.9717130661011\n",
      "Epoch 343\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.15\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.15\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.15\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 702.593883800506\n",
      "Test: \n",
      "Accuracy: 87.59765625\tLoss: 602.2820807099338\n",
      "Epoch 344\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.15\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.15\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.15\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 702.6723368525502\n",
      "Test: \n",
      "Accuracy: 87.607421875\tLoss: 602.2564583718777\n",
      "Epoch 345\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.15\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.15\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.15\n",
      "Validation: \n",
      "Accuracy: 89.64428191489363\tLoss: 702.5159403741357\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 602.1183724701405\n",
      "Epoch 346\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.15\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.15\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.15\n",
      "Validation: \n",
      "Accuracy: 89.7190824468085\tLoss: 702.6932485699654\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 602.3664912879467\n",
      "Epoch 347\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.15\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.16\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.16\n",
      "Validation: \n",
      "Accuracy: 89.66090425531915\tLoss: 702.7890930414203\n",
      "Test: \n",
      "Accuracy: 87.67578125\tLoss: 602.4308322370052\n",
      "Epoch 348\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.16\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.16\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.16\n",
      "Validation: \n",
      "Accuracy: 89.69414893617021\tLoss: 703.1938623428348\n",
      "Test: \n",
      "Accuracy: 87.685546875\tLoss: 602.7440980374813\n",
      "Epoch 349\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.16\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.16\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.16\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 703.2377770483497\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 602.8382097482681\n",
      "Epoch 350\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.16\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.16\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.16\n",
      "Validation: \n",
      "Accuracy: 89.65259308510637\tLoss: 703.1804212033749\n",
      "Test: \n",
      "Accuracy: 87.67578125\tLoss: 602.7713911533356\n",
      "Epoch 351\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.16\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.16\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.16\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 703.4050989806649\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 602.9761282205582\n",
      "Epoch 352\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.16\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.16\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.17\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 703.4350961446762\n",
      "Test: \n",
      "Accuracy: 87.724609375\tLoss: 602.9110891819\n",
      "Epoch 353\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.17\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.17\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.17\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 703.4806987166411\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 602.9721697568898\n",
      "Epoch 354\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.17\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.17\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.17\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 703.6489518165582\n",
      "Test: \n",
      "Accuracy: 87.6953125\tLoss: 603.1719347536559\n",
      "Epoch 355\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.17\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.17\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.17\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 703.74951671958\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 603.2579532563686\n",
      "Epoch 356\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.17\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.17\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.17\n",
      "Validation: \n",
      "Accuracy: 89.65259308510637\tLoss: 704.0955471634868\n",
      "Test: \n",
      "Accuracy: 87.607421875\tLoss: 603.5627525448799\n",
      "Epoch 357\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.17\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.17\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.17\n",
      "Validation: \n",
      "Accuracy: 89.64428191489363\tLoss: 704.0830707907674\n",
      "Test: \n",
      "Accuracy: 87.607421875\tLoss: 603.5618743002415\n",
      "Epoch 358\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.17\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.18\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.18\n",
      "Validation: \n",
      "Accuracy: 89.64428191489363\tLoss: 704.0703041136259\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 603.5178868770595\n",
      "Epoch 359\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.18\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.18\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.18\n",
      "Validation: \n",
      "Accuracy: 89.66090425531915\tLoss: 704.2057679653165\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 603.6677460670471\n",
      "Epoch 360\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.18\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.18\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.18\n",
      "Validation: \n",
      "Accuracy: 89.71077127659575\tLoss: 704.0287918150425\n",
      "Test: \n",
      "Accuracy: 87.607421875\tLoss: 603.4574176967144\n",
      "Epoch 361\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.18\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.18\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.18\n",
      "Validation: \n",
      "Accuracy: 89.65259308510637\tLoss: 704.3048454582688\n",
      "Test: \n",
      "Accuracy: 87.607421875\tLoss: 603.7884254455566\n",
      "Epoch 362\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.18\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.18\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.18\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 704.304045540095\n",
      "Test: \n",
      "Accuracy: 87.607421875\tLoss: 603.7104881107812\n",
      "Epoch 363\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.18\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.18\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.19\n",
      "Validation: \n",
      "Accuracy: 89.65259308510637\tLoss: 704.5933729708195\n",
      "Test: \n",
      "Accuracy: 87.607421875\tLoss: 604.0129466056824\n",
      "Epoch 364\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.19\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.19\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.19\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 704.5570565521714\n",
      "Test: \n",
      "Accuracy: 87.578125\tLoss: 603.9807775914669\n",
      "Epoch 365\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.19\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.19\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.19\n",
      "Validation: \n",
      "Accuracy: 89.66090425531915\tLoss: 704.8070337653154\n",
      "Test: \n",
      "Accuracy: 87.666015625\tLoss: 604.1136395931239\n",
      "Epoch 366\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.19\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.19\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.19\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 705.0522061407572\n",
      "Test: \n",
      "Accuracy: 87.666015625\tLoss: 604.3932234346871\n",
      "Epoch 367\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.19\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.19\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.19\n",
      "Validation: \n",
      "Accuracy: 89.69414893617021\tLoss: 704.9322025001055\n",
      "Test: \n",
      "Accuracy: 87.67578125\tLoss: 604.26456797123\n",
      "Epoch 368\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.19\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.19\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.19\n",
      "Validation: \n",
      "Accuracy: 89.66921542553192\tLoss: 704.9962902069092\n",
      "Test: \n",
      "Accuracy: 87.65625\tLoss: 604.321814417839\n",
      "Epoch 369\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.19\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.19\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.20\n",
      "Validation: \n",
      "Accuracy: 89.69414893617021\tLoss: 705.2121989130974\n",
      "Test: \n",
      "Accuracy: 87.587890625\tLoss: 604.5012530684471\n",
      "Epoch 370\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.20\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.20\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.20\n",
      "Validation: \n",
      "Accuracy: 89.69414893617021\tLoss: 705.3971884489063\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 604.7548690736294\n",
      "Epoch 371\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.20\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.20\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.20\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 705.3096666991707\n",
      "Test: \n",
      "Accuracy: 87.607421875\tLoss: 604.5119288265705\n",
      "Epoch 372\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.20\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.20\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.20\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 705.3027484536177\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 604.6340686082845\n",
      "Epoch 373\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.20\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.20\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.20\n",
      "Validation: \n",
      "Accuracy: 89.71077127659575\tLoss: 705.4352889120576\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 604.741432338953\n",
      "Epoch 374\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.20\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.20\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.20\n",
      "Validation: \n",
      "Accuracy: 89.63597074468085\tLoss: 705.568798142671\n",
      "Test: \n",
      "Accuracy: 87.71484375\tLoss: 604.7563927471633\n",
      "Epoch 375\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.20\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.20\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.21\n",
      "Validation: \n",
      "Accuracy: 89.66921542553192\tLoss: 705.892070102691\n",
      "Test: \n",
      "Accuracy: 87.59765625\tLoss: 605.0859366953368\n",
      "Epoch 376\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.21\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.21\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.21\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 705.9186150789258\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 605.2084510028362\n",
      "Epoch 377\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.21\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.21\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.21\n",
      "Validation: \n",
      "Accuracy: 89.63597074468085\tLoss: 706.1906320333475\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 605.425518751144\n",
      "Epoch 378\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.21\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.21\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.21\n",
      "Validation: \n",
      "Accuracy: 89.62765957446808\tLoss: 706.0419829249379\n",
      "Test: \n",
      "Accuracy: 87.587890625\tLoss: 605.2304599881172\n",
      "Epoch 379\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.21\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.21\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.21\n",
      "Validation: \n",
      "Accuracy: 89.61934840425532\tLoss: 706.2529838502413\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 605.4680004715924\n",
      "Epoch 380\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.21\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.21\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.21\n",
      "Validation: \n",
      "Accuracy: 89.62765957446808\tLoss: 706.3391723036766\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 605.4871311783791\n",
      "Epoch 381\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.21\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.21\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.22\n",
      "Validation: \n",
      "Accuracy: 89.73570478723404\tLoss: 706.4527624726289\n",
      "Test: \n",
      "Accuracy: 87.607421875\tLoss: 605.5952783823009\n",
      "Epoch 382\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.22\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.22\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.22\n",
      "Validation: \n",
      "Accuracy: 89.63597074468085\tLoss: 706.5664234161377\n",
      "Test: \n",
      "Accuracy: 87.65625\tLoss: 605.6933605074883\n",
      "Epoch 383\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.22\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.22\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.22\n",
      "Validation: \n",
      "Accuracy: 89.65259308510637\tLoss: 706.4467519879338\n",
      "Test: \n",
      "Accuracy: 87.65625\tLoss: 605.5535248219967\n",
      "Epoch 384\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.22\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.22\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.22\n",
      "Validation: \n",
      "Accuracy: 89.66921542553192\tLoss: 706.7796130120757\n",
      "Test: \n",
      "Accuracy: 87.607421875\tLoss: 605.8576620519161\n",
      "Epoch 385\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.22\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.22\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.22\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 706.5754196047783\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 605.7559494674206\n",
      "Epoch 386\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.22\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.22\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.22\n",
      "Validation: \n",
      "Accuracy: 89.66090425531915\tLoss: 706.7161156892773\n",
      "Test: \n",
      "Accuracy: 87.65625\tLoss: 605.8160985708237\n",
      "Epoch 387\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.22\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.22\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.23\n",
      "Validation: \n",
      "Accuracy: 89.62765957446808\tLoss: 706.780768185854\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 605.88304489851\n",
      "Epoch 388\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.23\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.23\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.23\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 706.9986557304862\n",
      "Test: \n",
      "Accuracy: 87.67578125\tLoss: 606.1495634913445\n",
      "Epoch 389\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.23\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.23\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.23\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 707.0186854720122\n",
      "Test: \n",
      "Accuracy: 87.59765625\tLoss: 606.1445164084439\n",
      "Epoch 390\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.23\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.23\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.23\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 707.1975709199905\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 606.3333017528057\n",
      "Epoch 391\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.23\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.23\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.23\n",
      "Validation: \n",
      "Accuracy: 89.66921542553192\tLoss: 707.3147195696837\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 606.4193429052834\n",
      "Epoch 392\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.23\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.23\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.23\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 707.2678387343877\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 606.3333379030223\n",
      "Epoch 393\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.23\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.23\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.24\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 707.2742902755743\n",
      "Test: \n",
      "Accuracy: 87.666015625\tLoss: 606.3396710455422\n",
      "Epoch 394\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.24\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.24\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.24\n",
      "Validation: \n",
      "Accuracy: 89.66090425531915\tLoss: 707.689795166254\n",
      "Test: \n",
      "Accuracy: 87.587890625\tLoss: 606.6232046186924\n",
      "Epoch 395\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.24\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.24\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.24\n",
      "Validation: \n",
      "Accuracy: 89.69414893617021\tLoss: 707.7348472297198\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 606.7571299672131\n",
      "Epoch 396\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.24\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.24\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.24\n",
      "Validation: \n",
      "Accuracy: 89.64428191489363\tLoss: 707.6395757436756\n",
      "Test: \n",
      "Accuracy: 87.6953125\tLoss: 606.6890459060669\n",
      "Epoch 397\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.24\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.24\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.24\n",
      "Validation: \n",
      "Accuracy: 89.62765957446808\tLoss: 707.9181763470176\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 606.876266092062\n",
      "Epoch 398\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.24\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.24\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.24\n",
      "Validation: \n",
      "Accuracy: 89.66921542553192\tLoss: 707.8588020682329\n",
      "Test: \n",
      "Accuracy: 87.646484375\tLoss: 606.8392987251277\n",
      "Epoch 399\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.24\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.24\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.24\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 707.9380274295801\n",
      "Test: \n",
      "Accuracy: 87.67578125\tLoss: 606.8838782608505\n",
      "Epoch 400\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.24\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.25\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.25\n",
      "Validation: \n",
      "Accuracy: 89.66090425531915\tLoss: 708.1175631582737\n",
      "Test: \n",
      "Accuracy: 87.65625\tLoss: 607.1299534142017\n",
      "Epoch 401\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.25\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.25\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.25\n",
      "Validation: \n",
      "Accuracy: 89.65259308510637\tLoss: 708.3054103076464\n",
      "Test: \n",
      "Accuracy: 87.67578125\tLoss: 607.2277240455155\n",
      "Epoch 402\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.25\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.25\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.25\n",
      "Validation: \n",
      "Accuracy: 89.66921542553192\tLoss: 708.2770735979077\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 607.2719803154469\n",
      "Epoch 403\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.25\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.25\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.25\n",
      "Validation: \n",
      "Accuracy: 89.66921542553192\tLoss: 708.3937501847747\n",
      "Test: \n",
      "Accuracy: 87.65625\tLoss: 607.2988245785236\n",
      "Epoch 404\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.25\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.25\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.25\n",
      "Validation: \n",
      "Accuracy: 89.66090425531915\tLoss: 708.5818528234959\n",
      "Test: \n",
      "Accuracy: 87.65625\tLoss: 607.4561415612698\n",
      "Epoch 405\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.25\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.25\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.25\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 708.6170578956601\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 607.49391451478\n",
      "Epoch 406\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.25\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.25\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.26\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 708.8575927138335\n",
      "Test: \n",
      "Accuracy: 87.587890625\tLoss: 607.7374371886258\n",
      "Epoch 407\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.26\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.26\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.26\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 708.7765601754182\n",
      "Test: \n",
      "Accuracy: 87.65625\tLoss: 607.6453972160812\n",
      "Epoch 408\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.26\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.26\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.26\n",
      "Validation: \n",
      "Accuracy: 89.69414893617021\tLoss: 708.8617008030415\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 607.7244620025158\n",
      "Epoch 409\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.26\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.26\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.26\n",
      "Validation: \n",
      "Accuracy: 89.7190824468085\tLoss: 708.7553360223767\n",
      "Test: \n",
      "Accuracy: 87.607421875\tLoss: 607.6410442888737\n",
      "Epoch 410\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.26\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.26\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.26\n",
      "Validation: \n",
      "Accuracy: 89.69414893617021\tLoss: 708.9711674988273\n",
      "Test: \n",
      "Accuracy: 87.685546875\tLoss: 607.8106934130192\n",
      "Epoch 411\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.26\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.26\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.26\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 708.9758701324463\n",
      "Test: \n",
      "Accuracy: 87.666015625\tLoss: 607.7842710018158\n",
      "Epoch 412\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.26\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.26\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.26\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 709.2488038361079\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 608.0601001679902\n",
      "Epoch 413\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.26\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.27\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.27\n",
      "Validation: \n",
      "Accuracy: 89.66090425531915\tLoss: 709.3317203402513\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 608.1217948496337\n",
      "Epoch 414\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.27\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.27\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.27\n",
      "Validation: \n",
      "Accuracy: 89.63597074468085\tLoss: 709.354619854689\n",
      "Test: \n",
      "Accuracy: 87.578125\tLoss: 608.1197136044502\n",
      "Epoch 415\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.27\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.27\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.27\n",
      "Validation: \n",
      "Accuracy: 89.66090425531915\tLoss: 709.5840940952295\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 608.3897320926185\n",
      "Epoch 416\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.27\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.27\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.27\n",
      "Validation: \n",
      "Accuracy: 89.66090425531915\tLoss: 709.5763235270971\n",
      "Test: \n",
      "Accuracy: 87.59765625\tLoss: 608.3724097907539\n",
      "Epoch 417\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.27\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.27\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.27\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 709.7317584395412\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 608.5029057860374\n",
      "Epoch 418\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.27\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.27\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.27\n",
      "Validation: \n",
      "Accuracy: 89.69414893617021\tLoss: 709.5926913797849\n",
      "Test: \n",
      "Accuracy: 87.685546875\tLoss: 608.4392903447147\n",
      "Epoch 419\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.27\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.27\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.27\n",
      "Validation: \n",
      "Accuracy: 89.64428191489363\tLoss: 709.8080712020394\n",
      "Test: \n",
      "Accuracy: 87.59765625\tLoss: 608.5515431463718\n",
      "Epoch 420\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.27\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.28\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.28\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 709.9765847623348\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 608.7269589304924\n",
      "Epoch 421\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.28\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.28\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.28\n",
      "Validation: \n",
      "Accuracy: 89.67752659574468\tLoss: 709.9334768295291\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 608.6380536258221\n",
      "Epoch 422\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.28\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.28\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.28\n",
      "Validation: \n",
      "Accuracy: 89.65259308510637\tLoss: 710.1089148461822\n",
      "Test: \n",
      "Accuracy: 87.65625\tLoss: 608.858202546835\n",
      "Epoch 423\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.28\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.28\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.28\n",
      "Validation: \n",
      "Accuracy: 89.68583776595744\tLoss: 710.0197451174256\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 608.7174770534039\n",
      "Epoch 424\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.28\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.28\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.28\n",
      "Validation: \n",
      "Accuracy: 89.65259308510637\tLoss: 710.1940830588344\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 608.8561854660511\n",
      "Epoch 425\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.28\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.28\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.28\n",
      "Validation: \n",
      "Accuracy: 89.66090425531915\tLoss: 710.2725689053539\n",
      "Test: \n",
      "Accuracy: 87.578125\tLoss: 608.9352644383907\n",
      "Epoch 426\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.28\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.28\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.28\n",
      "Validation: \n",
      "Accuracy: 89.65259308510637\tLoss: 710.4543553769582\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 609.1558396816249\n",
      "Epoch 427\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.28\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.29\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.29\n",
      "Validation: \n",
      "Accuracy: 89.72739361702128\tLoss: 710.3778063118455\n",
      "Test: \n",
      "Accuracy: 87.6171875\tLoss: 609.0867364704609\n",
      "Epoch 428\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.29\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.29\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.29\n",
      "Validation: \n",
      "Accuracy: 89.72739361702128\tLoss: 710.5819968760007\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 609.2570728063579\n",
      "Epoch 429\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.29\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.29\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.29\n",
      "Validation: \n",
      "Accuracy: 89.71077127659575\tLoss: 710.5508180439466\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 609.1954797208305\n",
      "Epoch 430\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.29\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.29\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.29\n",
      "Validation: \n",
      "Accuracy: 89.66090425531915\tLoss: 710.7024949967855\n",
      "Test: \n",
      "Accuracy: 87.59765625\tLoss: 609.3443827331062\n",
      "Epoch 431\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.29\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.29\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.29\n",
      "Validation: \n",
      "Accuracy: 89.69414893617021\tLoss: 710.8009829699987\n",
      "Test: \n",
      "Accuracy: 87.626953125\tLoss: 609.3733857870097\n",
      "Epoch 432\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.29\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.29\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.29\n",
      "Validation: \n",
      "Accuracy: 89.65259308510637\tLoss: 710.8234468281263\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 609.3949841260905\n",
      "Epoch 433\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.29\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.29\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.29\n",
      "Validation: \n",
      "Accuracy: 89.66090425531915\tLoss: 710.8676796495918\n",
      "Test: \n",
      "Accuracy: 87.63671875\tLoss: 609.4974540174007\n",
      "Epoch 434\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.29\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.30\n",
      "Batch: 180\tAccuracy: 100.00\tLoss: 14.30\n",
      "Validation: \n",
      "Accuracy: 89.70246010638297\tLoss: 710.9170613884926\n",
      "Test: \n",
      "Accuracy: 87.65625\tLoss: 609.5882667005062\n",
      "Epoch 435\n",
      "Train: \n",
      "Batch: 0\tAccuracy: 100.00\tLoss: 14.30\n",
      "Batch: 90\tAccuracy: 100.00\tLoss: 14.30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[307], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Do not change this cell\u001b[39;00m\n\u001b[1;32m      2\u001b[0m random_seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      3\u001b[0m (\n\u001b[1;32m      4\u001b[0m     model,\n\u001b[1;32m      5\u001b[0m     train_losses,\n\u001b[1;32m      6\u001b[0m     train_accuracies,\n\u001b[1;32m      7\u001b[0m     valid_losses,\n\u001b[1;32m      8\u001b[0m     valid_accuracies,\n\u001b[1;32m      9\u001b[0m     test_losses,\n\u001b[1;32m     10\u001b[0m     test_accuracies,\n\u001b[0;32m---> 11\u001b[0m ) \u001b[38;5;241m=\u001b[39m train_conv_model(train_loader, valid_loader, test_loader, random_seed)\n",
      "Cell \u001b[0;32mIn[306], line 34\u001b[0m, in \u001b[0;36mtrain_conv_model\u001b[0;34m(train_loader, valid_loader, test_loader, random_seed)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m model, train_accuracy[i], train_loss[i] \u001b[38;5;241m=\u001b[39m train_step(model, train_loader, loss_fn, optimizer, reg_param, device)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m model, validation_accuracy[i], validation_loss[i] \u001b[38;5;241m=\u001b[39m evaluation_step(model, valid_loader, loss_fn, reg_param, device)\n",
      "Cell \u001b[0;32mIn[273], line 18\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, train_loader, loss_fn, optimizer, reg_param, device)\u001b[0m\n\u001b[1;32m     15\u001b[0m correct[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meq(prediction, y)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m loss_step\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m norms \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(param\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()) \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Do not change this cell\n",
    "random_seed = 1\n",
    "(\n",
    "    model,\n",
    "    train_losses,\n",
    "    train_accuracies,\n",
    "    valid_losses,\n",
    "    valid_accuracies,\n",
    "    test_losses,\n",
    "    test_accuracies,\n",
    ") = train_conv_model(train_loader, valid_loader, test_loader, random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0408016e-99e3-4a8a-add3-c3e7a29d5162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change this cell\n",
    "plot_loss_performance(train_losses, valid_losses, test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ffc77-8d95-4676-9ba0-4fa083b235ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change this cell\n",
    "plot_accuracy_performance(train_accuracies, valid_accuracies, test_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20143cfd-82b4-4fc6-8a8a-f4c4977cb9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_validations = 10\n",
    "    n_epochs = 10\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    train_accuracy = np.empty(0)\n",
    "    train_loss = np.empty(0)\n",
    "    validation_accuracy = np.empty(0)\n",
    "    validation_loss = np.empty(0)\n",
    "    test_accuracy = np.empty(0)\n",
    "    test_loss = np.empty(0)\n",
    "\n",
    "    \n",
    "    conv1out_range = np.arange(2, 12, 2)\n",
    "    conv2out_range = np.arange(8, 24, 2)\n",
    "    linout_range = np.arange(4, 16, 2)\n",
    "    lr_range = np.linspace(0.001, .05, 20)\n",
    "    reg_param_range = np.linspace(0, 1, 20)\n",
    "    input_size = 1\n",
    "    output_size = 10\n",
    "\n",
    "    param_grid = {\n",
    "        \"conv1out\": conv1out_range,\n",
    "        \"conv2out\": conv2out_range,\n",
    "        \"linout\": linout_range,\n",
    "        \"lr\": lr_range,\n",
    "        \"reg_param\": reg_param_range\n",
    "    }\n",
    "    \n",
    "    conv1out_accuracies = np.zeros(len(conv1out_range))\n",
    "    conv2out_accuracies = np.zeros(len(conv2out_range))\n",
    "    linout_accuracies = np.zeros(len(linout_range))\n",
    "    lr_accuracies = np.zeros(len(lr_range))\n",
    "    reg_param_accuracies = np.zeros(len(reg_param_range))\n",
    "\n",
    "    j = 0\n",
    "    for reg_param in reg_param_range:\n",
    "        train_accuracies = np.zeros(n_epochs)\n",
    "        train_losses = np.zeros(n_epochs)\n",
    "        validation_accuracies = np.zeros(n_epochs)\n",
    "        validation_losses = np.zeros(n_epochs)\n",
    "        \n",
    "        model = ConvModel(input_size, output_size).to(device)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "        for i in np.arange(n_epochs):\n",
    "            model, train_accuracies[i], train_losses[i] = train_step(model, train_loader, loss_fn, optimizer, reg_param, device)\n",
    "            model, validation_accuracies[i], validation_losses[i] = evaluation_step(model, valid_loader, loss_fn, reg_param, device)\n",
    "        train_accuracy = np.append(train_accuracy, train_accuracies)\n",
    "        validation_accuracy = np.append(validation_accuracy, validation_accuracies)\n",
    "        train_loss = np.append(train_loss, train_losses)\n",
    "        validation_loss = np.append(validation_loss, validation_losses)\n",
    "\n",
    "        reg_param_accuracies[j] = validation_accuracies\n",
    "        j += 1\n",
    "        \n",
    "    reg_param = reg_param_range[np.argmax(reg_param_accuracies, dim=1)]\n",
    "    print(\"Tuned Reg Param\")\n",
    "\n",
    "    j = 0\n",
    "    for lr in lr_range:\n",
    "        train_accuracies = np.zeros(n_epochs)\n",
    "        train_losses = np.zeros(n_epochs)\n",
    "        validation_accuracies = np.zeros(n_epochs)\n",
    "        validation_losses = np.zeros(n_epochs)\n",
    "        \n",
    "        model = ConvModel(input_size, output_size)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "        for i in np.arange(n_epochs):\n",
    "            model, train_accuracies[i], train_losses[i] = train_step(model, train_loader, loss_fn, optimizer, reg_param, device)\n",
    "            model, validation_accuracies[i], validation_losses[i] = evaluation_step(model, valid_loader, loss_fn, reg_param, device)\n",
    "        train_accuracy = np.append(train_accuracy, train_accuracies)\n",
    "        validation_accuracy = np.append(validation_accuracy, validation_accuracies)\n",
    "        train_loss = np.append(train_loss, train_losses)\n",
    "        validation_loss = np.append(validation_loss, validation_losses)\n",
    "\n",
    "        lr_accuracies[j] = validation_accuracies\n",
    "        j += 1\n",
    "\n",
    "    lr_pick = lr_range[np.argmax(lr_accuracies)]\n",
    "    print(\"Tuned Learning Rate\")\n",
    "    \n",
    "    j = 0\n",
    "    for conv1out in conv1out_range:\n",
    "        print(\"j\")\n",
    "        train_accuracies = np.zeros(n_epochs)\n",
    "        train_losses = np.zeros(n_epochs)\n",
    "        validation_accuracies = np.zeros(n_epochs)\n",
    "        validation_losses = np.zeros(n_epochs)\n",
    "        \n",
    "        model = ConvModel(input_size, output_size, conv1out=conv1out)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr_pick)\n",
    "\n",
    "        for i in np.arange(n_epochs):\n",
    "            model, train_accuracies[i], train_losses[i] = train_step(model, train_loader, loss_fn, optimizer, reg_param, device)\n",
    "            model, validation_accuracies[i], validation_losses[i] = evaluation_step(model, valid_loader, loss_fn, reg_param, device)\n",
    "        train_accuracy = np.append(train_accuracy, train_accuracies)\n",
    "        validation_accuracy = np.append(validation_accuracy, validation_accuracies)\n",
    "        train_loss = np.append(train_loss, train_losses)\n",
    "        validation_loss = np.append(validation_loss, validation_losses)\n",
    "\n",
    "        conv1out_accuracies[j] = validation_accuracies\n",
    "        j += 1\n",
    "        \n",
    "    conv1out_pick = conv1out_range[np.argmax(conv1out_accuracies, dim=1)]\n",
    "    print(\"Tuned conv1out\")\n",
    "    \n",
    "    j = 0\n",
    "    for conv2out in conv2out_range:\n",
    "        train_accuracies = np.zeros(n_epochs)\n",
    "        train_losses = np.zeros(n_epochs)\n",
    "        validation_accuracies = np.zeros(n_epochs)\n",
    "        validation_losses = np.zeros(n_epochs)\n",
    "        \n",
    "        model = ConvModel(input_size, output_size, conv1out=conv1out_pick, conv2out=conv2out)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr_pick)\n",
    "        for i in np.arange(n_epochs):\n",
    "            model, train_accuracies[i], train_losses[i] = train_step(model, train_loader, loss_fn, optimizer, reg_param, device)\n",
    "            model, validation_accuracies[i], validation_losses[i] = evaluation_step(model, valid_loader, loss_fn, reg_param, device)\n",
    "        train_accuracy = np.append(train_accuracy, train_accuracies)\n",
    "        validation_accuracy = np.append(validation_accuracy, validation_accuracies)\n",
    "        train_loss = np.append(train_loss, train_losses)\n",
    "        validation_loss = np.append(validation_loss, validation_losses)\n",
    "\n",
    "        conv2out_accuracies[j] = validation_accuracies\n",
    "        j += 1\n",
    "\n",
    "    conv2out_pick = conv2out_range[np.argmax(conv2out_accuracies, dim=1)]\n",
    "    print(\"Tuned conv2out\")\n",
    "    \n",
    "    j = 0\n",
    "    for linout in linout_range:\n",
    "        train_accuracies = np.zeros(n_epochs)\n",
    "        train_losses = np.zeros(n_epochs)\n",
    "        validation_accuracies = np.zeros(n_epochs)\n",
    "        validation_losses = np.zeros(n_epochs)\n",
    "        \n",
    "        model = ConvModel(input_size, output_size, conv1out=conv1out_pick, conv2out=conv2out_pick, linout=linout)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr_pick)\n",
    "        for i in np.arange(n_epochs):\n",
    "            model, train_accuracies[i], train_losses[i] = train_step(model, train_loader, loss_fn, optimizer, reg_param, device)\n",
    "            model, validation_accuracies[i], validation_losses[i] = evaluation_step(model, valid_loader, loss_fn, reg_param, device)\n",
    "        train_accuracy = np.append(train_accuracy, train_accuracies)\n",
    "        validation_accuracy = np.append(validation_accuracy, validation_accuracies)\n",
    "        train_loss = np.append(train_loss, train_losses)\n",
    "        validation_loss = np.append(validation_loss, validation_losses)\n",
    "\n",
    "        linout_accuracies[j] = validation_accuracies\n",
    "        j += 1\n",
    "        \n",
    "    linout_pick = linout_range[np.argmax(linout_accuracies, dim=1)]\n",
    "    print(\"Finished tuning\")\n",
    "\n",
    "    model = ConvModel(input_size, output_size, conv1out=conv1out_pick, conv2out=conv2out_pick, linout=linout_pick)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr_pick)\n",
    "\n",
    "    train_accuracy_best = np.zeros(n_epochs)\n",
    "    validation_accuracy_best = np.zeros(n_epochs)\n",
    "    test_accuracy_best = np.zeros(n_epochs)\n",
    "    train_loss_best = np.zeros(n_epochs)\n",
    "    validation_loss_best = np.zeros(n_epochs)\n",
    "    test_loss_best = np.zeros(n_epochs)\n",
    "    \n",
    "    for i in np.arange(n_epochs):\n",
    "        model, train_accuracy_best[i], train_loss_best[i] = train_step(model, train_loader, loss_fn, optimizer, reg_param, device)\n",
    "        model, validation_accuracy_best[i], validation_loss_best[i] = evaluation_step(model, valid_loader, loss_fn, reg_param, device)\n",
    "        model, test_accuracy_best[i], test_loss_best[i] = evaluation_step(model, test_loader, loss_fn, reg_param, device)\n",
    "\n",
    "    return model, train_loss_best, train_accuracy_best, validation_loss_best, validation_accuracy_best, test_loss_best, test_accuracy_best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
